{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of cnn_dist.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHecrF91fJ3h"
      },
      "source": [
        "import numpy as np, os\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras as tfk\r\n",
        "keras = tfk\r\n",
        "import datetime as dt\r\n",
        "import six\r\n",
        "import h5py\r\n",
        "import requests\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt \r\n",
        "import seaborn as sns\r\n",
        "sns.set()\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3psP7yOQIVD"
      },
      "source": [
        "# Data loading "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKPa5-b4L7hX"
      },
      "source": [
        "# download the data \r\n",
        "url = 'https://www.dropbox.com/s/ysrim2re8mh22z9/synthetic_code_dataset.h5?dl=0'\r\n",
        "save_name = 'data.h5'\r\n",
        "_=!wget {url} -O {save_name}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCvBWh9oNYhd"
      },
      "source": [
        "# load the data into x_train, y_train, .....\r\n",
        "f = h5py.File(save_name, 'r')\r\n",
        "suffixes = ['train', 'test', 'valid']\r\n",
        "for suffix in suffixes:\r\n",
        "    exec(\"x_%s=np.transpose(f.get(\\\"X_%s\\\")[:], (0, 2, 1))\"%(suffix, suffix))\r\n",
        "    exec(\"y_%s=f.get(\\\"Y_%s\\\")[:]\"%(suffix, suffix))\r\n",
        "f.close()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISQFZ12vQKeu"
      },
      "source": [
        "# Model definition function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5qiwx1sfthV"
      },
      "source": [
        "def get_activation(activation = 'relu'):\r\n",
        "    \"\"\"\r\n",
        "    Create an activation function. The activation argument should one of:\r\n",
        "    1. A string representing the keras name of the activation. \r\n",
        "    2. A callable which may or may not be an instance of keras.layers.Layer. \r\n",
        "    \"\"\"\r\n",
        "    if isinstance(activation, str):\r\n",
        "        actfn = tfk.layers.Activation(activation)\r\n",
        "    else:\r\n",
        "        if callable(activation) and not isinstance(activation, tfk.layers.Layer):\r\n",
        "            actfn = tfk.layers.Activation(activation)\r\n",
        "        else:\r\n",
        "            actfn = activation\r\n",
        "    return actfn"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baDorqePRC-5"
      },
      "source": [
        "def conv_layer(x, num_filters, kernel_size, padding, activation, dropout=0.5, l2=1e-6, bn=True): \r\n",
        "    \"\"\"\r\n",
        "    A convolutional block comprising of a convolutional layer followed by\r\n",
        "    batch normalization, an activation function, and dropout. \r\n",
        "    \"\"\"\r\n",
        "    y = tfk.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, kernel_regularizer=tfk.regularizers.l2(l2), padding=padding)(x)\r\n",
        "    if bn:\r\n",
        "        y = tfk.layers.BatchNormalization()(y)\r\n",
        "    actfn = get_activation(activation)\r\n",
        "    y = actfn(y)\r\n",
        "    if dropout:\r\n",
        "        y = tfk.layers.Dropout(dropout)(y)\r\n",
        "    return y\r\n",
        "\r\n",
        "def dense_layer(x, num_units, activation, dropout=0.5, l2=None, bn=True):\r\n",
        "    \"\"\"\r\n",
        "    A dense block comprising of a dense layer followed by batch normalization, \r\n",
        "    activation and dropout. \r\n",
        "    \"\"\"\r\n",
        "    y = tfk.layers.Dense(num_units, use_bias=False, kernel_regularizer=tfk.regularizers.l2(l2))(x)\r\n",
        "    if bn:\r\n",
        "        y = tfk.layers.BatchNormalization()(y)\r\n",
        "    actfn = get_activation(activation)\r\n",
        "    y = actfn(y)\r\n",
        "    if dropout:\r\n",
        "        y = tfk.layers.Dropout(dropout)(y)\r\n",
        "    return y\r\n",
        "\r\n",
        "def get_model(L, A, name=\"cnn_att\"):\r\n",
        "\t## input layer\r\n",
        "\tx = tfk.layers.Input((L, A), name='Input')\r\n",
        "\t\r\n",
        "\t## 1st conv layer\r\n",
        "\ty = keras.layers.Conv1D(filters=32, kernel_size=19, kernel_regularizer=tfk.regularizers.l2(1e-3), padding='same', name='conv1', use_bias=True)(x)\r\n",
        "\ty = keras.layers.Activation('relu')(y)\r\n",
        "\ty = keras.layers.MaxPool1D(pool_size=4)(y)\r\n",
        "\t\r\n",
        "\t# multi head attention layer\r\n",
        "\tembedding = keras.layers.Dropout(0.1)(y)\r\n",
        "\ty, weights = keras.layers.MultiHeadAttention(num_heads=8, key_dim=64, value_dim=64)(embedding, embedding, return_attention_scores=True)\r\n",
        "\ty = keras.layers.Dropout(0.1)(y)\r\n",
        "\ty = keras.layers.LayerNormalization(epsilon=1e-6)(y)\r\n",
        "\t\r\n",
        "\t# everything else\r\n",
        "\ty = keras.layers.Flatten()(y)\r\n",
        "\ty = keras.layers.Dense(128, activation=None, use_bias=False)(y)\r\n",
        "\ty = keras.layers.BatchNormalization()(y)\r\n",
        "\ty = keras.layers.Activation('relu')(y)\r\n",
        "\ty = keras.layers.Dropout(0.5)(y)\r\n",
        "\ty = keras.layers.Dense(1, name='logits')(y)\r\n",
        "\ty = keras.layers.Activation('sigmoid', name='output')(y)\r\n",
        "\tmodel = tfk.Model(inputs=x, outputs=y, name=name)\r\n",
        "\treturn model\r\n",
        "\r\n",
        "# def get_model(L, A, activation='relu', name='cnn_dist'):\r\n",
        "#     \"\"\"\r\n",
        "#     A function to assemble the full CNN distributed model. \r\n",
        "#     \"\"\"\r\n",
        "#     # input layer \r\n",
        "#     x = tfk.layers.Input((L, A), name='input')\r\n",
        "\r\n",
        "#     # 1st convolutional block \r\n",
        "#     y = conv_layer(x,num_filters=24, kernel_size=19, padding='same', dropout=0.1,l2=1e-6, bn=True, activation=activation)\r\n",
        "    \r\n",
        "#     # 2nd conv. block + pooling \r\n",
        "#     y = conv_layer(y,num_filters=32, kernel_size=7, padding='same', activation=activation, dropout=0.2,l2=1e-6, bn=True)\r\n",
        "#     y = tfk.layers.MaxPool1D(pool_size=4)(y)\r\n",
        "    \r\n",
        "#     # 3rd convolutional block + pooling \r\n",
        "#     y = conv_layer(y,num_filters=64, kernel_size=3, padding='same', activation=activation, dropout=0.4,l2=1e-6, bn=True)\r\n",
        "#     y = tfk.layers.MaxPool1D(pool_size=3, strides=3, padding='same')(y)\r\n",
        "    \r\n",
        "#     # dense block and final output layer \r\n",
        "#     y = tfk.layers.Flatten()(y)\r\n",
        "#     y = dense_layer(y, num_units=96, activation=activation, dropout=0.5, l2=1e-6, bn=True)\r\n",
        "#     y = tfk.layers.Dense(1, use_bias=True, name = 'logits')(y)\r\n",
        "#     y = tfk.layers.Activation('sigmoid')(y)\r\n",
        "\r\n",
        "#     # assemble full model\r\n",
        "#     model = tfk.Model(x, y, name=name)\r\n",
        "#     return model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQzKg97ayWQ-"
      },
      "source": [
        "# Train a teacher model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUkdVPMdifWD",
        "outputId": "b614d386-80af-479d-9cf7-5bf294e9dbfb"
      },
      "source": [
        "# instantiate the teacher model \r\n",
        "activation = 'relu' \r\n",
        "#activation = lambda x : tf.math.sin(x) + tf.math.cos(x)\r\n",
        "L, A = x_train.shape[1:]\r\n",
        "\r\n",
        "teacher_model = get_model(L, A, name='teacher')\r\n",
        "#teacher_model = get_model(L, A, activation, name='teacher')\r\n",
        "\r\n",
        "# compile the teacher model \r\n",
        "lossfn = tfk.losses.BinaryCrossentropy(name='bce')\r\n",
        "modelmetrics = [tfk.metrics.BinaryAccuracy(name='ACC'), tfk.metrics.AUC(curve='PR', name='AUPR'), tfk.metrics.AUC(curve='ROC', name='AUROC')]\r\n",
        "optimizer = tfk.optimizers.Adam(learning_rate=1e-2)\r\n",
        "teacher_model.compile(loss=lossfn, metrics=modelmetrics, optimizer=optimizer)\r\n",
        "\r\n",
        "# fit the teacher model \r\n",
        "num_epochs = 100\r\n",
        "callbacks = [tfk.callbacks.EarlyStopping(monitor='val_AUROC', patience=20), \r\n",
        "             tfk.callbacks.ModelCheckpoint(\"best_teacher_model.hdf5\", monitor='val_AUROC', mode='max', save_best_only=True)]\r\n",
        "teacher_model.fit(x_train, y_train, \r\n",
        "                    epochs=num_epochs, \r\n",
        "                    batch_size=128, \r\n",
        "                    callbacks=callbacks, \r\n",
        "                    shuffle=True, \r\n",
        "                    initial_epoch=0,\r\n",
        "                    validation_data=(x_valid, y_valid))\r\n",
        "teacher_model = tfk.models.load_model('best_teacher_model.hdf5')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "110/110 [==============================] - 9s 17ms/step - loss: 0.6517 - ACC: 0.6821 - AUPR: 0.7160 - AUROC: 0.7392 - val_loss: 1.6342 - val_ACC: 0.6425 - val_AUPR: 0.8502 - val_AUROC: 0.8229\n",
            "Epoch 2/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.4315 - ACC: 0.8240 - AUPR: 0.8932 - AUROC: 0.8992 - val_loss: 0.5414 - val_ACC: 0.8260 - val_AUPR: 0.9028 - val_AUROC: 0.9240\n",
            "Epoch 3/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.3644 - ACC: 0.8667 - AUPR: 0.9302 - AUROC: 0.9363 - val_loss: 0.4989 - val_ACC: 0.8170 - val_AUPR: 0.9500 - val_AUROC: 0.9517\n",
            "Epoch 4/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.3227 - ACC: 0.8854 - AUPR: 0.9503 - AUROC: 0.9531 - val_loss: 0.3098 - val_ACC: 0.8995 - val_AUPR: 0.9555 - val_AUROC: 0.9589\n",
            "Epoch 5/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.2943 - ACC: 0.8972 - AUPR: 0.9603 - AUROC: 0.9623 - val_loss: 0.3399 - val_ACC: 0.8755 - val_AUPR: 0.9592 - val_AUROC: 0.9618\n",
            "Epoch 6/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.2886 - ACC: 0.9042 - AUPR: 0.9630 - AUROC: 0.9645 - val_loss: 0.5865 - val_ACC: 0.8140 - val_AUPR: 0.9447 - val_AUROC: 0.9630\n",
            "Epoch 7/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.2805 - ACC: 0.9079 - AUPR: 0.9641 - AUROC: 0.9670 - val_loss: 0.2802 - val_ACC: 0.9095 - val_AUPR: 0.9645 - val_AUROC: 0.9685\n",
            "Epoch 8/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.2893 - ACC: 0.9046 - AUPR: 0.9629 - AUROC: 0.9643 - val_loss: 0.3439 - val_ACC: 0.8845 - val_AUPR: 0.9680 - val_AUROC: 0.9693\n",
            "Epoch 9/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.2693 - ACC: 0.9142 - AUPR: 0.9679 - AUROC: 0.9701 - val_loss: 0.4733 - val_ACC: 0.8335 - val_AUPR: 0.9650 - val_AUROC: 0.9657\n",
            "Epoch 10/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.2672 - ACC: 0.9116 - AUPR: 0.9691 - AUROC: 0.9705 - val_loss: 0.2807 - val_ACC: 0.9065 - val_AUPR: 0.9739 - val_AUROC: 0.9767\n",
            "Epoch 11/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.2743 - ACC: 0.9075 - AUPR: 0.9641 - AUROC: 0.9683 - val_loss: 0.3485 - val_ACC: 0.8830 - val_AUPR: 0.9618 - val_AUROC: 0.9695\n",
            "Epoch 12/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.2651 - ACC: 0.9109 - AUPR: 0.9680 - AUROC: 0.9705 - val_loss: 0.2808 - val_ACC: 0.8955 - val_AUPR: 0.9729 - val_AUROC: 0.9745\n",
            "Epoch 13/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.2642 - ACC: 0.9108 - AUPR: 0.9680 - AUROC: 0.9705 - val_loss: 0.8385 - val_ACC: 0.7430 - val_AUPR: 0.9591 - val_AUROC: 0.9503\n",
            "Epoch 14/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.2623 - ACC: 0.9130 - AUPR: 0.9686 - AUROC: 0.9708 - val_loss: 0.5873 - val_ACC: 0.8275 - val_AUPR: 0.9641 - val_AUROC: 0.9596\n",
            "Epoch 15/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.2526 - ACC: 0.9165 - AUPR: 0.9722 - AUROC: 0.9734 - val_loss: 0.2965 - val_ACC: 0.8875 - val_AUPR: 0.9719 - val_AUROC: 0.9742\n",
            "Epoch 16/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.2540 - ACC: 0.9165 - AUPR: 0.9710 - AUROC: 0.9729 - val_loss: 0.4572 - val_ACC: 0.8495 - val_AUPR: 0.9527 - val_AUROC: 0.9682\n",
            "Epoch 17/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.2461 - ACC: 0.9205 - AUPR: 0.9732 - AUROC: 0.9746 - val_loss: 0.2602 - val_ACC: 0.9095 - val_AUPR: 0.9739 - val_AUROC: 0.9787\n",
            "Epoch 18/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.2514 - ACC: 0.9195 - AUPR: 0.9714 - AUROC: 0.9735 - val_loss: 0.3553 - val_ACC: 0.8540 - val_AUPR: 0.9730 - val_AUROC: 0.9758\n",
            "Epoch 19/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.2457 - ACC: 0.9184 - AUPR: 0.9740 - AUROC: 0.9748 - val_loss: 0.3079 - val_ACC: 0.8915 - val_AUPR: 0.9742 - val_AUROC: 0.9786\n",
            "Epoch 20/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.2468 - ACC: 0.9199 - AUPR: 0.9730 - AUROC: 0.9744 - val_loss: 0.3759 - val_ACC: 0.8790 - val_AUPR: 0.9652 - val_AUROC: 0.9730\n",
            "Epoch 21/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.2548 - ACC: 0.9196 - AUPR: 0.9703 - AUROC: 0.9720 - val_loss: 0.2603 - val_ACC: 0.9085 - val_AUPR: 0.9763 - val_AUROC: 0.9783\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kpvZ10Vvxcr"
      },
      "source": [
        "teacher_model = tfk.models.load_model('best_teacher_model.hdf5')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lep5Pun5E0k-"
      },
      "source": [
        "# Knowledge distillation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTehiZ5nGzou"
      },
      "source": [
        "## Define a `Distiller` class that takes in a trained teacher model, an untrained student model and distills the knowledge in the teacher model onto the student model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tai-epSfGkUz"
      },
      "source": [
        "class Distiller(keras.Model):\r\n",
        "    def get_config(self,):\r\n",
        "        \"\"\"\r\n",
        "        Implement the config dictionary to enable serialization\r\n",
        "        \"\"\"\r\n",
        "        config = {}\r\n",
        "        config['student'] = self.student\r\n",
        "        config['teacher'] = self.teacher\r\n",
        "        return config\r\n",
        "    \r\n",
        "    def __init__(self, student, teacher):\r\n",
        "        super(Distiller, self).__init__()\r\n",
        "        self.teacher = teacher\r\n",
        "        self.student = student\r\n",
        "\r\n",
        "    def compile(\r\n",
        "        self,\r\n",
        "        optimizer,\r\n",
        "        metrics,\r\n",
        "        student_loss_fn,\r\n",
        "        distillation_loss_fn,\r\n",
        "        alpha=0.1,\r\n",
        "        temperature=3,\r\n",
        "    ):\r\n",
        "        \"\"\" Configure the distiller.\r\n",
        "\r\n",
        "        Args:\r\n",
        "            optimizer: Keras optimizer for the student weights\r\n",
        "            metrics: Keras metrics for evaluation\r\n",
        "            student_loss_fn: Loss function of difference between student\r\n",
        "                predictions and ground-truth\r\n",
        "            distillation_loss_fn: Loss function of difference between soft\r\n",
        "                student predictions and soft teacher predictions\r\n",
        "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\r\n",
        "            temperature: Temperature for softening probability distributions.\r\n",
        "                Larger temperature gives softer distributions.\r\n",
        "        \"\"\"\r\n",
        "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\r\n",
        "        self.student_loss_fn = student_loss_fn\r\n",
        "        self.distillation_loss_fn = distillation_loss_fn\r\n",
        "        self.alpha = alpha\r\n",
        "        self.temperature = temperature\r\n",
        "\r\n",
        "    def train_step(self, data):\r\n",
        "        # Unpack data\r\n",
        "        x, y = data\r\n",
        "\r\n",
        "        # Forward pass of teacher\r\n",
        "        teacher_predictions = self.teacher(x, training=False)\r\n",
        "\r\n",
        "        with tf.GradientTape() as tape:\r\n",
        "            # Forward pass of student\r\n",
        "            student_predictions = self.student(x, training=True)\r\n",
        "\r\n",
        "            # Compute losses\r\n",
        "            student_loss = self.student_loss_fn(y, student_predictions)\r\n",
        "            distillation_loss = self.distillation_loss_fn(\r\n",
        "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\r\n",
        "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\r\n",
        "            )\r\n",
        "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\r\n",
        "\r\n",
        "        # Compute gradients\r\n",
        "        trainable_vars = self.student.trainable_variables\r\n",
        "        gradients = tape.gradient(loss, trainable_vars)\r\n",
        "\r\n",
        "        # Update weights\r\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\r\n",
        "\r\n",
        "        # Update the metrics configured in `compile()`.\r\n",
        "        self.compiled_metrics.update_state(y, student_predictions)\r\n",
        "\r\n",
        "        # Return a dict of performance\r\n",
        "        results = {m.name: m.result() for m in self.metrics}\r\n",
        "        results.update(\r\n",
        "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\r\n",
        "        )\r\n",
        "        return results\r\n",
        "\r\n",
        "    def test_step(self, data):\r\n",
        "        # Unpack the data\r\n",
        "        x, y = data\r\n",
        "\r\n",
        "        # Compute predictions\r\n",
        "        y_prediction = self.student(x, training=False)\r\n",
        "\r\n",
        "        # Calculate the loss\r\n",
        "        student_loss = self.student_loss_fn(y, y_prediction)\r\n",
        "\r\n",
        "        # Update the metrics.\r\n",
        "        self.compiled_metrics.update_state(y, y_prediction)\r\n",
        "\r\n",
        "        # Return a dict of performance\r\n",
        "        results = {\"student_loss\": student_loss}\r\n",
        "        results.update({m.name: m.result() for m in self.metrics})\r\n",
        "        return results\r\n",
        "    \r\n",
        "    @property\r\n",
        "    def metrics_names(self):\r\n",
        "        return ['student_loss']+[m.name for m in self.metrics]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvOT3OZDHCgu"
      },
      "source": [
        "def get_student_model(L, A, activation='relu', name='deepbind'):\r\n",
        "    \"\"\"\r\n",
        "    Defining the deepbind architecture in here. \r\n",
        "    \"\"\"\r\n",
        "    x = tfk.layers.Input((L, A), name='input')\r\n",
        "    y = tfk.layers.Conv1D(filters=16, kernel_size=24, padding='valid', kernel_regularizer=tfk.regularizers.l2(1e-6))(x)\r\n",
        "    actfn = get_activation(activation=activation)\r\n",
        "    y = actfn(y)\r\n",
        "    y = tfk.layers.Lambda(lambda x : tf.reduce_max(x, axis=1))(y)  # max pooling\r\n",
        "    y = tfk.layers.Dropout(0.5)(y)  \r\n",
        "    y = tfk.layers.Dense(32, activation='relu')(y)\r\n",
        "    y = tfk.layers.Dense(1, name='logits')(y)\r\n",
        "    y = tfk.layers.Activation('sigmoid', name='output')(y)\r\n",
        "\r\n",
        "    model = tfk.Model(inputs=x, outputs=y, name=name)\r\n",
        "    return model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K79gsB5RIhPV",
        "outputId": "a93b8808-d895-4471-ce9e-827accd15b6a"
      },
      "source": [
        "# instantiate the student model and the distiller \r\n",
        "student_model = get_student_model(L, A)\r\n",
        "distiller = Distiller(student_model, teacher_model)\r\n",
        "\r\n",
        "# compile the distiller\r\n",
        "alpha = 0.8\r\n",
        "temperature = 1. \r\n",
        "distiller.compile(\r\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\r\n",
        "    metrics=modelmetrics,\r\n",
        "    student_loss_fn=keras.losses.BinaryCrossentropy(name='bce'),\r\n",
        "    distillation_loss_fn=keras.losses.KLDivergence(),\r\n",
        "    alpha=alpha,\r\n",
        "    temperature=temperature,\r\n",
        ")\r\n",
        "\r\n",
        "# perform distillation\r\n",
        "num_epochs = 50\r\n",
        "callbacks = [tfk.callbacks.EarlyStopping(monitor='val_AUROC', patience=20), \r\n",
        "             tfk.callbacks.ModelCheckpoint(\"best_distiller.hdf5\", monitor='val_AUROC', mode='max',save_weights_only=True, save_best_only=True)]\r\n",
        "distiller.fit(x_train, y_train, \r\n",
        "                epochs=num_epochs, \r\n",
        "                batch_size=128, \r\n",
        "                callbacks=callbacks, \r\n",
        "                shuffle=True, \r\n",
        "                validation_data=(x_valid, y_valid))\r\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "110/110 [==============================] - 2s 11ms/step - ACC: 0.6225 - AUPR: 0.7180 - AUROC: 0.6999 - student_loss: 0.6985 - distillation_loss: 0.0000e+00 - val_student_loss: 0.6856 - val_ACC: 0.5955 - val_AUPR: 0.6198 - val_AUROC: 0.6378\n",
            "Epoch 2/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.5527 - AUPR: 0.5569 - AUROC: 0.5685 - student_loss: 0.6812 - distillation_loss: 0.0000e+00 - val_student_loss: 0.6659 - val_ACC: 0.6505 - val_AUPR: 0.7548 - val_AUROC: 0.7729\n",
            "Epoch 3/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.6249 - AUPR: 0.6647 - AUROC: 0.6754 - student_loss: 0.6398 - distillation_loss: 0.0000e+00 - val_student_loss: 0.6058 - val_ACC: 0.7405 - val_AUPR: 0.8120 - val_AUROC: 0.8277\n",
            "Epoch 4/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.6868 - AUPR: 0.7417 - AUROC: 0.7520 - student_loss: 0.5906 - distillation_loss: 0.0000e+00 - val_student_loss: 0.5726 - val_ACC: 0.7695 - val_AUPR: 0.8418 - val_AUROC: 0.8544\n",
            "Epoch 5/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.7182 - AUPR: 0.7834 - AUROC: 0.7935 - student_loss: 0.5524 - distillation_loss: 0.0000e+00 - val_student_loss: 0.5408 - val_ACC: 0.7975 - val_AUPR: 0.8693 - val_AUROC: 0.8803\n",
            "Epoch 6/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.7374 - AUPR: 0.8176 - AUROC: 0.8166 - student_loss: 0.5251 - distillation_loss: 0.0000e+00 - val_student_loss: 0.5323 - val_ACC: 0.7985 - val_AUPR: 0.8861 - val_AUROC: 0.8965\n",
            "Epoch 7/50\n",
            "110/110 [==============================] - 1s 9ms/step - ACC: 0.7588 - AUPR: 0.8368 - AUROC: 0.8376 - student_loss: 0.5016 - distillation_loss: 0.0000e+00 - val_student_loss: 0.5213 - val_ACC: 0.8215 - val_AUPR: 0.8991 - val_AUROC: 0.9065\n",
            "Epoch 8/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.7713 - AUPR: 0.8469 - AUROC: 0.8493 - student_loss: 0.4814 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4990 - val_ACC: 0.8345 - val_AUPR: 0.9046 - val_AUROC: 0.9117\n",
            "Epoch 9/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.7691 - AUPR: 0.8585 - AUROC: 0.8543 - student_loss: 0.4740 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4930 - val_ACC: 0.8440 - val_AUPR: 0.9098 - val_AUROC: 0.9188\n",
            "Epoch 10/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.7858 - AUPR: 0.8699 - AUROC: 0.8686 - student_loss: 0.4577 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4873 - val_ACC: 0.8390 - val_AUPR: 0.9099 - val_AUROC: 0.9158\n",
            "Epoch 11/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.7820 - AUPR: 0.8758 - AUROC: 0.8699 - student_loss: 0.4414 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4555 - val_ACC: 0.8570 - val_AUPR: 0.9244 - val_AUROC: 0.9300\n",
            "Epoch 12/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.7900 - AUPR: 0.8761 - AUROC: 0.8738 - student_loss: 0.4421 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4355 - val_ACC: 0.8585 - val_AUPR: 0.9276 - val_AUROC: 0.9306\n",
            "Epoch 13/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.8060 - AUPR: 0.8816 - AUROC: 0.8846 - student_loss: 0.4276 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4332 - val_ACC: 0.8645 - val_AUPR: 0.9311 - val_AUROC: 0.9340\n",
            "Epoch 14/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.8011 - AUPR: 0.8926 - AUROC: 0.8878 - student_loss: 0.4225 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4287 - val_ACC: 0.8725 - val_AUPR: 0.9321 - val_AUROC: 0.9364\n",
            "Epoch 15/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.8021 - AUPR: 0.8890 - AUROC: 0.8833 - student_loss: 0.4239 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4470 - val_ACC: 0.8670 - val_AUPR: 0.9385 - val_AUROC: 0.9425\n",
            "Epoch 16/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.8121 - AUPR: 0.8927 - AUROC: 0.8918 - student_loss: 0.4152 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4128 - val_ACC: 0.8715 - val_AUPR: 0.9396 - val_AUROC: 0.9438\n",
            "Epoch 17/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.8090 - AUPR: 0.9001 - AUROC: 0.8949 - student_loss: 0.4136 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4128 - val_ACC: 0.8810 - val_AUPR: 0.9393 - val_AUROC: 0.9454\n",
            "Epoch 18/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.8165 - AUPR: 0.8963 - AUROC: 0.8957 - student_loss: 0.4119 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4035 - val_ACC: 0.8790 - val_AUPR: 0.9382 - val_AUROC: 0.9443\n",
            "Epoch 19/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.8130 - AUPR: 0.8989 - AUROC: 0.8964 - student_loss: 0.4092 - distillation_loss: 0.0000e+00 - val_student_loss: 0.3817 - val_ACC: 0.8785 - val_AUPR: 0.9409 - val_AUROC: 0.9462\n",
            "Epoch 20/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.8086 - AUPR: 0.8945 - AUROC: 0.8908 - student_loss: 0.4159 - distillation_loss: 0.0000e+00 - val_student_loss: 0.3917 - val_ACC: 0.8730 - val_AUPR: 0.9385 - val_AUROC: 0.9422\n",
            "Epoch 21/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.8104 - AUPR: 0.8960 - AUROC: 0.8966 - student_loss: 0.4011 - distillation_loss: 0.0000e+00 - val_student_loss: 0.3798 - val_ACC: 0.8860 - val_AUPR: 0.9435 - val_AUROC: 0.9478\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe4bb2edd68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eg2dHq2vKK70"
      },
      "source": [
        "def plot_f_and_grad(model):\r\n",
        "    # pick a random sample \r\n",
        "    N, L, A = x_train.shape\r\n",
        "    xsample = x_train[np.random.randint(0, N)][None, :, :]\r\n",
        "\r\n",
        "    # define a keras model mapping an input sequence to the logits of the teacher model\r\n",
        "    func = tfk.Model(inputs=model.input, outputs=model.get_layer('logits').output)\r\n",
        "\r\n",
        "    # define a set of probe sequences by sampling points in the ith nucleotide, jth channel \r\n",
        "    # i and j are picked randomly\r\n",
        "    n_probe = 100\r\n",
        "    x_probe = np.linspace(0, 1, n_probe)\r\n",
        "    n_samples = 50\r\n",
        "    Is, Js, y_ijs, y_ij_grads = [], [], [], []\r\n",
        "    for i in range(n_samples):  \r\n",
        "        i, j = np.random.randint(0, L), np.random.randint(0, A)\r\n",
        "        Is.append(i)\r\n",
        "        Js.append(j)\r\n",
        "        \r\n",
        "        x_ij_probe = np.zeros((n_probe, L, A))\r\n",
        "        x_ij_probe[:, i, j] = x_probe\r\n",
        "        x_ij_probe = tf.convert_to_tensor(x_ij_probe)\r\n",
        "\r\n",
        "        with tf.GradientTape() as tape:\r\n",
        "            tape.watch(x_ij_probe)\r\n",
        "            y_ij_pred = func(x_ij_probe)\r\n",
        "        y_ij_grad = tape.gradient(y_ij_pred, x_ij_probe)\r\n",
        "        \r\n",
        "        #y_ij_pred = func(x_ij_probe)\r\n",
        "        y_ijs.append(y_ij_pred.numpy())\r\n",
        "        y_ij_grads.append(y_ij_grad.numpy()[:, i, j])\r\n",
        "\r\n",
        "    # plot\r\n",
        "    fig = plt.figure(figsize=(14, 10))\r\n",
        "    for k in range(4):\r\n",
        "        idx = np.random.randint(0, len(Is))\r\n",
        "        i = Is[idx]\r\n",
        "        j = Js[idx]\r\n",
        "        ax = fig.add_subplot(2,2,k+1)\r\n",
        "        ax1 = ax.twinx()\r\n",
        "        title=\"i=%d, j=%d\"%(i, j)\r\n",
        "        figure_options = {'linewidth':2}\r\n",
        "\r\n",
        "        c, c1 = 'blue', 'red'\r\n",
        "        ax.plot(x_probe, y_ijs[idx], color=c, label='$f(x)$',**figure_options)\r\n",
        "        ax.tick_params(axis='y', color=c, labelcolor=c)\r\n",
        "        ax.legend(loc='upper right', fontsize=15)\r\n",
        "        \r\n",
        "        ax1.plot(x_probe, y_ij_grads[idx], color=c1, label=\"$\\\\nabla f_{ij}$\", **figure_options)\r\n",
        "        ax1.tick_params(axis='y',color=c1, labelcolor=c1)\r\n",
        "        ax1.legend(loc='lower left', fontsize=15)\r\n",
        "\r\n",
        "        ax.set_title(title, fontsize=15)\r\n",
        "    fig.tight_layout()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYBEWe-SqBDo"
      },
      "source": [
        "#plot_f_and_grad(distiller.teacher)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfcn7FDxKmlD"
      },
      "source": [
        "#plot_f_and_grad(distiller.student)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43FEwHoLwI1I"
      },
      "source": [
        "## Train a simple student model from scratch without distillation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0HgMTuQoSNj",
        "outputId": "9236384f-0bd8-4cde-b07e-4902547c64b4"
      },
      "source": [
        "# train a deep bind model by itself \r\n",
        "deepbind_model = get_student_model(L, A)\r\n",
        "\r\n",
        "# compile the model \r\n",
        "lossfn = tfk.losses.BinaryCrossentropy(name='bce')\r\n",
        "modelmetrics = [tfk.metrics.BinaryAccuracy(name='ACC'), tfk.metrics.AUC(curve='PR', name='AUPR'), tfk.metrics.AUC(curve='ROC', name='AUROC')]\r\n",
        "optimizer = tfk.optimizers.Adam(learning_rate=1e-2)\r\n",
        "deepbind_model.compile(loss=lossfn, metrics=modelmetrics, optimizer=optimizer)\r\n",
        "\r\n",
        "# fit the teacher model \r\n",
        "num_epochs = 100\r\n",
        "callbacks = [tfk.callbacks.EarlyStopping(monitor='val_AUROC', patience=20), \r\n",
        "             tfk.callbacks.ModelCheckpoint(\"best_deepbind_model.hdf5\", monitor='val_AUROC', mode='max', save_best_only=True)]\r\n",
        "deepbind_model.fit(x_train, y_train, \r\n",
        "                    epochs=num_epochs, \r\n",
        "                    batch_size=128, \r\n",
        "                    callbacks=callbacks, \r\n",
        "                    shuffle=True, \r\n",
        "                    initial_epoch=0,\r\n",
        "                    validation_data=(x_valid, y_valid))\r\n",
        "deepbind_model = tfk.models.load_model('best_deepbind_model.hdf5')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "110/110 [==============================] - 2s 9ms/step - loss: 0.6741 - ACC: 0.5728 - AUPR: 0.5963 - AUROC: 0.6033 - val_loss: 0.5250 - val_ACC: 0.7675 - val_AUPR: 0.8473 - val_AUROC: 0.8457\n",
            "Epoch 2/100\n",
            "110/110 [==============================] - 1s 5ms/step - loss: 0.5588 - ACC: 0.7125 - AUPR: 0.7884 - AUROC: 0.7851 - val_loss: 0.4632 - val_ACC: 0.8020 - val_AUPR: 0.8865 - val_AUROC: 0.8869\n",
            "Epoch 3/100\n",
            "110/110 [==============================] - 1s 5ms/step - loss: 0.5097 - ACC: 0.7488 - AUPR: 0.8319 - AUROC: 0.8288 - val_loss: 0.4430 - val_ACC: 0.8325 - val_AUPR: 0.9042 - val_AUROC: 0.9051\n",
            "Epoch 4/100\n",
            "110/110 [==============================] - 1s 5ms/step - loss: 0.4907 - ACC: 0.7590 - AUPR: 0.8442 - AUROC: 0.8424 - val_loss: 0.4078 - val_ACC: 0.8475 - val_AUPR: 0.9255 - val_AUROC: 0.9279\n",
            "Epoch 5/100\n",
            "110/110 [==============================] - 1s 5ms/step - loss: 0.4743 - ACC: 0.7745 - AUPR: 0.8552 - AUROC: 0.8540 - val_loss: 0.4137 - val_ACC: 0.8295 - val_AUPR: 0.9226 - val_AUROC: 0.9244\n",
            "Epoch 6/100\n",
            "110/110 [==============================] - 1s 5ms/step - loss: 0.4634 - ACC: 0.7782 - AUPR: 0.8676 - AUROC: 0.8615 - val_loss: 0.4083 - val_ACC: 0.8440 - val_AUPR: 0.9262 - val_AUROC: 0.9308\n",
            "Epoch 7/100\n",
            "110/110 [==============================] - 1s 5ms/step - loss: 0.4641 - ACC: 0.7693 - AUPR: 0.8626 - AUROC: 0.8598 - val_loss: 0.3968 - val_ACC: 0.8515 - val_AUPR: 0.9289 - val_AUROC: 0.9313\n",
            "Epoch 8/100\n",
            "110/110 [==============================] - 1s 5ms/step - loss: 0.4657 - ACC: 0.7754 - AUPR: 0.8596 - AUROC: 0.8596 - val_loss: 0.4023 - val_ACC: 0.8365 - val_AUPR: 0.9272 - val_AUROC: 0.9314\n",
            "Epoch 9/100\n",
            "110/110 [==============================] - 1s 5ms/step - loss: 0.4526 - ACC: 0.7832 - AUPR: 0.8704 - AUROC: 0.8677 - val_loss: 0.3763 - val_ACC: 0.8600 - val_AUPR: 0.9314 - val_AUROC: 0.9372\n",
            "Epoch 10/100\n",
            "110/110 [==============================] - 1s 5ms/step - loss: 0.4579 - ACC: 0.7829 - AUPR: 0.8687 - AUROC: 0.8650 - val_loss: 0.3810 - val_ACC: 0.8455 - val_AUPR: 0.9277 - val_AUROC: 0.9337\n",
            "Epoch 11/100\n",
            "110/110 [==============================] - 1s 5ms/step - loss: 0.4481 - ACC: 0.7869 - AUPR: 0.8756 - AUROC: 0.8712 - val_loss: 0.3738 - val_ACC: 0.8590 - val_AUPR: 0.9339 - val_AUROC: 0.9381\n",
            "Epoch 12/100\n",
            "110/110 [==============================] - 1s 5ms/step - loss: 0.4375 - ACC: 0.7896 - AUPR: 0.8819 - AUROC: 0.8777 - val_loss: 0.3788 - val_ACC: 0.8710 - val_AUPR: 0.9307 - val_AUROC: 0.9355\n",
            "Epoch 13/100\n",
            "110/110 [==============================] - 1s 5ms/step - loss: 0.4522 - ACC: 0.7899 - AUPR: 0.8720 - AUROC: 0.8691 - val_loss: 0.3632 - val_ACC: 0.8690 - val_AUPR: 0.9315 - val_AUROC: 0.9352\n",
            "Epoch 14/100\n",
            "110/110 [==============================] - 1s 5ms/step - loss: 0.4484 - ACC: 0.7824 - AUPR: 0.8755 - AUROC: 0.8706 - val_loss: 0.3813 - val_ACC: 0.8695 - val_AUPR: 0.9337 - val_AUROC: 0.9382\n",
            "Epoch 15/100\n",
            "110/110 [==============================] - 1s 5ms/step - loss: 0.4362 - ACC: 0.7936 - AUPR: 0.8831 - AUROC: 0.8785 - val_loss: 0.3806 - val_ACC: 0.8615 - val_AUPR: 0.9327 - val_AUROC: 0.9360\n",
            "Epoch 16/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4359 - ACC: 0.7931 - AUPR: 0.8801 - AUROC: 0.8785 - val_loss: 0.3805 - val_ACC: 0.8455 - val_AUPR: 0.9307 - val_AUROC: 0.9351\n",
            "Epoch 17/100\n",
            "110/110 [==============================] - 1s 5ms/step - loss: 0.4519 - ACC: 0.7833 - AUPR: 0.8747 - AUROC: 0.8686 - val_loss: 0.3696 - val_ACC: 0.8625 - val_AUPR: 0.9377 - val_AUROC: 0.9401\n",
            "Epoch 18/100\n",
            "110/110 [==============================] - 1s 5ms/step - loss: 0.4323 - ACC: 0.7994 - AUPR: 0.8820 - AUROC: 0.8819 - val_loss: 0.3656 - val_ACC: 0.8600 - val_AUPR: 0.9358 - val_AUROC: 0.9398\n",
            "Epoch 19/100\n",
            "110/110 [==============================] - 1s 5ms/step - loss: 0.4412 - ACC: 0.7907 - AUPR: 0.8795 - AUROC: 0.8756 - val_loss: 0.3971 - val_ACC: 0.8420 - val_AUPR: 0.9359 - val_AUROC: 0.9413\n",
            "Epoch 20/100\n",
            "110/110 [==============================] - 1s 5ms/step - loss: 0.4407 - ACC: 0.7868 - AUPR: 0.8769 - AUROC: 0.8761 - val_loss: 0.3733 - val_ACC: 0.8635 - val_AUPR: 0.9354 - val_AUROC: 0.9395\n",
            "Epoch 21/100\n",
            "110/110 [==============================] - 1s 5ms/step - loss: 0.4379 - ACC: 0.7906 - AUPR: 0.8770 - AUROC: 0.8780 - val_loss: 0.3698 - val_ACC: 0.8555 - val_AUPR: 0.9346 - val_AUROC: 0.9401\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_LNYVnewPy7"
      },
      "source": [
        "## Compute metrics on all 3 models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "3ovGuCNkuXuv",
        "outputId": "a7287a00-d119-443d-a97a-61dde0ec450f"
      },
      "source": [
        "distilled_student_metrics = distiller.evaluate(x_test, y_test, verbose=False)\r\n",
        "teacher_metrics = distiller.teacher.evaluate(x_test, y_test, verbose=False)\r\n",
        "deepbind_from_scratch_metrics = deepbind_model.evaluate(x_test, y_test, verbose=False)\r\n",
        "names = deepbind_model.metrics_names\r\n",
        "df = pd.DataFrame(data={'Name':names, 'Student (distilled)':distilled_student_metrics, 'Student (from scratch)':deepbind_from_scratch_metrics, 'Teacher ':teacher_metrics})\r\n",
        "df"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Student (distilled)</th>\n",
              "      <th>Student (from scratch)</th>\n",
              "      <th>Teacher</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>loss</td>\n",
              "      <td>0.338246</td>\n",
              "      <td>0.390032</td>\n",
              "      <td>0.255308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ACC</td>\n",
              "      <td>0.879750</td>\n",
              "      <td>0.847750</td>\n",
              "      <td>0.915000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AUPR</td>\n",
              "      <td>0.950392</td>\n",
              "      <td>0.944664</td>\n",
              "      <td>0.978242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AUROC</td>\n",
              "      <td>0.949496</td>\n",
              "      <td>0.945795</td>\n",
              "      <td>0.979731</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Name  Student (distilled)  Student (from scratch)  Teacher \n",
              "0   loss             0.338246                0.390032  0.255308\n",
              "1    ACC             0.879750                0.847750  0.915000\n",
              "2   AUPR             0.950392                0.944664  0.978242\n",
              "3  AUROC             0.949496                0.945795  0.979731"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_QRpUulvAfV"
      },
      "source": [
        "df.to_csv('modelmetrics-multihead')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD54SNjNo3Kx"
      },
      "source": [
        "##Run experiment on varying alpha and temperature values\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPsxCNE-jHe1",
        "outputId": "0740f998-e8e3-44bd-8cda-39cc1f7eb289"
      },
      "source": [
        "alpha_values = [.1,.2,.3,.4,.5,.6,.7,.8,.9]\r\n",
        "temperature_values = [1.,2.,3.,4.,5.]\r\n",
        "\r\n",
        "experiment_data=[]\r\n",
        "\r\n",
        "\r\n",
        "for alpha_value in alpha_values:\r\n",
        "  for temperature_value in temperature_values:\r\n",
        "    print('Alpha: ',alpha_value, ', Temperature: ',temperature_value)\r\n",
        "    # instantiate the student model and the distiller\r\n",
        "    student_model = get_student_model(L, A)\r\n",
        "    distiller = Distiller(student_model, teacher_model)\r\n",
        "    # compile the distiller\r\n",
        "    alpha = alpha_value\r\n",
        "    temperature = temperature_value\r\n",
        "    distiller.compile(\r\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\r\n",
        "        metrics=modelmetrics,\r\n",
        "        student_loss_fn=keras.losses.BinaryCrossentropy(name='bce'),\r\n",
        "        distillation_loss_fn=keras.losses.KLDivergence(),\r\n",
        "        alpha=alpha,\r\n",
        "        temperature=temperature,\r\n",
        "    )\r\n",
        "\r\n",
        "    # perform distillation\r\n",
        "    num_epochs = 50\r\n",
        "    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_AUROC', patience=20), \r\n",
        "                tfk.callbacks.ModelCheckpoint(\"best_distiller.hdf5\", monitor='val_AUROC', mode='max',save_weights_only=True, save_best_only=True)]\r\n",
        "    distiller.fit(x_train, y_train, \r\n",
        "                    epochs=num_epochs, \r\n",
        "                    batch_size=128, \r\n",
        "                    callbacks=callbacks, \r\n",
        "                    shuffle=True, \r\n",
        "                    validation_data=(x_valid, y_valid),\r\n",
        "                    verbose=0)\r\n",
        "    \r\n",
        "    #evaluate and save Distilled metrics\r\n",
        "    experiment_dist_student_metrics = distiller.evaluate(x_test, y_test, verbose=False)\r\n",
        "    hyperparameters = [alpha_value, temperature_value]\r\n",
        "    all_values = hyperparameters+experiment_dist_student_metrics\r\n",
        "    #add data to list\r\n",
        "    experiment_data.append(all_values)\r\n",
        "\r\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Alpha:  0.1 , Temperature:  1.0\n",
            "Alpha:  0.1 , Temperature:  2.0\n",
            "Alpha:  0.1 , Temperature:  3.0\n",
            "Alpha:  0.1 , Temperature:  4.0\n",
            "Alpha:  0.1 , Temperature:  5.0\n",
            "Alpha:  0.2 , Temperature:  1.0\n",
            "Alpha:  0.2 , Temperature:  2.0\n",
            "Alpha:  0.2 , Temperature:  3.0\n",
            "Alpha:  0.2 , Temperature:  4.0\n",
            "Alpha:  0.2 , Temperature:  5.0\n",
            "Alpha:  0.3 , Temperature:  1.0\n",
            "Alpha:  0.3 , Temperature:  2.0\n",
            "Alpha:  0.3 , Temperature:  3.0\n",
            "Alpha:  0.3 , Temperature:  4.0\n",
            "Alpha:  0.3 , Temperature:  5.0\n",
            "Alpha:  0.4 , Temperature:  1.0\n",
            "Alpha:  0.4 , Temperature:  2.0\n",
            "Alpha:  0.4 , Temperature:  3.0\n",
            "Alpha:  0.4 , Temperature:  4.0\n",
            "Alpha:  0.4 , Temperature:  5.0\n",
            "Alpha:  0.5 , Temperature:  1.0\n",
            "Alpha:  0.5 , Temperature:  2.0\n",
            "Alpha:  0.5 , Temperature:  3.0\n",
            "Alpha:  0.5 , Temperature:  4.0\n",
            "Alpha:  0.5 , Temperature:  5.0\n",
            "Alpha:  0.6 , Temperature:  1.0\n",
            "Alpha:  0.6 , Temperature:  2.0\n",
            "Alpha:  0.6 , Temperature:  3.0\n",
            "Alpha:  0.6 , Temperature:  4.0\n",
            "Alpha:  0.6 , Temperature:  5.0\n",
            "Alpha:  0.7 , Temperature:  1.0\n",
            "Alpha:  0.7 , Temperature:  2.0\n",
            "Alpha:  0.7 , Temperature:  3.0\n",
            "Alpha:  0.7 , Temperature:  4.0\n",
            "Alpha:  0.7 , Temperature:  5.0\n",
            "Alpha:  0.8 , Temperature:  1.0\n",
            "Alpha:  0.8 , Temperature:  2.0\n",
            "Alpha:  0.8 , Temperature:  3.0\n",
            "Alpha:  0.8 , Temperature:  4.0\n",
            "Alpha:  0.8 , Temperature:  5.0\n",
            "Alpha:  0.9 , Temperature:  1.0\n",
            "Alpha:  0.9 , Temperature:  2.0\n",
            "Alpha:  0.9 , Temperature:  3.0\n",
            "Alpha:  0.9 , Temperature:  4.0\n",
            "Alpha:  0.9 , Temperature:  5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bQw21PwHm9Sv",
        "outputId": "b9225752-d610-4c38-9a85-b1ef48104d75"
      },
      "source": [
        "#put results into a data table\r\n",
        "df = pd.DataFrame(experiment_data)\r\n",
        "columns = ['alpha', 'temperature', 'loss', 'ACC', 'AUPR', 'AUROC']\r\n",
        "df.columns=columns\r\n",
        "df\r\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>alpha</th>\n",
              "      <th>temperature</th>\n",
              "      <th>loss</th>\n",
              "      <th>ACC</th>\n",
              "      <th>AUPR</th>\n",
              "      <th>AUROC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.407842</td>\n",
              "      <td>0.86200</td>\n",
              "      <td>0.938746</td>\n",
              "      <td>0.939499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.395727</td>\n",
              "      <td>0.86075</td>\n",
              "      <td>0.942073</td>\n",
              "      <td>0.940393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.337760</td>\n",
              "      <td>0.85325</td>\n",
              "      <td>0.936509</td>\n",
              "      <td>0.935197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.365863</td>\n",
              "      <td>0.88950</td>\n",
              "      <td>0.956928</td>\n",
              "      <td>0.957002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.1</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.357643</td>\n",
              "      <td>0.88525</td>\n",
              "      <td>0.956552</td>\n",
              "      <td>0.955474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.366504</td>\n",
              "      <td>0.86175</td>\n",
              "      <td>0.937096</td>\n",
              "      <td>0.937519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.321979</td>\n",
              "      <td>0.86475</td>\n",
              "      <td>0.941821</td>\n",
              "      <td>0.942764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.328398</td>\n",
              "      <td>0.86025</td>\n",
              "      <td>0.958070</td>\n",
              "      <td>0.957553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.2</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.389909</td>\n",
              "      <td>0.87375</td>\n",
              "      <td>0.943598</td>\n",
              "      <td>0.942831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.2</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.362833</td>\n",
              "      <td>0.86925</td>\n",
              "      <td>0.942623</td>\n",
              "      <td>0.943344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.382096</td>\n",
              "      <td>0.86725</td>\n",
              "      <td>0.943903</td>\n",
              "      <td>0.942404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.3</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.453202</td>\n",
              "      <td>0.87875</td>\n",
              "      <td>0.948580</td>\n",
              "      <td>0.948023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.404453</td>\n",
              "      <td>0.84475</td>\n",
              "      <td>0.924036</td>\n",
              "      <td>0.926241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.385752</td>\n",
              "      <td>0.85325</td>\n",
              "      <td>0.932031</td>\n",
              "      <td>0.930419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.3</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.405830</td>\n",
              "      <td>0.85825</td>\n",
              "      <td>0.933911</td>\n",
              "      <td>0.932151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.375577</td>\n",
              "      <td>0.88175</td>\n",
              "      <td>0.952780</td>\n",
              "      <td>0.951881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.405216</td>\n",
              "      <td>0.86875</td>\n",
              "      <td>0.943446</td>\n",
              "      <td>0.942789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.4</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.410259</td>\n",
              "      <td>0.83950</td>\n",
              "      <td>0.917472</td>\n",
              "      <td>0.915608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.4</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.379560</td>\n",
              "      <td>0.85975</td>\n",
              "      <td>0.943983</td>\n",
              "      <td>0.942569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.4</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.386663</td>\n",
              "      <td>0.85950</td>\n",
              "      <td>0.938736</td>\n",
              "      <td>0.938171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.287182</td>\n",
              "      <td>0.83850</td>\n",
              "      <td>0.955657</td>\n",
              "      <td>0.955709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.329198</td>\n",
              "      <td>0.87425</td>\n",
              "      <td>0.949891</td>\n",
              "      <td>0.948309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.333429</td>\n",
              "      <td>0.85025</td>\n",
              "      <td>0.938284</td>\n",
              "      <td>0.936411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.390801</td>\n",
              "      <td>0.87675</td>\n",
              "      <td>0.950050</td>\n",
              "      <td>0.951041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.433702</td>\n",
              "      <td>0.86275</td>\n",
              "      <td>0.940030</td>\n",
              "      <td>0.939334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.391612</td>\n",
              "      <td>0.86600</td>\n",
              "      <td>0.945554</td>\n",
              "      <td>0.943679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.6</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.348114</td>\n",
              "      <td>0.86350</td>\n",
              "      <td>0.945068</td>\n",
              "      <td>0.945125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.6</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.455167</td>\n",
              "      <td>0.85375</td>\n",
              "      <td>0.929765</td>\n",
              "      <td>0.931129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.6</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.495722</td>\n",
              "      <td>0.85875</td>\n",
              "      <td>0.949861</td>\n",
              "      <td>0.948805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.6</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.338305</td>\n",
              "      <td>0.87375</td>\n",
              "      <td>0.943051</td>\n",
              "      <td>0.943481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.354001</td>\n",
              "      <td>0.86700</td>\n",
              "      <td>0.949960</td>\n",
              "      <td>0.949712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.7</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.357170</td>\n",
              "      <td>0.85350</td>\n",
              "      <td>0.946214</td>\n",
              "      <td>0.946801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.7</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.398278</td>\n",
              "      <td>0.86375</td>\n",
              "      <td>0.940602</td>\n",
              "      <td>0.938155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.7</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.346442</td>\n",
              "      <td>0.88800</td>\n",
              "      <td>0.957171</td>\n",
              "      <td>0.956884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.7</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.518518</td>\n",
              "      <td>0.84650</td>\n",
              "      <td>0.928399</td>\n",
              "      <td>0.927995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.359650</td>\n",
              "      <td>0.86925</td>\n",
              "      <td>0.943144</td>\n",
              "      <td>0.943354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.8</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.350567</td>\n",
              "      <td>0.88075</td>\n",
              "      <td>0.950451</td>\n",
              "      <td>0.950640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.8</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.353553</td>\n",
              "      <td>0.85875</td>\n",
              "      <td>0.940328</td>\n",
              "      <td>0.939470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.8</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.338763</td>\n",
              "      <td>0.87450</td>\n",
              "      <td>0.947226</td>\n",
              "      <td>0.946268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.8</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.357215</td>\n",
              "      <td>0.84475</td>\n",
              "      <td>0.944688</td>\n",
              "      <td>0.943794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.357099</td>\n",
              "      <td>0.85825</td>\n",
              "      <td>0.932610</td>\n",
              "      <td>0.931242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.9</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.399670</td>\n",
              "      <td>0.85075</td>\n",
              "      <td>0.937592</td>\n",
              "      <td>0.935810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.360419</td>\n",
              "      <td>0.86925</td>\n",
              "      <td>0.944983</td>\n",
              "      <td>0.943205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.9</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.472512</td>\n",
              "      <td>0.85000</td>\n",
              "      <td>0.933488</td>\n",
              "      <td>0.931145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.9</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.487575</td>\n",
              "      <td>0.85675</td>\n",
              "      <td>0.931208</td>\n",
              "      <td>0.932427</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    alpha  temperature      loss      ACC      AUPR     AUROC\n",
              "0     0.1          1.0  0.407842  0.86200  0.938746  0.939499\n",
              "1     0.1          2.0  0.395727  0.86075  0.942073  0.940393\n",
              "2     0.1          3.0  0.337760  0.85325  0.936509  0.935197\n",
              "3     0.1          4.0  0.365863  0.88950  0.956928  0.957002\n",
              "4     0.1          5.0  0.357643  0.88525  0.956552  0.955474\n",
              "5     0.2          1.0  0.366504  0.86175  0.937096  0.937519\n",
              "6     0.2          2.0  0.321979  0.86475  0.941821  0.942764\n",
              "7     0.2          3.0  0.328398  0.86025  0.958070  0.957553\n",
              "8     0.2          4.0  0.389909  0.87375  0.943598  0.942831\n",
              "9     0.2          5.0  0.362833  0.86925  0.942623  0.943344\n",
              "10    0.3          1.0  0.382096  0.86725  0.943903  0.942404\n",
              "11    0.3          2.0  0.453202  0.87875  0.948580  0.948023\n",
              "12    0.3          3.0  0.404453  0.84475  0.924036  0.926241\n",
              "13    0.3          4.0  0.385752  0.85325  0.932031  0.930419\n",
              "14    0.3          5.0  0.405830  0.85825  0.933911  0.932151\n",
              "15    0.4          1.0  0.375577  0.88175  0.952780  0.951881\n",
              "16    0.4          2.0  0.405216  0.86875  0.943446  0.942789\n",
              "17    0.4          3.0  0.410259  0.83950  0.917472  0.915608\n",
              "18    0.4          4.0  0.379560  0.85975  0.943983  0.942569\n",
              "19    0.4          5.0  0.386663  0.85950  0.938736  0.938171\n",
              "20    0.5          1.0  0.287182  0.83850  0.955657  0.955709\n",
              "21    0.5          2.0  0.329198  0.87425  0.949891  0.948309\n",
              "22    0.5          3.0  0.333429  0.85025  0.938284  0.936411\n",
              "23    0.5          4.0  0.390801  0.87675  0.950050  0.951041\n",
              "24    0.5          5.0  0.433702  0.86275  0.940030  0.939334\n",
              "25    0.6          1.0  0.391612  0.86600  0.945554  0.943679\n",
              "26    0.6          2.0  0.348114  0.86350  0.945068  0.945125\n",
              "27    0.6          3.0  0.455167  0.85375  0.929765  0.931129\n",
              "28    0.6          4.0  0.495722  0.85875  0.949861  0.948805\n",
              "29    0.6          5.0  0.338305  0.87375  0.943051  0.943481\n",
              "30    0.7          1.0  0.354001  0.86700  0.949960  0.949712\n",
              "31    0.7          2.0  0.357170  0.85350  0.946214  0.946801\n",
              "32    0.7          3.0  0.398278  0.86375  0.940602  0.938155\n",
              "33    0.7          4.0  0.346442  0.88800  0.957171  0.956884\n",
              "34    0.7          5.0  0.518518  0.84650  0.928399  0.927995\n",
              "35    0.8          1.0  0.359650  0.86925  0.943144  0.943354\n",
              "36    0.8          2.0  0.350567  0.88075  0.950451  0.950640\n",
              "37    0.8          3.0  0.353553  0.85875  0.940328  0.939470\n",
              "38    0.8          4.0  0.338763  0.87450  0.947226  0.946268\n",
              "39    0.8          5.0  0.357215  0.84475  0.944688  0.943794\n",
              "40    0.9          1.0  0.357099  0.85825  0.932610  0.931242\n",
              "41    0.9          2.0  0.399670  0.85075  0.937592  0.935810\n",
              "42    0.9          3.0  0.360419  0.86925  0.944983  0.943205\n",
              "43    0.9          4.0  0.472512  0.85000  0.933488  0.931145\n",
              "44    0.9          5.0  0.487575  0.85675  0.931208  0.932427"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32LbJfGuuC_m"
      },
      "source": [
        "df.to_csv('performancemetrics-multihead')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "e5G23QDkHlqS",
        "outputId": "d7f37bd8-4f3e-461b-b4d5-b136301f69cf"
      },
      "source": [
        "#find best performing combination\r\n",
        "max_metric = experiment_data[0]\r\n",
        "for metrics in experiment_data:\r\n",
        "  if metrics[3]>max_metric[3]:\r\n",
        "    max_metric = metrics\r\n",
        "\r\n",
        "mf = pd.DataFrame(data={'Name':columns, 'Best Performance':max_metric})\r\n",
        "mf"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Best Performance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>alpha</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>temperature</td>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>loss</td>\n",
              "      <td>0.365863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ACC</td>\n",
              "      <td>0.889500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AUPR</td>\n",
              "      <td>0.956928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>AUROC</td>\n",
              "      <td>0.957002</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Name  Best Performance\n",
              "0        alpha          0.100000\n",
              "1  temperature          4.000000\n",
              "2         loss          0.365863\n",
              "3          ACC          0.889500\n",
              "4         AUPR          0.956928\n",
              "5        AUROC          0.957002"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    }
  ]
}