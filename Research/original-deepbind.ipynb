{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of cnn_dist.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHecrF91fJ3h"
      },
      "source": [
        "import numpy as np, os\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras as tfk\r\n",
        "keras = tfk\r\n",
        "import datetime as dt\r\n",
        "import six\r\n",
        "import h5py\r\n",
        "import requests\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt \r\n",
        "import seaborn as sns\r\n",
        "sns.set()\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3psP7yOQIVD"
      },
      "source": [
        "# Data loading "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKPa5-b4L7hX"
      },
      "source": [
        "# download the data \r\n",
        "url = 'https://www.dropbox.com/s/ysrim2re8mh22z9/synthetic_code_dataset.h5?dl=0'\r\n",
        "save_name = 'data.h5'\r\n",
        "_=!wget {url} -O {save_name}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCvBWh9oNYhd"
      },
      "source": [
        "# load the data into x_train, y_train, .....\r\n",
        "f = h5py.File(save_name, 'r')\r\n",
        "suffixes = ['train', 'test', 'valid']\r\n",
        "for suffix in suffixes:\r\n",
        "    exec(\"x_%s=np.transpose(f.get(\\\"X_%s\\\")[:], (0, 2, 1))\"%(suffix, suffix))\r\n",
        "    exec(\"y_%s=f.get(\\\"Y_%s\\\")[:]\"%(suffix, suffix))\r\n",
        "f.close()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISQFZ12vQKeu"
      },
      "source": [
        "# Model definition function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5qiwx1sfthV"
      },
      "source": [
        "def get_activation(activation = 'relu'):\r\n",
        "    \"\"\"\r\n",
        "    Create an activation function. The activation argument should one of:\r\n",
        "    1. A string representing the keras name of the activation. \r\n",
        "    2. A callable which may or may not be an instance of keras.layers.Layer. \r\n",
        "    \"\"\"\r\n",
        "    if isinstance(activation, str):\r\n",
        "        actfn = tfk.layers.Activation(activation)\r\n",
        "    else:\r\n",
        "        if callable(activation) and not isinstance(activation, tfk.layers.Layer):\r\n",
        "            actfn = tfk.layers.Activation(activation)\r\n",
        "        else:\r\n",
        "            actfn = activation\r\n",
        "    return actfn"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baDorqePRC-5"
      },
      "source": [
        "def conv_layer(x, num_filters, kernel_size, padding, activation, dropout=0.5, l2=1e-6, bn=True): \r\n",
        "    \"\"\"\r\n",
        "    A convolutional block comprising of a convolutional layer followed by\r\n",
        "    batch normalization, an activation function, and dropout. \r\n",
        "    \"\"\"\r\n",
        "    y = tfk.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, kernel_regularizer=tfk.regularizers.l2(l2), padding=padding)(x)\r\n",
        "    if bn:\r\n",
        "        y = tfk.layers.BatchNormalization()(y)\r\n",
        "    actfn = get_activation(activation)\r\n",
        "    y = actfn(y)\r\n",
        "    if dropout:\r\n",
        "        y = tfk.layers.Dropout(dropout)(y)\r\n",
        "    return y\r\n",
        "\r\n",
        "def dense_layer(x, num_units, activation, dropout=0.5, l2=None, bn=True):\r\n",
        "    \"\"\"\r\n",
        "    A dense block comprising of a dense layer followed by batch normalization, \r\n",
        "    activation and dropout. \r\n",
        "    \"\"\"\r\n",
        "    y = tfk.layers.Dense(num_units, use_bias=False, kernel_regularizer=tfk.regularizers.l2(l2))(x)\r\n",
        "    if bn:\r\n",
        "        y = tfk.layers.BatchNormalization()(y)\r\n",
        "    actfn = get_activation(activation)\r\n",
        "    y = actfn(y)\r\n",
        "    if dropout:\r\n",
        "        y = tfk.layers.Dropout(dropout)(y)\r\n",
        "    return y\r\n",
        "\r\n",
        "def get_model(L, A, activation='relu', name='cnn_dist'):\r\n",
        "    \"\"\"\r\n",
        "    A function to assemble the full CNN distributed model. \r\n",
        "    \"\"\"\r\n",
        "    # input layer \r\n",
        "    x = tfk.layers.Input((L, A), name='input')\r\n",
        "\r\n",
        "    # 1st convolutional block \r\n",
        "    y = conv_layer(x,num_filters=24, kernel_size=19, padding='same', dropout=0.1,l2=1e-6, bn=True, activation=activation)\r\n",
        "    \r\n",
        "    # 2nd conv. block + pooling \r\n",
        "    y = conv_layer(y,num_filters=32, kernel_size=7, padding='same', activation=activation, dropout=0.2,l2=1e-6, bn=True)\r\n",
        "    y = tfk.layers.MaxPool1D(pool_size=4)(y)\r\n",
        "    \r\n",
        "    # 3rd convolutional block + pooling \r\n",
        "    y = conv_layer(y,num_filters=64, kernel_size=3, padding='same', activation=activation, dropout=0.4,l2=1e-6, bn=True)\r\n",
        "    y = tfk.layers.MaxPool1D(pool_size=3, strides=3, padding='same')(y)\r\n",
        "    \r\n",
        "    # dense block and final output layer \r\n",
        "    y = tfk.layers.Flatten()(y)\r\n",
        "    y = dense_layer(y, num_units=96, activation=activation, dropout=0.5, l2=1e-6, bn=True)\r\n",
        "    y = tfk.layers.Dense(1, use_bias=True, name = 'logits')(y)\r\n",
        "    y = tfk.layers.Activation('sigmoid')(y)\r\n",
        "\r\n",
        "    # assemble full model\r\n",
        "    model = tfk.Model(x, y, name=name)\r\n",
        "    return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQzKg97ayWQ-"
      },
      "source": [
        "# Train a teacher model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUkdVPMdifWD",
        "outputId": "45bba1e7-4d11-496d-f2ef-7d85d9d98fc1"
      },
      "source": [
        "# instantiate the teacher model \r\n",
        "activation = 'relu' \r\n",
        "#activation = lambda x : tf.math.sin(x) + tf.math.cos(x)\r\n",
        "L, A = x_train.shape[1:]\r\n",
        "teacher_model = get_model(L, A, activation, name='teacher')\r\n",
        "\r\n",
        "# compile the teacher model \r\n",
        "lossfn = tfk.losses.BinaryCrossentropy(name='bce')\r\n",
        "modelmetrics = [tfk.metrics.BinaryAccuracy(name='ACC'), tfk.metrics.AUC(curve='PR', name='AUPR'), tfk.metrics.AUC(curve='ROC', name='AUROC')]\r\n",
        "optimizer = tfk.optimizers.Adam(learning_rate=1e-2)\r\n",
        "teacher_model.compile(loss=lossfn, metrics=modelmetrics, optimizer=optimizer)\r\n",
        "\r\n",
        "# fit the teacher model \r\n",
        "num_epochs = 100\r\n",
        "callbacks = [tfk.callbacks.EarlyStopping(monitor='val_AUROC', patience=20), \r\n",
        "             tfk.callbacks.ModelCheckpoint(\"best_teacher_model.hdf5\", monitor='val_AUROC', mode='max', save_best_only=True)]\r\n",
        "teacher_model.fit(x_train, y_train, \r\n",
        "                    epochs=num_epochs, \r\n",
        "                    batch_size=128, \r\n",
        "                    callbacks=callbacks, \r\n",
        "                    shuffle=True, \r\n",
        "                    initial_epoch=0,\r\n",
        "                    validation_data=(x_valid, y_valid))\r\n",
        "teacher_model = tfk.models.load_model('best_teacher_model.hdf5')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "110/110 [==============================] - 11s 16ms/step - loss: 0.6348 - ACC: 0.6691 - AUPR: 0.7127 - AUROC: 0.7283 - val_loss: 1.3705 - val_ACC: 0.5065 - val_AUPR: 0.8352 - val_AUROC: 0.8478\n",
            "Epoch 2/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.4644 - ACC: 0.7802 - AUPR: 0.8586 - AUROC: 0.8623 - val_loss: 0.7399 - val_ACC: 0.5770 - val_AUPR: 0.8889 - val_AUROC: 0.8914\n",
            "Epoch 3/100\n",
            "110/110 [==============================] - 1s 9ms/step - loss: 0.4109 - ACC: 0.8111 - AUPR: 0.8915 - AUROC: 0.8953 - val_loss: 1.1271 - val_ACC: 0.5365 - val_AUPR: 0.8936 - val_AUROC: 0.9156\n",
            "Epoch 4/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.3456 - ACC: 0.8535 - AUPR: 0.9251 - AUROC: 0.9270 - val_loss: 0.3193 - val_ACC: 0.8685 - val_AUPR: 0.9496 - val_AUROC: 0.9555\n",
            "Epoch 5/100\n",
            "110/110 [==============================] - 1s 10ms/step - loss: 0.2826 - ACC: 0.8808 - AUPR: 0.9503 - AUROC: 0.9517 - val_loss: 0.2530 - val_ACC: 0.9015 - val_AUPR: 0.9570 - val_AUROC: 0.9626\n",
            "Epoch 6/100\n",
            "110/110 [==============================] - 1s 9ms/step - loss: 0.2455 - ACC: 0.8980 - AUPR: 0.9610 - AUROC: 0.9638 - val_loss: 0.2928 - val_ACC: 0.8790 - val_AUPR: 0.9606 - val_AUROC: 0.9665\n",
            "Epoch 7/100\n",
            "110/110 [==============================] - 1s 10ms/step - loss: 0.2242 - ACC: 0.9106 - AUPR: 0.9679 - AUROC: 0.9695 - val_loss: 0.4220 - val_ACC: 0.8190 - val_AUPR: 0.9686 - val_AUROC: 0.9702\n",
            "Epoch 8/100\n",
            "110/110 [==============================] - 1s 10ms/step - loss: 0.2054 - ACC: 0.9227 - AUPR: 0.9729 - AUROC: 0.9744 - val_loss: 0.2293 - val_ACC: 0.9075 - val_AUPR: 0.9689 - val_AUROC: 0.9732\n",
            "Epoch 9/100\n",
            "110/110 [==============================] - 1s 9ms/step - loss: 0.2020 - ACC: 0.9222 - AUPR: 0.9747 - AUROC: 0.9756 - val_loss: 0.2220 - val_ACC: 0.9155 - val_AUPR: 0.9719 - val_AUROC: 0.9756\n",
            "Epoch 10/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.1953 - ACC: 0.9250 - AUPR: 0.9759 - AUROC: 0.9769 - val_loss: 0.2338 - val_ACC: 0.9075 - val_AUPR: 0.9709 - val_AUROC: 0.9760\n",
            "Epoch 11/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.1755 - ACC: 0.9293 - AUPR: 0.9813 - AUROC: 0.9820 - val_loss: 0.1905 - val_ACC: 0.9240 - val_AUPR: 0.9743 - val_AUROC: 0.9779\n",
            "Epoch 12/100\n",
            "110/110 [==============================] - 1s 10ms/step - loss: 0.1821 - ACC: 0.9274 - AUPR: 0.9797 - AUROC: 0.9804 - val_loss: 0.2298 - val_ACC: 0.9130 - val_AUPR: 0.9697 - val_AUROC: 0.9756\n",
            "Epoch 13/100\n",
            "110/110 [==============================] - 1s 9ms/step - loss: 0.1698 - ACC: 0.9346 - AUPR: 0.9827 - AUROC: 0.9828 - val_loss: 0.2899 - val_ACC: 0.8830 - val_AUPR: 0.9744 - val_AUROC: 0.9767\n",
            "Epoch 14/100\n",
            "110/110 [==============================] - 1s 9ms/step - loss: 0.1744 - ACC: 0.9328 - AUPR: 0.9820 - AUROC: 0.9822 - val_loss: 0.1903 - val_ACC: 0.9320 - val_AUPR: 0.9774 - val_AUROC: 0.9807\n",
            "Epoch 15/100\n",
            "110/110 [==============================] - 1s 10ms/step - loss: 0.1617 - ACC: 0.9371 - AUPR: 0.9836 - AUROC: 0.9847 - val_loss: 0.1782 - val_ACC: 0.9360 - val_AUPR: 0.9763 - val_AUROC: 0.9804\n",
            "Epoch 16/100\n",
            "110/110 [==============================] - 1s 9ms/step - loss: 0.1638 - ACC: 0.9391 - AUPR: 0.9837 - AUROC: 0.9845 - val_loss: 0.1890 - val_ACC: 0.9280 - val_AUPR: 0.9756 - val_AUROC: 0.9800\n",
            "Epoch 17/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.1538 - ACC: 0.9437 - AUPR: 0.9858 - AUROC: 0.9863 - val_loss: 0.3270 - val_ACC: 0.8800 - val_AUPR: 0.9675 - val_AUROC: 0.9773\n",
            "Epoch 18/100\n",
            "110/110 [==============================] - 1s 9ms/step - loss: 0.1645 - ACC: 0.9366 - AUPR: 0.9839 - AUROC: 0.9844 - val_loss: 0.1844 - val_ACC: 0.9315 - val_AUPR: 0.9770 - val_AUROC: 0.9813\n",
            "Epoch 19/100\n",
            "110/110 [==============================] - 1s 10ms/step - loss: 0.1514 - ACC: 0.9441 - AUPR: 0.9859 - AUROC: 0.9867 - val_loss: 0.2188 - val_ACC: 0.9210 - val_AUPR: 0.9755 - val_AUROC: 0.9787\n",
            "Epoch 20/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.1543 - ACC: 0.9413 - AUPR: 0.9863 - AUROC: 0.9864 - val_loss: 0.2048 - val_ACC: 0.9270 - val_AUPR: 0.9722 - val_AUROC: 0.9786\n",
            "Epoch 21/100\n",
            "110/110 [==============================] - 1s 10ms/step - loss: 0.1526 - ACC: 0.9400 - AUPR: 0.9852 - AUROC: 0.9866 - val_loss: 0.1963 - val_ACC: 0.9330 - val_AUPR: 0.9754 - val_AUROC: 0.9795\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kpvZ10Vvxcr"
      },
      "source": [
        "teacher_model = tfk.models.load_model('best_teacher_model.hdf5')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lep5Pun5E0k-"
      },
      "source": [
        "# Knowledge distillation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTehiZ5nGzou"
      },
      "source": [
        "## Define a `Distiller` class that takes in a trained teacher model, an untrained student model and distills the knowledge in the teacher model onto the student model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tai-epSfGkUz"
      },
      "source": [
        "class Distiller(keras.Model):\r\n",
        "    def get_config(self,):\r\n",
        "        \"\"\"\r\n",
        "        Implement the config dictionary to enable serialization\r\n",
        "        \"\"\"\r\n",
        "        config = {}\r\n",
        "        config['student'] = self.student\r\n",
        "        config['teacher'] = self.teacher\r\n",
        "        return config\r\n",
        "    \r\n",
        "    def __init__(self, student, teacher):\r\n",
        "        super(Distiller, self).__init__()\r\n",
        "        self.teacher = teacher\r\n",
        "        self.student = student\r\n",
        "\r\n",
        "    def compile(\r\n",
        "        self,\r\n",
        "        optimizer,\r\n",
        "        metrics,\r\n",
        "        student_loss_fn,\r\n",
        "        distillation_loss_fn,\r\n",
        "        alpha=0.1,\r\n",
        "        temperature=3,\r\n",
        "    ):\r\n",
        "        \"\"\" Configure the distiller.\r\n",
        "\r\n",
        "        Args:\r\n",
        "            optimizer: Keras optimizer for the student weights\r\n",
        "            metrics: Keras metrics for evaluation\r\n",
        "            student_loss_fn: Loss function of difference between student\r\n",
        "                predictions and ground-truth\r\n",
        "            distillation_loss_fn: Loss function of difference between soft\r\n",
        "                student predictions and soft teacher predictions\r\n",
        "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\r\n",
        "            temperature: Temperature for softening probability distributions.\r\n",
        "                Larger temperature gives softer distributions.\r\n",
        "        \"\"\"\r\n",
        "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\r\n",
        "        self.student_loss_fn = student_loss_fn\r\n",
        "        self.distillation_loss_fn = distillation_loss_fn\r\n",
        "        self.alpha = alpha\r\n",
        "        self.temperature = temperature\r\n",
        "\r\n",
        "    def train_step(self, data):\r\n",
        "        # Unpack data\r\n",
        "        x, y = data\r\n",
        "\r\n",
        "        # Forward pass of teacher\r\n",
        "        teacher_predictions = self.teacher(x, training=False)\r\n",
        "\r\n",
        "        with tf.GradientTape() as tape:\r\n",
        "            # Forward pass of student\r\n",
        "            student_predictions = self.student(x, training=True)\r\n",
        "\r\n",
        "            # Compute losses\r\n",
        "            student_loss = self.student_loss_fn(y, student_predictions)\r\n",
        "            distillation_loss = self.distillation_loss_fn(\r\n",
        "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\r\n",
        "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\r\n",
        "            )\r\n",
        "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\r\n",
        "\r\n",
        "        # Compute gradients\r\n",
        "        trainable_vars = self.student.trainable_variables\r\n",
        "        gradients = tape.gradient(loss, trainable_vars)\r\n",
        "\r\n",
        "        # Update weights\r\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\r\n",
        "\r\n",
        "        # Update the metrics configured in `compile()`.\r\n",
        "        self.compiled_metrics.update_state(y, student_predictions)\r\n",
        "\r\n",
        "        # Return a dict of performance\r\n",
        "        results = {m.name: m.result() for m in self.metrics}\r\n",
        "        results.update(\r\n",
        "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\r\n",
        "        )\r\n",
        "        return results\r\n",
        "\r\n",
        "    def test_step(self, data):\r\n",
        "        # Unpack the data\r\n",
        "        x, y = data\r\n",
        "\r\n",
        "        # Compute predictions\r\n",
        "        y_prediction = self.student(x, training=False)\r\n",
        "\r\n",
        "        # Calculate the loss\r\n",
        "        student_loss = self.student_loss_fn(y, y_prediction)\r\n",
        "\r\n",
        "        # Update the metrics.\r\n",
        "        self.compiled_metrics.update_state(y, y_prediction)\r\n",
        "\r\n",
        "        # Return a dict of performance\r\n",
        "        results = {\"student_loss\": student_loss}\r\n",
        "        results.update({m.name: m.result() for m in self.metrics})\r\n",
        "        return results\r\n",
        "    \r\n",
        "    @property\r\n",
        "    def metrics_names(self):\r\n",
        "        return ['student_loss']+[m.name for m in self.metrics]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvOT3OZDHCgu"
      },
      "source": [
        "def get_student_model(L, A, activation='relu', name='deepbind'):\r\n",
        "    \"\"\"\r\n",
        "    Defining the deepbind architecture in here. \r\n",
        "    \"\"\"\r\n",
        "    x = tfk.layers.Input((L, A), name='input')\r\n",
        "    y = tfk.layers.Conv1D(filters=16, kernel_size=24, padding='valid', kernel_regularizer=tfk.regularizers.l2(1e-6))(x)\r\n",
        "    actfn = get_activation(activation=activation)\r\n",
        "    y = actfn(y)\r\n",
        "    y = tfk.layers.Lambda(lambda x : tf.reduce_max(x, axis=1))(y)  # max pooling\r\n",
        "    y = tfk.layers.Dropout(0.5)(y)  \r\n",
        "    y = tfk.layers.Dense(32, activation='relu')(y)\r\n",
        "    y = tfk.layers.Dense(1, name='logits')(y)\r\n",
        "    y = tfk.layers.Activation('sigmoid', name='output')(y)\r\n",
        "\r\n",
        "    model = tfk.Model(inputs=x, outputs=y, name=name)\r\n",
        "    return model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K79gsB5RIhPV",
        "outputId": "86013223-5bd4-484b-8c42-460ef075a044"
      },
      "source": [
        "# instantiate the student model and the distiller \r\n",
        "student_model = get_student_model(L, A)\r\n",
        "distiller = Distiller(student_model, teacher_model)\r\n",
        "\r\n",
        "# compile the distiller\r\n",
        "alpha = 0.8\r\n",
        "temperature = 1. \r\n",
        "distiller.compile(\r\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\r\n",
        "    metrics=modelmetrics,\r\n",
        "    student_loss_fn=keras.losses.BinaryCrossentropy(name='bce'),\r\n",
        "    distillation_loss_fn=keras.losses.KLDivergence(),\r\n",
        "    alpha=alpha,\r\n",
        "    temperature=temperature,\r\n",
        ")\r\n",
        "\r\n",
        "# perform distillation\r\n",
        "num_epochs = 50\r\n",
        "callbacks = [tfk.callbacks.EarlyStopping(monitor='val_AUROC', patience=20), \r\n",
        "             tfk.callbacks.ModelCheckpoint(\"best_distiller.hdf5\", monitor='val_AUROC', mode='max',save_weights_only=True, save_best_only=True)]\r\n",
        "distiller.fit(x_train, y_train, \r\n",
        "                epochs=num_epochs, \r\n",
        "                batch_size=128, \r\n",
        "                callbacks=callbacks, \r\n",
        "                shuffle=True, \r\n",
        "                validation_data=(x_valid, y_valid))\r\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "110/110 [==============================] - 2s 11ms/step - ACC: 0.6293 - AUPR: 0.7280 - AUROC: 0.7088 - student_loss: 0.7023 - distillation_loss: 0.0000e+00 - val_student_loss: 0.6863 - val_ACC: 0.6255 - val_AUPR: 0.6716 - val_AUROC: 0.7004\n",
            "Epoch 2/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.5678 - AUPR: 0.5924 - AUROC: 0.5981 - student_loss: 0.6737 - distillation_loss: 0.0000e+00 - val_student_loss: 0.6486 - val_ACC: 0.7125 - val_AUPR: 0.7900 - val_AUROC: 0.8041\n",
            "Epoch 3/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.6392 - AUPR: 0.7002 - AUROC: 0.6949 - student_loss: 0.6269 - distillation_loss: 0.0000e+00 - val_student_loss: 0.5799 - val_ACC: 0.7505 - val_AUPR: 0.8337 - val_AUROC: 0.8416\n",
            "Epoch 4/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.6958 - AUPR: 0.7617 - AUROC: 0.7684 - student_loss: 0.5742 - distillation_loss: 0.0000e+00 - val_student_loss: 0.5418 - val_ACC: 0.7680 - val_AUPR: 0.8533 - val_AUROC: 0.8620\n",
            "Epoch 5/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.7213 - AUPR: 0.7944 - AUROC: 0.7974 - student_loss: 0.5412 - distillation_loss: 0.0000e+00 - val_student_loss: 0.5144 - val_ACC: 0.7770 - val_AUPR: 0.8576 - val_AUROC: 0.8672\n",
            "Epoch 6/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.7302 - AUPR: 0.8084 - AUROC: 0.8091 - student_loss: 0.5323 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4896 - val_ACC: 0.7905 - val_AUPR: 0.8665 - val_AUROC: 0.8749\n",
            "Epoch 7/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.7462 - AUPR: 0.8205 - AUROC: 0.8234 - student_loss: 0.5155 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4709 - val_ACC: 0.7935 - val_AUPR: 0.8727 - val_AUROC: 0.8822\n",
            "Epoch 8/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.7521 - AUPR: 0.8268 - AUROC: 0.8292 - student_loss: 0.5042 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4704 - val_ACC: 0.7945 - val_AUPR: 0.8744 - val_AUROC: 0.8803\n",
            "Epoch 9/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.7510 - AUPR: 0.8319 - AUROC: 0.8353 - student_loss: 0.4923 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4638 - val_ACC: 0.8045 - val_AUPR: 0.8821 - val_AUROC: 0.8897\n",
            "Epoch 10/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.7736 - AUPR: 0.8483 - AUROC: 0.8518 - student_loss: 0.4747 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4440 - val_ACC: 0.8215 - val_AUPR: 0.8987 - val_AUROC: 0.9048\n",
            "Epoch 11/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.7815 - AUPR: 0.8559 - AUROC: 0.8607 - student_loss: 0.4630 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4479 - val_ACC: 0.8275 - val_AUPR: 0.9037 - val_AUROC: 0.9083\n",
            "Epoch 12/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.7786 - AUPR: 0.8635 - AUROC: 0.8659 - student_loss: 0.4578 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4197 - val_ACC: 0.8435 - val_AUPR: 0.9113 - val_AUROC: 0.9164\n",
            "Epoch 13/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.7832 - AUPR: 0.8665 - AUROC: 0.8680 - student_loss: 0.4571 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4266 - val_ACC: 0.8455 - val_AUPR: 0.9172 - val_AUROC: 0.9225\n",
            "Epoch 14/50\n",
            "110/110 [==============================] - 1s 10ms/step - ACC: 0.7916 - AUPR: 0.8727 - AUROC: 0.8753 - student_loss: 0.4488 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4327 - val_ACC: 0.8485 - val_AUPR: 0.9190 - val_AUROC: 0.9237\n",
            "Epoch 15/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.7918 - AUPR: 0.8735 - AUROC: 0.8745 - student_loss: 0.4428 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4198 - val_ACC: 0.8555 - val_AUPR: 0.9235 - val_AUROC: 0.9280\n",
            "Epoch 16/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.7953 - AUPR: 0.8793 - AUROC: 0.8784 - student_loss: 0.4378 - distillation_loss: 0.0000e+00 - val_student_loss: 0.3967 - val_ACC: 0.8545 - val_AUPR: 0.9262 - val_AUROC: 0.9314\n",
            "Epoch 17/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.7918 - AUPR: 0.8790 - AUROC: 0.8793 - student_loss: 0.4322 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4011 - val_ACC: 0.8540 - val_AUPR: 0.9285 - val_AUROC: 0.9331\n",
            "Epoch 18/50\n",
            "110/110 [==============================] - 1s 9ms/step - ACC: 0.7942 - AUPR: 0.8767 - AUROC: 0.8792 - student_loss: 0.4404 - distillation_loss: 0.0000e+00 - val_student_loss: 0.3922 - val_ACC: 0.8580 - val_AUPR: 0.9311 - val_AUROC: 0.9350\n",
            "Epoch 19/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.7929 - AUPR: 0.8785 - AUROC: 0.8804 - student_loss: 0.4287 - distillation_loss: 0.0000e+00 - val_student_loss: 0.3936 - val_ACC: 0.8505 - val_AUPR: 0.9328 - val_AUROC: 0.9364\n",
            "Epoch 20/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.8028 - AUPR: 0.8826 - AUROC: 0.8846 - student_loss: 0.4301 - distillation_loss: 0.0000e+00 - val_student_loss: 0.3807 - val_ACC: 0.8645 - val_AUPR: 0.9361 - val_AUROC: 0.9402\n",
            "Epoch 21/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.7960 - AUPR: 0.8778 - AUROC: 0.8796 - student_loss: 0.4312 - distillation_loss: 0.0000e+00 - val_student_loss: 0.3716 - val_ACC: 0.8620 - val_AUPR: 0.9371 - val_AUROC: 0.9403\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdfc34e0240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eg2dHq2vKK70"
      },
      "source": [
        "def plot_f_and_grad(model):\r\n",
        "    # pick a random sample \r\n",
        "    N, L, A = x_train.shape\r\n",
        "    xsample = x_train[np.random.randint(0, N)][None, :, :]\r\n",
        "\r\n",
        "    # define a keras model mapping an input sequence to the logits of the teacher model\r\n",
        "    func = tfk.Model(inputs=model.input, outputs=model.get_layer('logits').output)\r\n",
        "\r\n",
        "    # define a set of probe sequences by sampling points in the ith nucleotide, jth channel \r\n",
        "    # i and j are picked randomly\r\n",
        "    n_probe = 100\r\n",
        "    x_probe = np.linspace(0, 1, n_probe)\r\n",
        "    n_samples = 50\r\n",
        "    Is, Js, y_ijs, y_ij_grads = [], [], [], []\r\n",
        "    for i in range(n_samples):  \r\n",
        "        i, j = np.random.randint(0, L), np.random.randint(0, A)\r\n",
        "        Is.append(i)\r\n",
        "        Js.append(j)\r\n",
        "        \r\n",
        "        x_ij_probe = np.zeros((n_probe, L, A))\r\n",
        "        x_ij_probe[:, i, j] = x_probe\r\n",
        "        x_ij_probe = tf.convert_to_tensor(x_ij_probe)\r\n",
        "\r\n",
        "        with tf.GradientTape() as tape:\r\n",
        "            tape.watch(x_ij_probe)\r\n",
        "            y_ij_pred = func(x_ij_probe)\r\n",
        "        y_ij_grad = tape.gradient(y_ij_pred, x_ij_probe)\r\n",
        "        \r\n",
        "        #y_ij_pred = func(x_ij_probe)\r\n",
        "        y_ijs.append(y_ij_pred.numpy())\r\n",
        "        y_ij_grads.append(y_ij_grad.numpy()[:, i, j])\r\n",
        "\r\n",
        "    # plot\r\n",
        "    fig = plt.figure(figsize=(14, 10))\r\n",
        "    for k in range(4):\r\n",
        "        idx = np.random.randint(0, len(Is))\r\n",
        "        i = Is[idx]\r\n",
        "        j = Js[idx]\r\n",
        "        ax = fig.add_subplot(2,2,k+1)\r\n",
        "        ax1 = ax.twinx()\r\n",
        "        title=\"i=%d, j=%d\"%(i, j)\r\n",
        "        figure_options = {'linewidth':2}\r\n",
        "\r\n",
        "        c, c1 = 'blue', 'red'\r\n",
        "        ax.plot(x_probe, y_ijs[idx], color=c, label='$f(x)$',**figure_options)\r\n",
        "        ax.tick_params(axis='y', color=c, labelcolor=c)\r\n",
        "        ax.legend(loc='upper right', fontsize=15)\r\n",
        "        \r\n",
        "        ax1.plot(x_probe, y_ij_grads[idx], color=c1, label=\"$\\\\nabla f_{ij}$\", **figure_options)\r\n",
        "        ax1.tick_params(axis='y',color=c1, labelcolor=c1)\r\n",
        "        ax1.legend(loc='lower left', fontsize=15)\r\n",
        "\r\n",
        "        ax.set_title(title, fontsize=15)\r\n",
        "    fig.tight_layout()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYBEWe-SqBDo"
      },
      "source": [
        "#plot_f_and_grad(distiller.teacher)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfcn7FDxKmlD"
      },
      "source": [
        "#plot_f_and_grad(distiller.student)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43FEwHoLwI1I"
      },
      "source": [
        "## Train a simple student model from scratch without distillation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0HgMTuQoSNj",
        "outputId": "5e406162-facc-418b-b960-54bbdb5e0600"
      },
      "source": [
        "# train a deep bind model by itself \r\n",
        "deepbind_model = get_student_model(L, A)\r\n",
        "\r\n",
        "# compile the model \r\n",
        "lossfn = tfk.losses.BinaryCrossentropy(name='bce')\r\n",
        "modelmetrics = [tfk.metrics.BinaryAccuracy(name='ACC'), tfk.metrics.AUC(curve='PR', name='AUPR'), tfk.metrics.AUC(curve='ROC', name='AUROC')]\r\n",
        "optimizer = tfk.optimizers.Adam(learning_rate=1e-2)\r\n",
        "deepbind_model.compile(loss=lossfn, metrics=modelmetrics, optimizer=optimizer)\r\n",
        "\r\n",
        "# fit the teacher model \r\n",
        "num_epochs = 100\r\n",
        "callbacks = [tfk.callbacks.EarlyStopping(monitor='val_AUROC', patience=20), \r\n",
        "             tfk.callbacks.ModelCheckpoint(\"best_deepbind_model.hdf5\", monitor='val_AUROC', mode='max', save_best_only=True)]\r\n",
        "deepbind_model.fit(x_train, y_train, \r\n",
        "                    epochs=num_epochs, \r\n",
        "                    batch_size=128, \r\n",
        "                    callbacks=callbacks, \r\n",
        "                    shuffle=True, \r\n",
        "                    initial_epoch=0,\r\n",
        "                    validation_data=(x_valid, y_valid))\r\n",
        "deepbind_model = tfk.models.load_model('best_deepbind_model.hdf5')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "110/110 [==============================] - 2s 9ms/step - loss: 0.6848 - ACC: 0.5508 - AUPR: 0.5746 - AUROC: 0.5788 - val_loss: 0.5702 - val_ACC: 0.7110 - val_AUPR: 0.8079 - val_AUROC: 0.8170\n",
            "Epoch 2/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.5865 - ACC: 0.6808 - AUPR: 0.7525 - AUROC: 0.7516 - val_loss: 0.5191 - val_ACC: 0.7740 - val_AUPR: 0.8508 - val_AUROC: 0.8581\n",
            "Epoch 3/100\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.5496 - ACC: 0.7166 - AUPR: 0.7975 - AUROC: 0.7902 - val_loss: 0.4728 - val_ACC: 0.8075 - val_AUPR: 0.8780 - val_AUROC: 0.8887\n",
            "Epoch 4/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.5197 - ACC: 0.7433 - AUPR: 0.8207 - AUROC: 0.8177 - val_loss: 0.4643 - val_ACC: 0.8280 - val_AUPR: 0.9061 - val_AUROC: 0.9068\n",
            "Epoch 5/100\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.4887 - ACC: 0.7642 - AUPR: 0.8511 - AUROC: 0.8434 - val_loss: 0.4321 - val_ACC: 0.8395 - val_AUPR: 0.9159 - val_AUROC: 0.9202\n",
            "Epoch 6/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4899 - ACC: 0.7620 - AUPR: 0.8467 - AUROC: 0.8416 - val_loss: 0.4225 - val_ACC: 0.8460 - val_AUPR: 0.9224 - val_AUROC: 0.9265\n",
            "Epoch 7/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4731 - ACC: 0.7730 - AUPR: 0.8590 - AUROC: 0.8528 - val_loss: 0.4345 - val_ACC: 0.8490 - val_AUPR: 0.9204 - val_AUROC: 0.9268\n",
            "Epoch 8/100\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.4905 - ACC: 0.7611 - AUPR: 0.8427 - AUROC: 0.8416 - val_loss: 0.4076 - val_ACC: 0.8475 - val_AUPR: 0.9230 - val_AUROC: 0.9273\n",
            "Epoch 9/100\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.4796 - ACC: 0.7672 - AUPR: 0.8522 - AUROC: 0.8482 - val_loss: 0.4349 - val_ACC: 0.8470 - val_AUPR: 0.9157 - val_AUROC: 0.9255\n",
            "Epoch 10/100\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.4818 - ACC: 0.7682 - AUPR: 0.8521 - AUROC: 0.8476 - val_loss: 0.3931 - val_ACC: 0.8415 - val_AUPR: 0.9246 - val_AUROC: 0.9306\n",
            "Epoch 11/100\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.4743 - ACC: 0.7746 - AUPR: 0.8587 - AUROC: 0.8539 - val_loss: 0.4097 - val_ACC: 0.8515 - val_AUPR: 0.9282 - val_AUROC: 0.9330\n",
            "Epoch 12/100\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.4677 - ACC: 0.7785 - AUPR: 0.8659 - AUROC: 0.8571 - val_loss: 0.4062 - val_ACC: 0.8500 - val_AUPR: 0.9291 - val_AUROC: 0.9342\n",
            "Epoch 13/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4726 - ACC: 0.7759 - AUPR: 0.8595 - AUROC: 0.8546 - val_loss: 0.4101 - val_ACC: 0.8505 - val_AUPR: 0.9305 - val_AUROC: 0.9375\n",
            "Epoch 14/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4702 - ACC: 0.7770 - AUPR: 0.8606 - AUROC: 0.8548 - val_loss: 0.4234 - val_ACC: 0.8375 - val_AUPR: 0.9333 - val_AUROC: 0.9401\n",
            "Epoch 15/100\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.4635 - ACC: 0.7806 - AUPR: 0.8673 - AUROC: 0.8598 - val_loss: 0.4287 - val_ACC: 0.8250 - val_AUPR: 0.9340 - val_AUROC: 0.9408\n",
            "Epoch 16/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.4680 - ACC: 0.7782 - AUPR: 0.8607 - AUROC: 0.8567 - val_loss: 0.4051 - val_ACC: 0.8620 - val_AUPR: 0.9326 - val_AUROC: 0.9376\n",
            "Epoch 17/100\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.4588 - ACC: 0.7837 - AUPR: 0.8690 - AUROC: 0.8625 - val_loss: 0.3923 - val_ACC: 0.8690 - val_AUPR: 0.9346 - val_AUROC: 0.9389\n",
            "Epoch 18/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4625 - ACC: 0.7808 - AUPR: 0.8691 - AUROC: 0.8609 - val_loss: 0.4079 - val_ACC: 0.8720 - val_AUPR: 0.9317 - val_AUROC: 0.9375\n",
            "Epoch 19/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4594 - ACC: 0.7903 - AUPR: 0.8662 - AUROC: 0.8641 - val_loss: 0.3956 - val_ACC: 0.8540 - val_AUPR: 0.9387 - val_AUROC: 0.9429\n",
            "Epoch 20/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4679 - ACC: 0.7751 - AUPR: 0.8615 - AUROC: 0.8566 - val_loss: 0.4082 - val_ACC: 0.8390 - val_AUPR: 0.9349 - val_AUROC: 0.9421\n",
            "Epoch 21/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4652 - ACC: 0.7746 - AUPR: 0.8629 - AUROC: 0.8568 - val_loss: 0.4030 - val_ACC: 0.8690 - val_AUPR: 0.9334 - val_AUROC: 0.9371\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_LNYVnewPy7"
      },
      "source": [
        "## Compute metrics on all 3 models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "3ovGuCNkuXuv",
        "outputId": "f418318e-619c-4ff0-8c29-b8794583a836"
      },
      "source": [
        "distilled_student_metrics = distiller.evaluate(x_test, y_test, verbose=False)\r\n",
        "teacher_metrics = distiller.teacher.evaluate(x_test, y_test, verbose=False)\r\n",
        "deepbind_from_scratch_metrics = deepbind_model.evaluate(x_test, y_test, verbose=False)\r\n",
        "names = deepbind_model.metrics_names\r\n",
        "df = pd.DataFrame(data={'Name':names, 'Student (distilled)':distilled_student_metrics, 'Student (from scratch)':deepbind_from_scratch_metrics})\r\n",
        "df"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Student (distilled)</th>\n",
              "      <th>Student (from scratch)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>loss</td>\n",
              "      <td>0.375016</td>\n",
              "      <td>0.393961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ACC</td>\n",
              "      <td>0.866500</td>\n",
              "      <td>0.860250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AUPR</td>\n",
              "      <td>0.941910</td>\n",
              "      <td>0.940537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AUROC</td>\n",
              "      <td>0.942043</td>\n",
              "      <td>0.940288</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Name  Student (distilled)  Student (from scratch)\n",
              "0   loss             0.375016                0.393961\n",
              "1    ACC             0.866500                0.860250\n",
              "2   AUPR             0.941910                0.940537\n",
              "3  AUROC             0.942043                0.940288"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD54SNjNo3Kx"
      },
      "source": [
        "##Run experiment on varying alpha and temperature values\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPsxCNE-jHe1",
        "outputId": "ee8c4e19-606d-472a-d0c6-8342d5f238f2"
      },
      "source": [
        "alpha_values = [.1,.2,.3,.4,.5,.6,.7,.8,.9]\r\n",
        "temperature_values = [1.,2.,3.,4.,5.]\r\n",
        "\r\n",
        "experiment_data=[]\r\n",
        "\r\n",
        "\r\n",
        "for alpha_value in alpha_values:\r\n",
        "  for temperature_value in temperature_values:\r\n",
        "    print('Alpha: ',alpha_value, ', Temperature: ',temperature_value)\r\n",
        "    # instantiate the student model and the distiller\r\n",
        "    student_model = get_student_model(L, A)\r\n",
        "    distiller = Distiller(student_model, teacher_model)\r\n",
        "    # compile the distiller\r\n",
        "    alpha = alpha_value\r\n",
        "    temperature = temperature_value\r\n",
        "    distiller.compile(\r\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\r\n",
        "        metrics=modelmetrics,\r\n",
        "        student_loss_fn=keras.losses.BinaryCrossentropy(name='bce'),\r\n",
        "        distillation_loss_fn=keras.losses.KLDivergence(),\r\n",
        "        alpha=alpha,\r\n",
        "        temperature=temperature,\r\n",
        "    )\r\n",
        "\r\n",
        "    # perform distillation\r\n",
        "    num_epochs = 50\r\n",
        "    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_AUROC', patience=20), \r\n",
        "                tfk.callbacks.ModelCheckpoint(\"best_distiller.hdf5\", monitor='val_AUROC', mode='max',save_weights_only=True, save_best_only=True)]\r\n",
        "    distiller.fit(x_train, y_train, \r\n",
        "                    epochs=num_epochs, \r\n",
        "                    batch_size=128, \r\n",
        "                    callbacks=callbacks, \r\n",
        "                    shuffle=True, \r\n",
        "                    validation_data=(x_valid, y_valid),\r\n",
        "                    verbose=0)\r\n",
        "    \r\n",
        "    #evaluate and save Distilled metrics\r\n",
        "    experiment_dist_student_metrics = distiller.evaluate(x_test, y_test, verbose=False)\r\n",
        "    hyperparameters = [alpha_value, temperature_value]\r\n",
        "    all_values = hyperparameters+experiment_dist_student_metrics\r\n",
        "    #add data to list\r\n",
        "    experiment_data.append(all_values)\r\n",
        "\r\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Alpha:  0.1 , Temperature:  1.0\n",
            "Alpha:  0.1 , Temperature:  2.0\n",
            "Alpha:  0.1 , Temperature:  3.0\n",
            "Alpha:  0.1 , Temperature:  4.0\n",
            "Alpha:  0.1 , Temperature:  5.0\n",
            "Alpha:  0.2 , Temperature:  1.0\n",
            "Alpha:  0.2 , Temperature:  2.0\n",
            "Alpha:  0.2 , Temperature:  3.0\n",
            "Alpha:  0.2 , Temperature:  4.0\n",
            "Alpha:  0.2 , Temperature:  5.0\n",
            "Alpha:  0.3 , Temperature:  1.0\n",
            "Alpha:  0.3 , Temperature:  2.0\n",
            "Alpha:  0.3 , Temperature:  3.0\n",
            "Alpha:  0.3 , Temperature:  4.0\n",
            "Alpha:  0.3 , Temperature:  5.0\n",
            "Alpha:  0.4 , Temperature:  1.0\n",
            "Alpha:  0.4 , Temperature:  2.0\n",
            "Alpha:  0.4 , Temperature:  3.0\n",
            "Alpha:  0.4 , Temperature:  4.0\n",
            "Alpha:  0.4 , Temperature:  5.0\n",
            "Alpha:  0.5 , Temperature:  1.0\n",
            "Alpha:  0.5 , Temperature:  2.0\n",
            "Alpha:  0.5 , Temperature:  3.0\n",
            "Alpha:  0.5 , Temperature:  4.0\n",
            "Alpha:  0.5 , Temperature:  5.0\n",
            "Alpha:  0.6 , Temperature:  1.0\n",
            "Alpha:  0.6 , Temperature:  2.0\n",
            "Alpha:  0.6 , Temperature:  3.0\n",
            "Alpha:  0.6 , Temperature:  4.0\n",
            "Alpha:  0.6 , Temperature:  5.0\n",
            "Alpha:  0.7 , Temperature:  1.0\n",
            "Alpha:  0.7 , Temperature:  2.0\n",
            "Alpha:  0.7 , Temperature:  3.0\n",
            "Alpha:  0.7 , Temperature:  4.0\n",
            "Alpha:  0.7 , Temperature:  5.0\n",
            "Alpha:  0.8 , Temperature:  1.0\n",
            "Alpha:  0.8 , Temperature:  2.0\n",
            "Alpha:  0.8 , Temperature:  3.0\n",
            "Alpha:  0.8 , Temperature:  4.0\n",
            "Alpha:  0.8 , Temperature:  5.0\n",
            "Alpha:  0.9 , Temperature:  1.0\n",
            "Alpha:  0.9 , Temperature:  2.0\n",
            "Alpha:  0.9 , Temperature:  3.0\n",
            "Alpha:  0.9 , Temperature:  4.0\n",
            "Alpha:  0.9 , Temperature:  5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bQw21PwHm9Sv",
        "outputId": "8382665b-0956-46d7-a29a-a3a05023bc2e"
      },
      "source": [
        "#put results into a data table\r\n",
        "df = pd.DataFrame(experiment_data)\r\n",
        "columns = ['alpha', 'temperature', 'loss', 'ACC', 'AUPR', 'AUROC']\r\n",
        "df.columns=columns\r\n",
        "df\r\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>alpha</th>\n",
              "      <th>temperature</th>\n",
              "      <th>loss</th>\n",
              "      <th>ACC</th>\n",
              "      <th>AUPR</th>\n",
              "      <th>AUROC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.393098</td>\n",
              "      <td>0.86700</td>\n",
              "      <td>0.939217</td>\n",
              "      <td>0.939907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.356389</td>\n",
              "      <td>0.87075</td>\n",
              "      <td>0.947534</td>\n",
              "      <td>0.947073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.455177</td>\n",
              "      <td>0.88050</td>\n",
              "      <td>0.948620</td>\n",
              "      <td>0.949078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.384916</td>\n",
              "      <td>0.85350</td>\n",
              "      <td>0.947147</td>\n",
              "      <td>0.947745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.1</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.427386</td>\n",
              "      <td>0.84625</td>\n",
              "      <td>0.922761</td>\n",
              "      <td>0.924354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.352032</td>\n",
              "      <td>0.86375</td>\n",
              "      <td>0.937383</td>\n",
              "      <td>0.937519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.380599</td>\n",
              "      <td>0.86600</td>\n",
              "      <td>0.937559</td>\n",
              "      <td>0.937082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.321218</td>\n",
              "      <td>0.87600</td>\n",
              "      <td>0.947062</td>\n",
              "      <td>0.946462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.2</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.356465</td>\n",
              "      <td>0.86175</td>\n",
              "      <td>0.941081</td>\n",
              "      <td>0.939707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.2</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.344857</td>\n",
              "      <td>0.86275</td>\n",
              "      <td>0.936704</td>\n",
              "      <td>0.937057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.391482</td>\n",
              "      <td>0.87700</td>\n",
              "      <td>0.952048</td>\n",
              "      <td>0.952554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.3</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.365667</td>\n",
              "      <td>0.86850</td>\n",
              "      <td>0.941731</td>\n",
              "      <td>0.941636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.387985</td>\n",
              "      <td>0.87975</td>\n",
              "      <td>0.950506</td>\n",
              "      <td>0.950694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.378128</td>\n",
              "      <td>0.86725</td>\n",
              "      <td>0.945079</td>\n",
              "      <td>0.942755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.3</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.353990</td>\n",
              "      <td>0.89100</td>\n",
              "      <td>0.961460</td>\n",
              "      <td>0.960214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.378328</td>\n",
              "      <td>0.87100</td>\n",
              "      <td>0.947095</td>\n",
              "      <td>0.948096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.361469</td>\n",
              "      <td>0.84950</td>\n",
              "      <td>0.932507</td>\n",
              "      <td>0.929756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.4</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.397700</td>\n",
              "      <td>0.86950</td>\n",
              "      <td>0.944404</td>\n",
              "      <td>0.941453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.4</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.406702</td>\n",
              "      <td>0.84700</td>\n",
              "      <td>0.928775</td>\n",
              "      <td>0.929071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.4</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.298778</td>\n",
              "      <td>0.87125</td>\n",
              "      <td>0.942370</td>\n",
              "      <td>0.943553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.396759</td>\n",
              "      <td>0.88025</td>\n",
              "      <td>0.952487</td>\n",
              "      <td>0.953085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.307904</td>\n",
              "      <td>0.87925</td>\n",
              "      <td>0.949139</td>\n",
              "      <td>0.948052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.357928</td>\n",
              "      <td>0.86425</td>\n",
              "      <td>0.939815</td>\n",
              "      <td>0.938595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.317398</td>\n",
              "      <td>0.87450</td>\n",
              "      <td>0.947244</td>\n",
              "      <td>0.947986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.372565</td>\n",
              "      <td>0.87350</td>\n",
              "      <td>0.946579</td>\n",
              "      <td>0.945369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.340479</td>\n",
              "      <td>0.88025</td>\n",
              "      <td>0.950159</td>\n",
              "      <td>0.949567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.6</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.320387</td>\n",
              "      <td>0.87725</td>\n",
              "      <td>0.951287</td>\n",
              "      <td>0.950975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.6</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.364304</td>\n",
              "      <td>0.88825</td>\n",
              "      <td>0.957275</td>\n",
              "      <td>0.957508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.6</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.330277</td>\n",
              "      <td>0.86100</td>\n",
              "      <td>0.948062</td>\n",
              "      <td>0.946436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.6</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.292262</td>\n",
              "      <td>0.89125</td>\n",
              "      <td>0.957045</td>\n",
              "      <td>0.957331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.360041</td>\n",
              "      <td>0.86975</td>\n",
              "      <td>0.946333</td>\n",
              "      <td>0.944328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.7</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.329753</td>\n",
              "      <td>0.84600</td>\n",
              "      <td>0.930530</td>\n",
              "      <td>0.931406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.7</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.271344</td>\n",
              "      <td>0.86825</td>\n",
              "      <td>0.954237</td>\n",
              "      <td>0.954322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.7</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.364181</td>\n",
              "      <td>0.85825</td>\n",
              "      <td>0.939402</td>\n",
              "      <td>0.936599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.7</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.273932</td>\n",
              "      <td>0.89000</td>\n",
              "      <td>0.959997</td>\n",
              "      <td>0.959790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.372668</td>\n",
              "      <td>0.87625</td>\n",
              "      <td>0.951218</td>\n",
              "      <td>0.952139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.8</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.365933</td>\n",
              "      <td>0.89300</td>\n",
              "      <td>0.960083</td>\n",
              "      <td>0.961204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.8</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.379996</td>\n",
              "      <td>0.85475</td>\n",
              "      <td>0.930722</td>\n",
              "      <td>0.933227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.8</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.346541</td>\n",
              "      <td>0.88375</td>\n",
              "      <td>0.951236</td>\n",
              "      <td>0.950496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.8</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.381825</td>\n",
              "      <td>0.85275</td>\n",
              "      <td>0.944836</td>\n",
              "      <td>0.942711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.376398</td>\n",
              "      <td>0.85825</td>\n",
              "      <td>0.935702</td>\n",
              "      <td>0.933465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.9</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.412511</td>\n",
              "      <td>0.86425</td>\n",
              "      <td>0.940249</td>\n",
              "      <td>0.942075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.348373</td>\n",
              "      <td>0.88150</td>\n",
              "      <td>0.954035</td>\n",
              "      <td>0.953241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.9</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.425680</td>\n",
              "      <td>0.85800</td>\n",
              "      <td>0.936140</td>\n",
              "      <td>0.933666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.9</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.349019</td>\n",
              "      <td>0.86025</td>\n",
              "      <td>0.935898</td>\n",
              "      <td>0.935224</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    alpha  temperature      loss      ACC      AUPR     AUROC\n",
              "0     0.1          1.0  0.393098  0.86700  0.939217  0.939907\n",
              "1     0.1          2.0  0.356389  0.87075  0.947534  0.947073\n",
              "2     0.1          3.0  0.455177  0.88050  0.948620  0.949078\n",
              "3     0.1          4.0  0.384916  0.85350  0.947147  0.947745\n",
              "4     0.1          5.0  0.427386  0.84625  0.922761  0.924354\n",
              "5     0.2          1.0  0.352032  0.86375  0.937383  0.937519\n",
              "6     0.2          2.0  0.380599  0.86600  0.937559  0.937082\n",
              "7     0.2          3.0  0.321218  0.87600  0.947062  0.946462\n",
              "8     0.2          4.0  0.356465  0.86175  0.941081  0.939707\n",
              "9     0.2          5.0  0.344857  0.86275  0.936704  0.937057\n",
              "10    0.3          1.0  0.391482  0.87700  0.952048  0.952554\n",
              "11    0.3          2.0  0.365667  0.86850  0.941731  0.941636\n",
              "12    0.3          3.0  0.387985  0.87975  0.950506  0.950694\n",
              "13    0.3          4.0  0.378128  0.86725  0.945079  0.942755\n",
              "14    0.3          5.0  0.353990  0.89100  0.961460  0.960214\n",
              "15    0.4          1.0  0.378328  0.87100  0.947095  0.948096\n",
              "16    0.4          2.0  0.361469  0.84950  0.932507  0.929756\n",
              "17    0.4          3.0  0.397700  0.86950  0.944404  0.941453\n",
              "18    0.4          4.0  0.406702  0.84700  0.928775  0.929071\n",
              "19    0.4          5.0  0.298778  0.87125  0.942370  0.943553\n",
              "20    0.5          1.0  0.396759  0.88025  0.952487  0.953085\n",
              "21    0.5          2.0  0.307904  0.87925  0.949139  0.948052\n",
              "22    0.5          3.0  0.357928  0.86425  0.939815  0.938595\n",
              "23    0.5          4.0  0.317398  0.87450  0.947244  0.947986\n",
              "24    0.5          5.0  0.372565  0.87350  0.946579  0.945369\n",
              "25    0.6          1.0  0.340479  0.88025  0.950159  0.949567\n",
              "26    0.6          2.0  0.320387  0.87725  0.951287  0.950975\n",
              "27    0.6          3.0  0.364304  0.88825  0.957275  0.957508\n",
              "28    0.6          4.0  0.330277  0.86100  0.948062  0.946436\n",
              "29    0.6          5.0  0.292262  0.89125  0.957045  0.957331\n",
              "30    0.7          1.0  0.360041  0.86975  0.946333  0.944328\n",
              "31    0.7          2.0  0.329753  0.84600  0.930530  0.931406\n",
              "32    0.7          3.0  0.271344  0.86825  0.954237  0.954322\n",
              "33    0.7          4.0  0.364181  0.85825  0.939402  0.936599\n",
              "34    0.7          5.0  0.273932  0.89000  0.959997  0.959790\n",
              "35    0.8          1.0  0.372668  0.87625  0.951218  0.952139\n",
              "36    0.8          2.0  0.365933  0.89300  0.960083  0.961204\n",
              "37    0.8          3.0  0.379996  0.85475  0.930722  0.933227\n",
              "38    0.8          4.0  0.346541  0.88375  0.951236  0.950496\n",
              "39    0.8          5.0  0.381825  0.85275  0.944836  0.942711\n",
              "40    0.9          1.0  0.376398  0.85825  0.935702  0.933465\n",
              "41    0.9          2.0  0.412511  0.86425  0.940249  0.942075\n",
              "42    0.9          3.0  0.348373  0.88150  0.954035  0.953241\n",
              "43    0.9          4.0  0.425680  0.85800  0.936140  0.933666\n",
              "44    0.9          5.0  0.349019  0.86025  0.935898  0.935224"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32LbJfGuuC_m"
      },
      "source": [
        "df.to_csv('performancemetrics-original')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "e5G23QDkHlqS",
        "outputId": "6f9590bd-aad2-4286-af27-d0894cf06646"
      },
      "source": [
        "#find best performing combination\r\n",
        "max_metric = experiment_data[0]\r\n",
        "for metrics in experiment_data:\r\n",
        "  if metrics[3]>max_metric[3]:\r\n",
        "    max_metric = metrics\r\n",
        "\r\n",
        "mf = pd.DataFrame(data={'Name':columns, 'Best Performance':max_metric})\r\n",
        "mf"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Best Performance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>alpha</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>temperature</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>loss</td>\n",
              "      <td>0.365933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ACC</td>\n",
              "      <td>0.893000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AUPR</td>\n",
              "      <td>0.960083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>AUROC</td>\n",
              "      <td>0.961204</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Name  Best Performance\n",
              "0        alpha          0.800000\n",
              "1  temperature          2.000000\n",
              "2         loss          0.365933\n",
              "3          ACC          0.893000\n",
              "4         AUPR          0.960083\n",
              "5        AUROC          0.961204"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    }
  ]
}