{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of cnn_dist.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHecrF91fJ3h"
      },
      "source": [
        "import numpy as np, os\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras as tfk\r\n",
        "keras = tfk\r\n",
        "import datetime as dt\r\n",
        "import six\r\n",
        "import h5py\r\n",
        "import requests\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt \r\n",
        "import seaborn as sns\r\n",
        "sns.set()\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3psP7yOQIVD"
      },
      "source": [
        "# Data loading "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKPa5-b4L7hX"
      },
      "source": [
        "# download the data \r\n",
        "url = 'https://www.dropbox.com/s/ysrim2re8mh22z9/synthetic_code_dataset.h5?dl=0'\r\n",
        "save_name = 'data.h5'\r\n",
        "_=!wget {url} -O {save_name}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCvBWh9oNYhd"
      },
      "source": [
        "# load the data into x_train, y_train, .....\r\n",
        "f = h5py.File(save_name, 'r')\r\n",
        "suffixes = ['train', 'test', 'valid']\r\n",
        "for suffix in suffixes:\r\n",
        "    exec(\"x_%s=np.transpose(f.get(\\\"X_%s\\\")[:], (0, 2, 1))\"%(suffix, suffix))\r\n",
        "    exec(\"y_%s=f.get(\\\"Y_%s\\\")[:]\"%(suffix, suffix))\r\n",
        "f.close()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISQFZ12vQKeu"
      },
      "source": [
        "# Model definition function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5qiwx1sfthV"
      },
      "source": [
        "def get_activation(activation = 'relu'):\r\n",
        "    \"\"\"\r\n",
        "    Create an activation function. The activation argument should one of:\r\n",
        "    1. A string representing the keras name of the activation. \r\n",
        "    2. A callable which may or may not be an instance of keras.layers.Layer. \r\n",
        "    \"\"\"\r\n",
        "    if isinstance(activation, str):\r\n",
        "        actfn = tfk.layers.Activation(activation)\r\n",
        "    else:\r\n",
        "        if callable(activation) and not isinstance(activation, tfk.layers.Layer):\r\n",
        "            actfn = tfk.layers.Activation(activation)\r\n",
        "        else:\r\n",
        "            actfn = activation\r\n",
        "    return actfn"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baDorqePRC-5"
      },
      "source": [
        "def conv_layer(x, num_filters, kernel_size, padding, activation, dropout=0.5, l2=1e-6, bn=True): \r\n",
        "    \"\"\"\r\n",
        "    A convolutional block comprising of a convolutional layer followed by\r\n",
        "    batch normalization, an activation function, and dropout. \r\n",
        "    \"\"\"\r\n",
        "    y = tfk.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, kernel_regularizer=tfk.regularizers.l2(l2), padding=padding)(x)\r\n",
        "    if bn:\r\n",
        "        y = tfk.layers.BatchNormalization()(y)\r\n",
        "    actfn = get_activation(activation)\r\n",
        "    y = actfn(y)\r\n",
        "    if dropout:\r\n",
        "        y = tfk.layers.Dropout(dropout)(y)\r\n",
        "    return y\r\n",
        "\r\n",
        "def dense_layer(x, num_units, activation, dropout=0.5, l2=None, bn=True):\r\n",
        "    \"\"\"\r\n",
        "    A dense block comprising of a dense layer followed by batch normalization, \r\n",
        "    activation and dropout. \r\n",
        "    \"\"\"\r\n",
        "    y = tfk.layers.Dense(num_units, use_bias=False, kernel_regularizer=tfk.regularizers.l2(l2))(x)\r\n",
        "    if bn:\r\n",
        "        y = tfk.layers.BatchNormalization()(y)\r\n",
        "    actfn = get_activation(activation)\r\n",
        "    y = actfn(y)\r\n",
        "    if dropout:\r\n",
        "        y = tfk.layers.Dropout(dropout)(y)\r\n",
        "    return y\r\n",
        "\r\n",
        "def get_model(L, A, name=\"cnn_att\"):\r\n",
        "\t## input layer\r\n",
        "\tx = tfk.layers.Input((L, A), name='Input')\r\n",
        "\t\r\n",
        "\t## 1st conv layer\r\n",
        "\ty = keras.layers.Conv1D(filters=32, kernel_size=19, kernel_regularizer=tfk.regularizers.l2(1e-6), padding='same', name='conv1', use_bias=True)(x)\r\n",
        "\ty = keras.layers.Activation('relu')(y)\r\n",
        "\ty = keras.layers.MaxPool1D(pool_size=4)(y)\r\n",
        "\t\r\n",
        "\t# multi head attention layer\r\n",
        "\tembedding = keras.layers.Dropout(0.1)(y)\r\n",
        "\ty, weights = keras.layers.MultiHeadAttention(num_heads=8, key_dim=64, value_dim=64)(embedding, embedding, return_attention_scores=True)\r\n",
        "\ty = keras.layers.Dropout(0.1)(y)\r\n",
        "\ty = keras.layers.LayerNormalization(epsilon=1e-6)(y)\r\n",
        "\t\r\n",
        "\t# everything else\r\n",
        "\ty = keras.layers.Flatten()(y)\r\n",
        "\ty = keras.layers.Dense(128, activation=None, use_bias=False)(y)\r\n",
        "\ty = keras.layers.BatchNormalization()(y)\r\n",
        "\ty = keras.layers.Activation('relu')(y)\r\n",
        "\ty = keras.layers.Dropout(0.5)(y)\r\n",
        "\ty = keras.layers.Dense(1, name='logits')(y)\r\n",
        "\ty = keras.layers.Activation('sigmoid', name='output')(y)\r\n",
        "\tmodel = tfk.Model(inputs=x, outputs=y, name=name)\r\n",
        "\treturn model\r\n",
        "\r\n",
        "# def get_model(L, A, activation='relu', name='cnn_dist'):\r\n",
        "#     \"\"\"\r\n",
        "#     A function to assemble the full CNN distributed model. \r\n",
        "#     \"\"\"\r\n",
        "#     # input layer \r\n",
        "#     x = tfk.layers.Input((L, A), name='input')\r\n",
        "\r\n",
        "#     # 1st convolutional block \r\n",
        "#     y = conv_layer(x,num_filters=24, kernel_size=19, padding='same', dropout=0.1,l2=1e-6, bn=True, activation=activation)\r\n",
        "    \r\n",
        "#     # 2nd conv. block + pooling \r\n",
        "#     y = conv_layer(y,num_filters=32, kernel_size=7, padding='same', activation=activation, dropout=0.2,l2=1e-6, bn=True)\r\n",
        "#     y = tfk.layers.MaxPool1D(pool_size=4)(y)\r\n",
        "    \r\n",
        "#     # 3rd convolutional block + pooling \r\n",
        "#     y = conv_layer(y,num_filters=64, kernel_size=3, padding='same', activation=activation, dropout=0.4,l2=1e-6, bn=True)\r\n",
        "#     y = tfk.layers.MaxPool1D(pool_size=3, strides=3, padding='same')(y)\r\n",
        "    \r\n",
        "#     # dense block and final output layer \r\n",
        "#     y = tfk.layers.Flatten()(y)\r\n",
        "#     y = dense_layer(y, num_units=96, activation=activation, dropout=0.5, l2=1e-6, bn=True)\r\n",
        "#     y = tfk.layers.Dense(1, use_bias=True, name = 'logits')(y)\r\n",
        "#     y = tfk.layers.Activation('sigmoid')(y)\r\n",
        "\r\n",
        "#     # assemble full model\r\n",
        "#     model = tfk.Model(x, y, name=name)\r\n",
        "#     return model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQzKg97ayWQ-"
      },
      "source": [
        "# Train a teacher model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUkdVPMdifWD",
        "outputId": "2e41448e-7c34-498f-b10d-190a883c536b"
      },
      "source": [
        "# instantiate the teacher model \r\n",
        "activation = 'relu' \r\n",
        "#activation = lambda x : tf.math.sin(x) + tf.math.cos(x)\r\n",
        "L, A = x_train.shape[1:]\r\n",
        "\r\n",
        "teacher_model = get_model(L, A, name='teacher')\r\n",
        "#teacher_model = get_model(L, A, activation, name='teacher')\r\n",
        "\r\n",
        "# compile the teacher model \r\n",
        "lossfn = tfk.losses.BinaryCrossentropy(name='bce')\r\n",
        "modelmetrics = [tfk.metrics.BinaryAccuracy(name='ACC'), tfk.metrics.AUC(curve='PR', name='AUPR'), tfk.metrics.AUC(curve='ROC', name='AUROC')]\r\n",
        "optimizer = tfk.optimizers.Adam(learning_rate=1e-2)\r\n",
        "teacher_model.compile(loss=lossfn, metrics=modelmetrics, optimizer=optimizer)\r\n",
        "\r\n",
        "# fit the teacher model \r\n",
        "num_epochs = 100\r\n",
        "callbacks = [tfk.callbacks.EarlyStopping(monitor='val_AUROC', patience=20), \r\n",
        "             tfk.callbacks.ModelCheckpoint(\"best_teacher_model.hdf5\", monitor='val_AUROC', mode='max', save_best_only=True)]\r\n",
        "teacher_model.fit(x_train, y_train, \r\n",
        "                    epochs=num_epochs, \r\n",
        "                    batch_size=128, \r\n",
        "                    callbacks=callbacks, \r\n",
        "                    shuffle=True, \r\n",
        "                    initial_epoch=0,\r\n",
        "                    validation_data=(x_valid, y_valid))\r\n",
        "teacher_model = tfk.models.load_model('best_teacher_model.hdf5')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "110/110 [==============================] - 11s 19ms/step - loss: 0.6421 - ACC: 0.6769 - AUPR: 0.7129 - AUROC: 0.7359 - val_loss: 0.8623 - val_ACC: 0.5380 - val_AUPR: 0.8617 - val_AUROC: 0.8656\n",
            "Epoch 2/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.4494 - ACC: 0.7975 - AUPR: 0.8649 - AUROC: 0.8733 - val_loss: 0.3749 - val_ACC: 0.8265 - val_AUPR: 0.9161 - val_AUROC: 0.9157\n",
            "Epoch 3/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.3802 - ACC: 0.8335 - AUPR: 0.9061 - AUROC: 0.9105 - val_loss: 0.7475 - val_ACC: 0.6555 - val_AUPR: 0.9324 - val_AUROC: 0.9379\n",
            "Epoch 4/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.3196 - ACC: 0.8619 - AUPR: 0.9329 - AUROC: 0.9378 - val_loss: 0.3831 - val_ACC: 0.8205 - val_AUPR: 0.9503 - val_AUROC: 0.9521\n",
            "Epoch 5/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.2733 - ACC: 0.8875 - AUPR: 0.9530 - AUROC: 0.9550 - val_loss: 0.3286 - val_ACC: 0.8600 - val_AUPR: 0.9622 - val_AUROC: 0.9649\n",
            "Epoch 6/100\n",
            "110/110 [==============================] - 1s 10ms/step - loss: 0.2501 - ACC: 0.8961 - AUPR: 0.9602 - AUROC: 0.9624 - val_loss: 0.2433 - val_ACC: 0.8955 - val_AUPR: 0.9681 - val_AUROC: 0.9708\n",
            "Epoch 7/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.2199 - ACC: 0.9111 - AUPR: 0.9698 - AUROC: 0.9710 - val_loss: 0.5753 - val_ACC: 0.7650 - val_AUPR: 0.9680 - val_AUROC: 0.9673\n",
            "Epoch 8/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.2110 - ACC: 0.9161 - AUPR: 0.9706 - AUROC: 0.9732 - val_loss: 0.2078 - val_ACC: 0.9205 - val_AUPR: 0.9726 - val_AUROC: 0.9747\n",
            "Epoch 9/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.2042 - ACC: 0.9176 - AUPR: 0.9728 - AUROC: 0.9749 - val_loss: 0.2027 - val_ACC: 0.9270 - val_AUPR: 0.9733 - val_AUROC: 0.9751\n",
            "Epoch 10/100\n",
            "110/110 [==============================] - 1s 10ms/step - loss: 0.1998 - ACC: 0.9201 - AUPR: 0.9751 - AUROC: 0.9763 - val_loss: 0.2138 - val_ACC: 0.9170 - val_AUPR: 0.9752 - val_AUROC: 0.9771\n",
            "Epoch 11/100\n",
            "110/110 [==============================] - 1s 10ms/step - loss: 0.1930 - ACC: 0.9249 - AUPR: 0.9782 - AUROC: 0.9782 - val_loss: 0.2290 - val_ACC: 0.9050 - val_AUPR: 0.9776 - val_AUROC: 0.9788\n",
            "Epoch 12/100\n",
            "110/110 [==============================] - 1s 10ms/step - loss: 0.1789 - ACC: 0.9327 - AUPR: 0.9807 - AUROC: 0.9812 - val_loss: 0.2130 - val_ACC: 0.9095 - val_AUPR: 0.9752 - val_AUROC: 0.9775\n",
            "Epoch 13/100\n",
            "110/110 [==============================] - 1s 10ms/step - loss: 0.1821 - ACC: 0.9288 - AUPR: 0.9797 - AUROC: 0.9804 - val_loss: 0.1798 - val_ACC: 0.9285 - val_AUPR: 0.9779 - val_AUROC: 0.9804\n",
            "Epoch 14/100\n",
            "110/110 [==============================] - 1s 10ms/step - loss: 0.1678 - ACC: 0.9340 - AUPR: 0.9830 - AUROC: 0.9834 - val_loss: 0.2086 - val_ACC: 0.9180 - val_AUPR: 0.9720 - val_AUROC: 0.9760\n",
            "Epoch 15/100\n",
            "110/110 [==============================] - 1s 10ms/step - loss: 0.1717 - ACC: 0.9351 - AUPR: 0.9825 - AUROC: 0.9829 - val_loss: 0.1902 - val_ACC: 0.9225 - val_AUPR: 0.9753 - val_AUROC: 0.9782\n",
            "Epoch 16/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.1613 - ACC: 0.9385 - AUPR: 0.9840 - AUROC: 0.9849 - val_loss: 0.1900 - val_ACC: 0.9255 - val_AUPR: 0.9764 - val_AUROC: 0.9801\n",
            "Epoch 17/100\n",
            "110/110 [==============================] - 1s 10ms/step - loss: 0.1595 - ACC: 0.9381 - AUPR: 0.9853 - AUROC: 0.9853 - val_loss: 0.2007 - val_ACC: 0.9200 - val_AUPR: 0.9775 - val_AUROC: 0.9809\n",
            "Epoch 18/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.1552 - ACC: 0.9389 - AUPR: 0.9853 - AUROC: 0.9861 - val_loss: 0.3878 - val_ACC: 0.8460 - val_AUPR: 0.9774 - val_AUROC: 0.9771\n",
            "Epoch 19/100\n",
            "110/110 [==============================] - 1s 10ms/step - loss: 0.1459 - ACC: 0.9439 - AUPR: 0.9880 - AUROC: 0.9879 - val_loss: 0.2201 - val_ACC: 0.9025 - val_AUPR: 0.9778 - val_AUROC: 0.9808\n",
            "Epoch 20/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.1434 - ACC: 0.9506 - AUPR: 0.9879 - AUROC: 0.9882 - val_loss: 0.2126 - val_ACC: 0.9140 - val_AUPR: 0.9773 - val_AUROC: 0.9805\n",
            "Epoch 21/100\n",
            "110/110 [==============================] - 1s 11ms/step - loss: 0.1511 - ACC: 0.9436 - AUPR: 0.9865 - AUROC: 0.9869 - val_loss: 0.1810 - val_ACC: 0.9300 - val_AUPR: 0.9763 - val_AUROC: 0.9805\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kpvZ10Vvxcr"
      },
      "source": [
        "teacher_model = tfk.models.load_model('best_teacher_model.hdf5')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lep5Pun5E0k-"
      },
      "source": [
        "# Knowledge distillation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTehiZ5nGzou"
      },
      "source": [
        "## Define a `Distiller` class that takes in a trained teacher model, an untrained student model and distills the knowledge in the teacher model onto the student model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tai-epSfGkUz"
      },
      "source": [
        "class Distiller(keras.Model):\r\n",
        "    def get_config(self,):\r\n",
        "        \"\"\"\r\n",
        "        Implement the config dictionary to enable serialization\r\n",
        "        \"\"\"\r\n",
        "        config = {}\r\n",
        "        config['student'] = self.student\r\n",
        "        config['teacher'] = self.teacher\r\n",
        "        return config\r\n",
        "    \r\n",
        "    def __init__(self, student, teacher):\r\n",
        "        super(Distiller, self).__init__()\r\n",
        "        self.teacher = teacher\r\n",
        "        self.student = student\r\n",
        "\r\n",
        "    def compile(\r\n",
        "        self,\r\n",
        "        optimizer,\r\n",
        "        metrics,\r\n",
        "        student_loss_fn,\r\n",
        "        distillation_loss_fn,\r\n",
        "        alpha=0.1,\r\n",
        "        temperature=3,\r\n",
        "    ):\r\n",
        "        \"\"\" Configure the distiller.\r\n",
        "\r\n",
        "        Args:\r\n",
        "            optimizer: Keras optimizer for the student weights\r\n",
        "            metrics: Keras metrics for evaluation\r\n",
        "            student_loss_fn: Loss function of difference between student\r\n",
        "                predictions and ground-truth\r\n",
        "            distillation_loss_fn: Loss function of difference between soft\r\n",
        "                student predictions and soft teacher predictions\r\n",
        "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\r\n",
        "            temperature: Temperature for softening probability distributions.\r\n",
        "                Larger temperature gives softer distributions.\r\n",
        "        \"\"\"\r\n",
        "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\r\n",
        "        self.student_loss_fn = student_loss_fn\r\n",
        "        self.distillation_loss_fn = distillation_loss_fn\r\n",
        "        self.alpha = alpha\r\n",
        "        self.temperature = temperature\r\n",
        "\r\n",
        "    def train_step(self, data):\r\n",
        "        # Unpack data\r\n",
        "        x, y = data\r\n",
        "\r\n",
        "        # Forward pass of teacher\r\n",
        "        teacher_predictions = self.teacher(x, training=False)\r\n",
        "\r\n",
        "        with tf.GradientTape() as tape:\r\n",
        "            # Forward pass of student\r\n",
        "            student_predictions = self.student(x, training=True)\r\n",
        "\r\n",
        "            # Compute losses\r\n",
        "            student_loss = self.student_loss_fn(y, student_predictions)\r\n",
        "            distillation_loss = self.distillation_loss_fn(\r\n",
        "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\r\n",
        "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\r\n",
        "            )\r\n",
        "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\r\n",
        "\r\n",
        "        # Compute gradients\r\n",
        "        trainable_vars = self.student.trainable_variables\r\n",
        "        gradients = tape.gradient(loss, trainable_vars)\r\n",
        "\r\n",
        "        # Update weights\r\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\r\n",
        "\r\n",
        "        # Update the metrics configured in `compile()`.\r\n",
        "        self.compiled_metrics.update_state(y, student_predictions)\r\n",
        "\r\n",
        "        # Return a dict of performance\r\n",
        "        results = {m.name: m.result() for m in self.metrics}\r\n",
        "        results.update(\r\n",
        "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\r\n",
        "        )\r\n",
        "        return results\r\n",
        "\r\n",
        "    def test_step(self, data):\r\n",
        "        # Unpack the data\r\n",
        "        x, y = data\r\n",
        "\r\n",
        "        # Compute predictions\r\n",
        "        y_prediction = self.student(x, training=False)\r\n",
        "\r\n",
        "        # Calculate the loss\r\n",
        "        student_loss = self.student_loss_fn(y, y_prediction)\r\n",
        "\r\n",
        "        # Update the metrics.\r\n",
        "        self.compiled_metrics.update_state(y, y_prediction)\r\n",
        "\r\n",
        "        # Return a dict of performance\r\n",
        "        results = {\"student_loss\": student_loss}\r\n",
        "        results.update({m.name: m.result() for m in self.metrics})\r\n",
        "        return results\r\n",
        "    \r\n",
        "    @property\r\n",
        "    def metrics_names(self):\r\n",
        "        return ['student_loss']+[m.name for m in self.metrics]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvOT3OZDHCgu"
      },
      "source": [
        "def get_student_model(L, A, activation='relu', name='deepbind'):\r\n",
        "    \"\"\"\r\n",
        "    Defining the deepbind architecture in here. \r\n",
        "    \"\"\"\r\n",
        "    x = tfk.layers.Input((L, A), name='input')\r\n",
        "    y = tfk.layers.Conv1D(filters=16, kernel_size=24, padding='valid', kernel_regularizer=tfk.regularizers.l2(1e-6))(x)\r\n",
        "    actfn = get_activation(activation=activation)\r\n",
        "    y = actfn(y)\r\n",
        "    y = tfk.layers.Lambda(lambda x : tf.reduce_max(x, axis=1))(y)  # max pooling\r\n",
        "    y = tfk.layers.Dropout(0.5)(y)  \r\n",
        "    y = tfk.layers.Dense(32, activation='relu')(y)\r\n",
        "    y = tfk.layers.Dense(1, name='logits')(y)\r\n",
        "    y = tfk.layers.Activation('sigmoid', name='output')(y)\r\n",
        "\r\n",
        "    model = tfk.Model(inputs=x, outputs=y, name=name)\r\n",
        "    return model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K79gsB5RIhPV",
        "outputId": "00ef45c3-c538-4aef-bc7c-35cc3c8a2908"
      },
      "source": [
        "# instantiate the student model and the distiller \r\n",
        "student_model = get_student_model(L, A)\r\n",
        "distiller = Distiller(student_model, teacher_model)\r\n",
        "\r\n",
        "# compile the distiller\r\n",
        "alpha = 0.8\r\n",
        "temperature = 1. \r\n",
        "distiller.compile(\r\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\r\n",
        "    metrics=modelmetrics,\r\n",
        "    student_loss_fn=keras.losses.BinaryCrossentropy(name='bce'),\r\n",
        "    distillation_loss_fn=keras.losses.KLDivergence(),\r\n",
        "    alpha=alpha,\r\n",
        "    temperature=temperature,\r\n",
        ")\r\n",
        "\r\n",
        "# perform distillation\r\n",
        "num_epochs = 50\r\n",
        "callbacks = [tfk.callbacks.EarlyStopping(monitor='val_AUROC', patience=20), \r\n",
        "             tfk.callbacks.ModelCheckpoint(\"best_distiller.hdf5\", monitor='val_AUROC', mode='max',save_weights_only=True, save_best_only=True)]\r\n",
        "distiller.fit(x_train, y_train, \r\n",
        "                epochs=num_epochs, \r\n",
        "                batch_size=128, \r\n",
        "                callbacks=callbacks, \r\n",
        "                shuffle=True, \r\n",
        "                validation_data=(x_valid, y_valid))\r\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "110/110 [==============================] - 2s 11ms/step - ACC: 0.6320 - AUPR: 0.7294 - AUROC: 0.7114 - student_loss: 0.7001 - distillation_loss: 0.0000e+00 - val_student_loss: 0.6796 - val_ACC: 0.6485 - val_AUPR: 0.6821 - val_AUROC: 0.6963\n",
            "Epoch 2/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.5625 - AUPR: 0.5841 - AUROC: 0.5951 - student_loss: 0.6733 - distillation_loss: 0.0000e+00 - val_student_loss: 0.6604 - val_ACC: 0.7145 - val_AUPR: 0.7731 - val_AUROC: 0.7895\n",
            "Epoch 3/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.6401 - AUPR: 0.6690 - AUROC: 0.6907 - student_loss: 0.6295 - distillation_loss: 0.0000e+00 - val_student_loss: 0.6199 - val_ACC: 0.7345 - val_AUPR: 0.8222 - val_AUROC: 0.8309\n",
            "Epoch 4/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.6814 - AUPR: 0.7449 - AUROC: 0.7477 - student_loss: 0.5874 - distillation_loss: 0.0000e+00 - val_student_loss: 0.5739 - val_ACC: 0.7770 - val_AUPR: 0.8547 - val_AUROC: 0.8619\n",
            "Epoch 5/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.7233 - AUPR: 0.7877 - AUROC: 0.7970 - student_loss: 0.5431 - distillation_loss: 0.0000e+00 - val_student_loss: 0.5470 - val_ACC: 0.7880 - val_AUPR: 0.8802 - val_AUROC: 0.8834\n",
            "Epoch 6/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.7418 - AUPR: 0.8167 - AUROC: 0.8180 - student_loss: 0.5250 - distillation_loss: 0.0000e+00 - val_student_loss: 0.5307 - val_ACC: 0.7975 - val_AUPR: 0.8943 - val_AUROC: 0.8946\n",
            "Epoch 7/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.7572 - AUPR: 0.8354 - AUROC: 0.8347 - student_loss: 0.5020 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4992 - val_ACC: 0.8065 - val_AUPR: 0.8991 - val_AUROC: 0.9008\n",
            "Epoch 8/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.7696 - AUPR: 0.8485 - AUROC: 0.8461 - student_loss: 0.4841 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4841 - val_ACC: 0.8225 - val_AUPR: 0.9074 - val_AUROC: 0.9090\n",
            "Epoch 9/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.7736 - AUPR: 0.8522 - AUROC: 0.8541 - student_loss: 0.4733 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4568 - val_ACC: 0.8265 - val_AUPR: 0.9126 - val_AUROC: 0.9144\n",
            "Epoch 10/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.7830 - AUPR: 0.8587 - AUROC: 0.8633 - student_loss: 0.4641 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4583 - val_ACC: 0.8335 - val_AUPR: 0.9178 - val_AUROC: 0.9201\n",
            "Epoch 11/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.7784 - AUPR: 0.8612 - AUROC: 0.8638 - student_loss: 0.4560 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4558 - val_ACC: 0.8415 - val_AUPR: 0.9205 - val_AUROC: 0.9249\n",
            "Epoch 12/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.8052 - AUPR: 0.8813 - AUROC: 0.8847 - student_loss: 0.4410 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4622 - val_ACC: 0.8440 - val_AUPR: 0.9265 - val_AUROC: 0.9323\n",
            "Epoch 13/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.8006 - AUPR: 0.8771 - AUROC: 0.8782 - student_loss: 0.4423 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4476 - val_ACC: 0.8560 - val_AUPR: 0.9302 - val_AUROC: 0.9367\n",
            "Epoch 14/50\n",
            "110/110 [==============================] - 1s 9ms/step - ACC: 0.8007 - AUPR: 0.8813 - AUROC: 0.8807 - student_loss: 0.4287 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4405 - val_ACC: 0.8720 - val_AUPR: 0.9341 - val_AUROC: 0.9402\n",
            "Epoch 15/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.7944 - AUPR: 0.8795 - AUROC: 0.8782 - student_loss: 0.4289 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4572 - val_ACC: 0.8700 - val_AUPR: 0.9365 - val_AUROC: 0.9418\n",
            "Epoch 16/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.8093 - AUPR: 0.8914 - AUROC: 0.8917 - student_loss: 0.4187 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4527 - val_ACC: 0.8735 - val_AUPR: 0.9377 - val_AUROC: 0.9417\n",
            "Epoch 17/50\n",
            "110/110 [==============================] - 1s 9ms/step - ACC: 0.7983 - AUPR: 0.8848 - AUROC: 0.8813 - student_loss: 0.4183 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4358 - val_ACC: 0.8755 - val_AUPR: 0.9408 - val_AUROC: 0.9459\n",
            "Epoch 18/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.8108 - AUPR: 0.8907 - AUROC: 0.8934 - student_loss: 0.4179 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4327 - val_ACC: 0.8765 - val_AUPR: 0.9417 - val_AUROC: 0.9473\n",
            "Epoch 19/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.8167 - AUPR: 0.8927 - AUROC: 0.8961 - student_loss: 0.4064 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4248 - val_ACC: 0.8755 - val_AUPR: 0.9431 - val_AUROC: 0.9485\n",
            "Epoch 20/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.8161 - AUPR: 0.8996 - AUROC: 0.8970 - student_loss: 0.4116 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4157 - val_ACC: 0.8775 - val_AUPR: 0.9451 - val_AUROC: 0.9509\n",
            "Epoch 21/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.8124 - AUPR: 0.8941 - AUROC: 0.8939 - student_loss: 0.4067 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4263 - val_ACC: 0.8795 - val_AUPR: 0.9429 - val_AUROC: 0.9482\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc87b4b67b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eg2dHq2vKK70"
      },
      "source": [
        "def plot_f_and_grad(model):\r\n",
        "    # pick a random sample \r\n",
        "    N, L, A = x_train.shape\r\n",
        "    xsample = x_train[np.random.randint(0, N)][None, :, :]\r\n",
        "\r\n",
        "    # define a keras model mapping an input sequence to the logits of the teacher model\r\n",
        "    func = tfk.Model(inputs=model.input, outputs=model.get_layer('logits').output)\r\n",
        "\r\n",
        "    # define a set of probe sequences by sampling points in the ith nucleotide, jth channel \r\n",
        "    # i and j are picked randomly\r\n",
        "    n_probe = 100\r\n",
        "    x_probe = np.linspace(0, 1, n_probe)\r\n",
        "    n_samples = 50\r\n",
        "    Is, Js, y_ijs, y_ij_grads = [], [], [], []\r\n",
        "    for i in range(n_samples):  \r\n",
        "        i, j = np.random.randint(0, L), np.random.randint(0, A)\r\n",
        "        Is.append(i)\r\n",
        "        Js.append(j)\r\n",
        "        \r\n",
        "        x_ij_probe = np.zeros((n_probe, L, A))\r\n",
        "        x_ij_probe[:, i, j] = x_probe\r\n",
        "        x_ij_probe = tf.convert_to_tensor(x_ij_probe)\r\n",
        "\r\n",
        "        with tf.GradientTape() as tape:\r\n",
        "            tape.watch(x_ij_probe)\r\n",
        "            y_ij_pred = func(x_ij_probe)\r\n",
        "        y_ij_grad = tape.gradient(y_ij_pred, x_ij_probe)\r\n",
        "        \r\n",
        "        #y_ij_pred = func(x_ij_probe)\r\n",
        "        y_ijs.append(y_ij_pred.numpy())\r\n",
        "        y_ij_grads.append(y_ij_grad.numpy()[:, i, j])\r\n",
        "\r\n",
        "    # plot\r\n",
        "    fig = plt.figure(figsize=(14, 10))\r\n",
        "    for k in range(4):\r\n",
        "        idx = np.random.randint(0, len(Is))\r\n",
        "        i = Is[idx]\r\n",
        "        j = Js[idx]\r\n",
        "        ax = fig.add_subplot(2,2,k+1)\r\n",
        "        ax1 = ax.twinx()\r\n",
        "        title=\"i=%d, j=%d\"%(i, j)\r\n",
        "        figure_options = {'linewidth':2}\r\n",
        "\r\n",
        "        c, c1 = 'blue', 'red'\r\n",
        "        ax.plot(x_probe, y_ijs[idx], color=c, label='$f(x)$',**figure_options)\r\n",
        "        ax.tick_params(axis='y', color=c, labelcolor=c)\r\n",
        "        ax.legend(loc='upper right', fontsize=15)\r\n",
        "        \r\n",
        "        ax1.plot(x_probe, y_ij_grads[idx], color=c1, label=\"$\\\\nabla f_{ij}$\", **figure_options)\r\n",
        "        ax1.tick_params(axis='y',color=c1, labelcolor=c1)\r\n",
        "        ax1.legend(loc='lower left', fontsize=15)\r\n",
        "\r\n",
        "        ax.set_title(title, fontsize=15)\r\n",
        "    fig.tight_layout()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYBEWe-SqBDo"
      },
      "source": [
        "#plot_f_and_grad(distiller.teacher)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfcn7FDxKmlD"
      },
      "source": [
        "#plot_f_and_grad(distiller.student)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43FEwHoLwI1I"
      },
      "source": [
        "## Train a simple student model from scratch without distillation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0HgMTuQoSNj",
        "outputId": "947ccf1f-e00a-41fb-9dd8-e616570f2424"
      },
      "source": [
        "# train a deep bind model by itself \r\n",
        "deepbind_model = get_student_model(L, A)\r\n",
        "\r\n",
        "# compile the model \r\n",
        "lossfn = tfk.losses.BinaryCrossentropy(name='bce')\r\n",
        "modelmetrics = [tfk.metrics.BinaryAccuracy(name='ACC'), tfk.metrics.AUC(curve='PR', name='AUPR'), tfk.metrics.AUC(curve='ROC', name='AUROC')]\r\n",
        "optimizer = tfk.optimizers.Adam(learning_rate=1e-2)\r\n",
        "deepbind_model.compile(loss=lossfn, metrics=modelmetrics, optimizer=optimizer)\r\n",
        "\r\n",
        "# fit the teacher model \r\n",
        "num_epochs = 100\r\n",
        "callbacks = [tfk.callbacks.EarlyStopping(monitor='val_AUROC', patience=20), \r\n",
        "             tfk.callbacks.ModelCheckpoint(\"best_deepbind_model.hdf5\", monitor='val_AUROC', mode='max', save_best_only=True)]\r\n",
        "deepbind_model.fit(x_train, y_train, \r\n",
        "                    epochs=num_epochs, \r\n",
        "                    batch_size=128, \r\n",
        "                    callbacks=callbacks, \r\n",
        "                    shuffle=True, \r\n",
        "                    initial_epoch=0,\r\n",
        "                    validation_data=(x_valid, y_valid))\r\n",
        "deepbind_model = tfk.models.load_model('best_deepbind_model.hdf5')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "110/110 [==============================] - 2s 9ms/step - loss: 0.6815 - ACC: 0.5460 - AUPR: 0.5702 - AUROC: 0.5727 - val_loss: 0.5672 - val_ACC: 0.7110 - val_AUPR: 0.7827 - val_AUROC: 0.7955\n",
            "Epoch 2/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.5971 - ACC: 0.6665 - AUPR: 0.7376 - AUROC: 0.7374 - val_loss: 0.5187 - val_ACC: 0.7620 - val_AUPR: 0.8273 - val_AUROC: 0.8408\n",
            "Epoch 3/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.5606 - ACC: 0.6990 - AUPR: 0.7747 - AUROC: 0.7798 - val_loss: 0.5019 - val_ACC: 0.7795 - val_AUPR: 0.8688 - val_AUROC: 0.8795\n",
            "Epoch 4/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.5326 - ACC: 0.7184 - AUPR: 0.8083 - AUROC: 0.8059 - val_loss: 0.4686 - val_ACC: 0.8105 - val_AUPR: 0.8881 - val_AUROC: 0.8933\n",
            "Epoch 5/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.5161 - ACC: 0.7358 - AUPR: 0.8245 - AUROC: 0.8222 - val_loss: 0.4467 - val_ACC: 0.8235 - val_AUPR: 0.8953 - val_AUROC: 0.9068\n",
            "Epoch 6/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.5224 - ACC: 0.7288 - AUPR: 0.8157 - AUROC: 0.8157 - val_loss: 0.4440 - val_ACC: 0.8345 - val_AUPR: 0.8905 - val_AUROC: 0.9050\n",
            "Epoch 7/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.5131 - ACC: 0.7366 - AUPR: 0.8261 - AUROC: 0.8218 - val_loss: 0.4445 - val_ACC: 0.8275 - val_AUPR: 0.9022 - val_AUROC: 0.9138\n",
            "Epoch 8/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.5135 - ACC: 0.7347 - AUPR: 0.8279 - AUROC: 0.8225 - val_loss: 0.4431 - val_ACC: 0.8260 - val_AUPR: 0.9023 - val_AUROC: 0.9114\n",
            "Epoch 9/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.5163 - ACC: 0.7429 - AUPR: 0.8231 - AUROC: 0.8211 - val_loss: 0.4471 - val_ACC: 0.8350 - val_AUPR: 0.8987 - val_AUROC: 0.9100\n",
            "Epoch 10/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.5082 - ACC: 0.7409 - AUPR: 0.8290 - AUROC: 0.8270 - val_loss: 0.4378 - val_ACC: 0.8285 - val_AUPR: 0.9012 - val_AUROC: 0.9108\n",
            "Epoch 11/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.5122 - ACC: 0.7334 - AUPR: 0.8250 - AUROC: 0.8228 - val_loss: 0.4616 - val_ACC: 0.8260 - val_AUPR: 0.9010 - val_AUROC: 0.9161\n",
            "Epoch 12/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.5023 - ACC: 0.7412 - AUPR: 0.8339 - AUROC: 0.8288 - val_loss: 0.4495 - val_ACC: 0.8395 - val_AUPR: 0.9013 - val_AUROC: 0.9141\n",
            "Epoch 13/100\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.5010 - ACC: 0.7413 - AUPR: 0.8312 - AUROC: 0.8320 - val_loss: 0.4390 - val_ACC: 0.8330 - val_AUPR: 0.8976 - val_AUROC: 0.9131\n",
            "Epoch 14/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.5007 - ACC: 0.7508 - AUPR: 0.8318 - AUROC: 0.8339 - val_loss: 0.4422 - val_ACC: 0.8400 - val_AUPR: 0.9002 - val_AUROC: 0.9149\n",
            "Epoch 15/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.5023 - ACC: 0.7510 - AUPR: 0.8408 - AUROC: 0.8318 - val_loss: 0.4442 - val_ACC: 0.8405 - val_AUPR: 0.9032 - val_AUROC: 0.9197\n",
            "Epoch 16/100\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.5082 - ACC: 0.7384 - AUPR: 0.8258 - AUROC: 0.8270 - val_loss: 0.4425 - val_ACC: 0.8385 - val_AUPR: 0.9086 - val_AUROC: 0.9188\n",
            "Epoch 17/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.5031 - ACC: 0.7471 - AUPR: 0.8349 - AUROC: 0.8312 - val_loss: 0.4356 - val_ACC: 0.8405 - val_AUPR: 0.9037 - val_AUROC: 0.9152\n",
            "Epoch 18/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4935 - ACC: 0.7485 - AUPR: 0.8393 - AUROC: 0.8380 - val_loss: 0.4268 - val_ACC: 0.8475 - val_AUPR: 0.9043 - val_AUROC: 0.9177\n",
            "Epoch 19/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.5001 - ACC: 0.7449 - AUPR: 0.8340 - AUROC: 0.8321 - val_loss: 0.4265 - val_ACC: 0.8395 - val_AUPR: 0.9061 - val_AUROC: 0.9206\n",
            "Epoch 20/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4970 - ACC: 0.7473 - AUPR: 0.8293 - AUROC: 0.8364 - val_loss: 0.4292 - val_ACC: 0.8555 - val_AUPR: 0.9059 - val_AUROC: 0.9199\n",
            "Epoch 21/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4947 - ACC: 0.7463 - AUPR: 0.8382 - AUROC: 0.8373 - val_loss: 0.4395 - val_ACC: 0.8565 - val_AUPR: 0.9045 - val_AUROC: 0.9191\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_LNYVnewPy7"
      },
      "source": [
        "## Compute metrics on all 3 models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "3ovGuCNkuXuv",
        "outputId": "7f86e876-f14d-4e6b-c9e6-fc3e02ba51bf"
      },
      "source": [
        "distilled_student_metrics = distiller.evaluate(x_test, y_test, verbose=False)\r\n",
        "teacher_metrics = distiller.teacher.evaluate(x_test, y_test, verbose=False)\r\n",
        "deepbind_from_scratch_metrics = deepbind_model.evaluate(x_test, y_test, verbose=False)\r\n",
        "names = deepbind_model.metrics_names\r\n",
        "df = pd.DataFrame(data={'Name':names, 'Student (distilled)':distilled_student_metrics, 'Student (from scratch)':deepbind_from_scratch_metrics, 'Teacher ':teacher_metrics})\r\n",
        "df"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Student (distilled)</th>\n",
              "      <th>Student (from scratch)</th>\n",
              "      <th>Teacher</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>loss</td>\n",
              "      <td>0.342785</td>\n",
              "      <td>0.427017</td>\n",
              "      <td>0.201470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ACC</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.825000</td>\n",
              "      <td>0.917000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AUPR</td>\n",
              "      <td>0.949092</td>\n",
              "      <td>0.914146</td>\n",
              "      <td>0.979375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AUROC</td>\n",
              "      <td>0.948674</td>\n",
              "      <td>0.916265</td>\n",
              "      <td>0.979556</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Name  Student (distilled)  Student (from scratch)  Teacher \n",
              "0   loss             0.342785                0.427017  0.201470\n",
              "1    ACC             0.875000                0.825000  0.917000\n",
              "2   AUPR             0.949092                0.914146  0.979375\n",
              "3  AUROC             0.948674                0.916265  0.979556"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_QRpUulvAfV"
      },
      "source": [
        "df.to_csv('modelmetrics-greaterrange-mutlihead')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD54SNjNo3Kx"
      },
      "source": [
        "##Run experiment on varying alpha and temperature values\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPsxCNE-jHe1",
        "outputId": "6393dc30-daa7-4235-9451-a8ac7c3388d1"
      },
      "source": [
        "alpha_values = [.1,.15,.2,.25,.3,.35,.4,.45,.5,.55,.6,.65,.7,.75,.8,.85,.9,.95]\r\n",
        "temperature_values = [1.,2.,3.,4.,5.,6.,7.,8.,9.,10.,11.,12.,13.,14.,15.]\r\n",
        "\r\n",
        "experiment_data=[]\r\n",
        "\r\n",
        "\r\n",
        "for alpha_value in alpha_values:\r\n",
        "  for temperature_value in temperature_values:\r\n",
        "    print('Alpha: ',alpha_value, ', Temperature: ',temperature_value)\r\n",
        "    # instantiate the student model and the distiller\r\n",
        "    student_model = get_student_model(L, A)\r\n",
        "    distiller = Distiller(student_model, teacher_model)\r\n",
        "    # compile the distiller\r\n",
        "    alpha = alpha_value\r\n",
        "    temperature = temperature_value\r\n",
        "    distiller.compile(\r\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\r\n",
        "        metrics=modelmetrics,\r\n",
        "        student_loss_fn=keras.losses.BinaryCrossentropy(name='bce'),\r\n",
        "        distillation_loss_fn=keras.losses.KLDivergence(),\r\n",
        "        alpha=alpha,\r\n",
        "        temperature=temperature,\r\n",
        "    )\r\n",
        "\r\n",
        "    # perform distillation\r\n",
        "    num_epochs = 50\r\n",
        "    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_AUROC', patience=20), \r\n",
        "                tfk.callbacks.ModelCheckpoint(\"best_distiller.hdf5\", monitor='val_AUROC', mode='max',save_weights_only=True, save_best_only=True)]\r\n",
        "    distiller.fit(x_train, y_train, \r\n",
        "                    epochs=num_epochs, \r\n",
        "                    batch_size=128, \r\n",
        "                    callbacks=callbacks, \r\n",
        "                    shuffle=True, \r\n",
        "                    validation_data=(x_valid, y_valid),\r\n",
        "                    verbose=0)\r\n",
        "    \r\n",
        "    #evaluate and save Distilled metrics\r\n",
        "    experiment_dist_student_metrics = distiller.evaluate(x_test, y_test, verbose=False)\r\n",
        "    hyperparameters = [alpha_value, temperature_value]\r\n",
        "    all_values = hyperparameters+experiment_dist_student_metrics\r\n",
        "    #add data to list\r\n",
        "    experiment_data.append(all_values)\r\n",
        "\r\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Alpha:  0.1 , Temperature:  1.0\n",
            "Alpha:  0.1 , Temperature:  2.0\n",
            "Alpha:  0.1 , Temperature:  3.0\n",
            "Alpha:  0.1 , Temperature:  4.0\n",
            "Alpha:  0.1 , Temperature:  5.0\n",
            "Alpha:  0.1 , Temperature:  6.0\n",
            "Alpha:  0.1 , Temperature:  7.0\n",
            "Alpha:  0.1 , Temperature:  8.0\n",
            "Alpha:  0.1 , Temperature:  9.0\n",
            "Alpha:  0.1 , Temperature:  10.0\n",
            "Alpha:  0.1 , Temperature:  11.0\n",
            "Alpha:  0.1 , Temperature:  12.0\n",
            "Alpha:  0.1 , Temperature:  13.0\n",
            "Alpha:  0.1 , Temperature:  14.0\n",
            "Alpha:  0.1 , Temperature:  15.0\n",
            "Alpha:  0.15 , Temperature:  1.0\n",
            "Alpha:  0.15 , Temperature:  2.0\n",
            "Alpha:  0.15 , Temperature:  3.0\n",
            "Alpha:  0.15 , Temperature:  4.0\n",
            "Alpha:  0.15 , Temperature:  5.0\n",
            "Alpha:  0.15 , Temperature:  6.0\n",
            "Alpha:  0.15 , Temperature:  7.0\n",
            "Alpha:  0.15 , Temperature:  8.0\n",
            "Alpha:  0.15 , Temperature:  9.0\n",
            "Alpha:  0.15 , Temperature:  10.0\n",
            "Alpha:  0.15 , Temperature:  11.0\n",
            "Alpha:  0.15 , Temperature:  12.0\n",
            "Alpha:  0.15 , Temperature:  13.0\n",
            "Alpha:  0.15 , Temperature:  14.0\n",
            "Alpha:  0.15 , Temperature:  15.0\n",
            "Alpha:  0.2 , Temperature:  1.0\n",
            "Alpha:  0.2 , Temperature:  2.0\n",
            "Alpha:  0.2 , Temperature:  3.0\n",
            "Alpha:  0.2 , Temperature:  4.0\n",
            "Alpha:  0.2 , Temperature:  5.0\n",
            "Alpha:  0.2 , Temperature:  6.0\n",
            "Alpha:  0.2 , Temperature:  7.0\n",
            "Alpha:  0.2 , Temperature:  8.0\n",
            "Alpha:  0.2 , Temperature:  9.0\n",
            "Alpha:  0.2 , Temperature:  10.0\n",
            "Alpha:  0.2 , Temperature:  11.0\n",
            "Alpha:  0.2 , Temperature:  12.0\n",
            "Alpha:  0.2 , Temperature:  13.0\n",
            "Alpha:  0.2 , Temperature:  14.0\n",
            "Alpha:  0.2 , Temperature:  15.0\n",
            "Alpha:  0.25 , Temperature:  1.0\n",
            "Alpha:  0.25 , Temperature:  2.0\n",
            "Alpha:  0.25 , Temperature:  3.0\n",
            "Alpha:  0.25 , Temperature:  4.0\n",
            "Alpha:  0.25 , Temperature:  5.0\n",
            "Alpha:  0.25 , Temperature:  6.0\n",
            "Alpha:  0.25 , Temperature:  7.0\n",
            "Alpha:  0.25 , Temperature:  8.0\n",
            "Alpha:  0.25 , Temperature:  9.0\n",
            "Alpha:  0.25 , Temperature:  10.0\n",
            "Alpha:  0.25 , Temperature:  11.0\n",
            "Alpha:  0.25 , Temperature:  12.0\n",
            "Alpha:  0.25 , Temperature:  13.0\n",
            "Alpha:  0.25 , Temperature:  14.0\n",
            "Alpha:  0.25 , Temperature:  15.0\n",
            "Alpha:  0.3 , Temperature:  1.0\n",
            "Alpha:  0.3 , Temperature:  2.0\n",
            "Alpha:  0.3 , Temperature:  3.0\n",
            "Alpha:  0.3 , Temperature:  4.0\n",
            "Alpha:  0.3 , Temperature:  5.0\n",
            "Alpha:  0.3 , Temperature:  6.0\n",
            "Alpha:  0.3 , Temperature:  7.0\n",
            "Alpha:  0.3 , Temperature:  8.0\n",
            "Alpha:  0.3 , Temperature:  9.0\n",
            "Alpha:  0.3 , Temperature:  10.0\n",
            "Alpha:  0.3 , Temperature:  11.0\n",
            "Alpha:  0.3 , Temperature:  12.0\n",
            "Alpha:  0.3 , Temperature:  13.0\n",
            "Alpha:  0.3 , Temperature:  14.0\n",
            "Alpha:  0.3 , Temperature:  15.0\n",
            "Alpha:  0.35 , Temperature:  1.0\n",
            "Alpha:  0.35 , Temperature:  2.0\n",
            "Alpha:  0.35 , Temperature:  3.0\n",
            "Alpha:  0.35 , Temperature:  4.0\n",
            "Alpha:  0.35 , Temperature:  5.0\n",
            "Alpha:  0.35 , Temperature:  6.0\n",
            "Alpha:  0.35 , Temperature:  7.0\n",
            "Alpha:  0.35 , Temperature:  8.0\n",
            "Alpha:  0.35 , Temperature:  9.0\n",
            "Alpha:  0.35 , Temperature:  10.0\n",
            "Alpha:  0.35 , Temperature:  11.0\n",
            "Alpha:  0.35 , Temperature:  12.0\n",
            "Alpha:  0.35 , Temperature:  13.0\n",
            "Alpha:  0.35 , Temperature:  14.0\n",
            "Alpha:  0.35 , Temperature:  15.0\n",
            "Alpha:  0.4 , Temperature:  1.0\n",
            "Alpha:  0.4 , Temperature:  2.0\n",
            "Alpha:  0.4 , Temperature:  3.0\n",
            "Alpha:  0.4 , Temperature:  4.0\n",
            "Alpha:  0.4 , Temperature:  5.0\n",
            "Alpha:  0.4 , Temperature:  6.0\n",
            "Alpha:  0.4 , Temperature:  7.0\n",
            "Alpha:  0.4 , Temperature:  8.0\n",
            "Alpha:  0.4 , Temperature:  9.0\n",
            "Alpha:  0.4 , Temperature:  10.0\n",
            "Alpha:  0.4 , Temperature:  11.0\n",
            "Alpha:  0.4 , Temperature:  12.0\n",
            "Alpha:  0.4 , Temperature:  13.0\n",
            "Alpha:  0.4 , Temperature:  14.0\n",
            "Alpha:  0.4 , Temperature:  15.0\n",
            "Alpha:  0.45 , Temperature:  1.0\n",
            "Alpha:  0.45 , Temperature:  2.0\n",
            "Alpha:  0.45 , Temperature:  3.0\n",
            "Alpha:  0.45 , Temperature:  4.0\n",
            "Alpha:  0.45 , Temperature:  5.0\n",
            "Alpha:  0.45 , Temperature:  6.0\n",
            "Alpha:  0.45 , Temperature:  7.0\n",
            "Alpha:  0.45 , Temperature:  8.0\n",
            "Alpha:  0.45 , Temperature:  9.0\n",
            "Alpha:  0.45 , Temperature:  10.0\n",
            "Alpha:  0.45 , Temperature:  11.0\n",
            "Alpha:  0.45 , Temperature:  12.0\n",
            "Alpha:  0.45 , Temperature:  13.0\n",
            "Alpha:  0.45 , Temperature:  14.0\n",
            "Alpha:  0.45 , Temperature:  15.0\n",
            "Alpha:  0.5 , Temperature:  1.0\n",
            "Alpha:  0.5 , Temperature:  2.0\n",
            "Alpha:  0.5 , Temperature:  3.0\n",
            "Alpha:  0.5 , Temperature:  4.0\n",
            "Alpha:  0.5 , Temperature:  5.0\n",
            "Alpha:  0.5 , Temperature:  6.0\n",
            "Alpha:  0.5 , Temperature:  7.0\n",
            "Alpha:  0.5 , Temperature:  8.0\n",
            "Alpha:  0.5 , Temperature:  9.0\n",
            "Alpha:  0.5 , Temperature:  10.0\n",
            "Alpha:  0.5 , Temperature:  11.0\n",
            "Alpha:  0.5 , Temperature:  12.0\n",
            "Alpha:  0.5 , Temperature:  13.0\n",
            "Alpha:  0.5 , Temperature:  14.0\n",
            "Alpha:  0.5 , Temperature:  15.0\n",
            "Alpha:  0.55 , Temperature:  1.0\n",
            "Alpha:  0.55 , Temperature:  2.0\n",
            "Alpha:  0.55 , Temperature:  3.0\n",
            "Alpha:  0.55 , Temperature:  4.0\n",
            "Alpha:  0.55 , Temperature:  5.0\n",
            "Alpha:  0.55 , Temperature:  6.0\n",
            "Alpha:  0.55 , Temperature:  7.0\n",
            "Alpha:  0.55 , Temperature:  8.0\n",
            "Alpha:  0.55 , Temperature:  9.0\n",
            "Alpha:  0.55 , Temperature:  10.0\n",
            "Alpha:  0.55 , Temperature:  11.0\n",
            "Alpha:  0.55 , Temperature:  12.0\n",
            "Alpha:  0.55 , Temperature:  13.0\n",
            "Alpha:  0.55 , Temperature:  14.0\n",
            "Alpha:  0.55 , Temperature:  15.0\n",
            "Alpha:  0.6 , Temperature:  1.0\n",
            "Alpha:  0.6 , Temperature:  2.0\n",
            "Alpha:  0.6 , Temperature:  3.0\n",
            "Alpha:  0.6 , Temperature:  4.0\n",
            "Alpha:  0.6 , Temperature:  5.0\n",
            "Alpha:  0.6 , Temperature:  6.0\n",
            "Alpha:  0.6 , Temperature:  7.0\n",
            "Alpha:  0.6 , Temperature:  8.0\n",
            "Alpha:  0.6 , Temperature:  9.0\n",
            "Alpha:  0.6 , Temperature:  10.0\n",
            "Alpha:  0.6 , Temperature:  11.0\n",
            "Alpha:  0.6 , Temperature:  12.0\n",
            "Alpha:  0.6 , Temperature:  13.0\n",
            "Alpha:  0.6 , Temperature:  14.0\n",
            "Alpha:  0.6 , Temperature:  15.0\n",
            "Alpha:  0.65 , Temperature:  1.0\n",
            "Alpha:  0.65 , Temperature:  2.0\n",
            "Alpha:  0.65 , Temperature:  3.0\n",
            "Alpha:  0.65 , Temperature:  4.0\n",
            "Alpha:  0.65 , Temperature:  5.0\n",
            "Alpha:  0.65 , Temperature:  6.0\n",
            "Alpha:  0.65 , Temperature:  7.0\n",
            "Alpha:  0.65 , Temperature:  8.0\n",
            "Alpha:  0.65 , Temperature:  9.0\n",
            "Alpha:  0.65 , Temperature:  10.0\n",
            "Alpha:  0.65 , Temperature:  11.0\n",
            "Alpha:  0.65 , Temperature:  12.0\n",
            "Alpha:  0.65 , Temperature:  13.0\n",
            "Alpha:  0.65 , Temperature:  14.0\n",
            "Alpha:  0.65 , Temperature:  15.0\n",
            "Alpha:  0.7 , Temperature:  1.0\n",
            "Alpha:  0.7 , Temperature:  2.0\n",
            "Alpha:  0.7 , Temperature:  3.0\n",
            "Alpha:  0.7 , Temperature:  4.0\n",
            "Alpha:  0.7 , Temperature:  5.0\n",
            "Alpha:  0.7 , Temperature:  6.0\n",
            "Alpha:  0.7 , Temperature:  7.0\n",
            "Alpha:  0.7 , Temperature:  8.0\n",
            "Alpha:  0.7 , Temperature:  9.0\n",
            "Alpha:  0.7 , Temperature:  10.0\n",
            "Alpha:  0.7 , Temperature:  11.0\n",
            "Alpha:  0.7 , Temperature:  12.0\n",
            "Alpha:  0.7 , Temperature:  13.0\n",
            "Alpha:  0.7 , Temperature:  14.0\n",
            "Alpha:  0.7 , Temperature:  15.0\n",
            "Alpha:  0.75 , Temperature:  1.0\n",
            "Alpha:  0.75 , Temperature:  2.0\n",
            "Alpha:  0.75 , Temperature:  3.0\n",
            "Alpha:  0.75 , Temperature:  4.0\n",
            "Alpha:  0.75 , Temperature:  5.0\n",
            "Alpha:  0.75 , Temperature:  6.0\n",
            "Alpha:  0.75 , Temperature:  7.0\n",
            "Alpha:  0.75 , Temperature:  8.0\n",
            "Alpha:  0.75 , Temperature:  9.0\n",
            "Alpha:  0.75 , Temperature:  10.0\n",
            "Alpha:  0.75 , Temperature:  11.0\n",
            "Alpha:  0.75 , Temperature:  12.0\n",
            "Alpha:  0.75 , Temperature:  13.0\n",
            "Alpha:  0.75 , Temperature:  14.0\n",
            "Alpha:  0.75 , Temperature:  15.0\n",
            "Alpha:  0.8 , Temperature:  1.0\n",
            "Alpha:  0.8 , Temperature:  2.0\n",
            "Alpha:  0.8 , Temperature:  3.0\n",
            "Alpha:  0.8 , Temperature:  4.0\n",
            "Alpha:  0.8 , Temperature:  5.0\n",
            "Alpha:  0.8 , Temperature:  6.0\n",
            "Alpha:  0.8 , Temperature:  7.0\n",
            "Alpha:  0.8 , Temperature:  8.0\n",
            "Alpha:  0.8 , Temperature:  9.0\n",
            "Alpha:  0.8 , Temperature:  10.0\n",
            "Alpha:  0.8 , Temperature:  11.0\n",
            "Alpha:  0.8 , Temperature:  12.0\n",
            "Alpha:  0.8 , Temperature:  13.0\n",
            "Alpha:  0.8 , Temperature:  14.0\n",
            "Alpha:  0.8 , Temperature:  15.0\n",
            "Alpha:  0.85 , Temperature:  1.0\n",
            "Alpha:  0.85 , Temperature:  2.0\n",
            "Alpha:  0.85 , Temperature:  3.0\n",
            "Alpha:  0.85 , Temperature:  4.0\n",
            "Alpha:  0.85 , Temperature:  5.0\n",
            "Alpha:  0.85 , Temperature:  6.0\n",
            "Alpha:  0.85 , Temperature:  7.0\n",
            "Alpha:  0.85 , Temperature:  8.0\n",
            "Alpha:  0.85 , Temperature:  9.0\n",
            "Alpha:  0.85 , Temperature:  10.0\n",
            "Alpha:  0.85 , Temperature:  11.0\n",
            "Alpha:  0.85 , Temperature:  12.0\n",
            "Alpha:  0.85 , Temperature:  13.0\n",
            "Alpha:  0.85 , Temperature:  14.0\n",
            "Alpha:  0.85 , Temperature:  15.0\n",
            "Alpha:  0.9 , Temperature:  1.0\n",
            "Alpha:  0.9 , Temperature:  2.0\n",
            "Alpha:  0.9 , Temperature:  3.0\n",
            "Alpha:  0.9 , Temperature:  4.0\n",
            "Alpha:  0.9 , Temperature:  5.0\n",
            "Alpha:  0.9 , Temperature:  6.0\n",
            "Alpha:  0.9 , Temperature:  7.0\n",
            "Alpha:  0.9 , Temperature:  8.0\n",
            "Alpha:  0.9 , Temperature:  9.0\n",
            "Alpha:  0.9 , Temperature:  10.0\n",
            "Alpha:  0.9 , Temperature:  11.0\n",
            "Alpha:  0.9 , Temperature:  12.0\n",
            "Alpha:  0.9 , Temperature:  13.0\n",
            "Alpha:  0.9 , Temperature:  14.0\n",
            "Alpha:  0.9 , Temperature:  15.0\n",
            "Alpha:  0.95 , Temperature:  1.0\n",
            "Alpha:  0.95 , Temperature:  2.0\n",
            "Alpha:  0.95 , Temperature:  3.0\n",
            "Alpha:  0.95 , Temperature:  4.0\n",
            "Alpha:  0.95 , Temperature:  5.0\n",
            "Alpha:  0.95 , Temperature:  6.0\n",
            "Alpha:  0.95 , Temperature:  7.0\n",
            "Alpha:  0.95 , Temperature:  8.0\n",
            "Alpha:  0.95 , Temperature:  9.0\n",
            "Alpha:  0.95 , Temperature:  10.0\n",
            "Alpha:  0.95 , Temperature:  11.0\n",
            "Alpha:  0.95 , Temperature:  12.0\n",
            "Alpha:  0.95 , Temperature:  13.0\n",
            "Alpha:  0.95 , Temperature:  14.0\n",
            "Alpha:  0.95 , Temperature:  15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "bQw21PwHm9Sv",
        "outputId": "a3557619-3b25-42a2-e70c-da28fa03f772"
      },
      "source": [
        "#put results into a data table\r\n",
        "df = pd.DataFrame(experiment_data)\r\n",
        "columns = ['alpha', 'temperature', 'loss', 'ACC', 'AUPR', 'AUROC']\r\n",
        "df.columns=columns\r\n",
        "df\r\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>alpha</th>\n",
              "      <th>temperature</th>\n",
              "      <th>loss</th>\n",
              "      <th>ACC</th>\n",
              "      <th>AUPR</th>\n",
              "      <th>AUROC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.451685</td>\n",
              "      <td>0.85425</td>\n",
              "      <td>0.931255</td>\n",
              "      <td>0.931530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.10</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.455778</td>\n",
              "      <td>0.85950</td>\n",
              "      <td>0.939064</td>\n",
              "      <td>0.937212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.10</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.384773</td>\n",
              "      <td>0.85675</td>\n",
              "      <td>0.935255</td>\n",
              "      <td>0.933663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.10</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.409642</td>\n",
              "      <td>0.87125</td>\n",
              "      <td>0.948614</td>\n",
              "      <td>0.948540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.10</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.403470</td>\n",
              "      <td>0.83400</td>\n",
              "      <td>0.915742</td>\n",
              "      <td>0.916235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>265</th>\n",
              "      <td>0.95</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.322380</td>\n",
              "      <td>0.88225</td>\n",
              "      <td>0.950412</td>\n",
              "      <td>0.950509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>266</th>\n",
              "      <td>0.95</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.430213</td>\n",
              "      <td>0.87175</td>\n",
              "      <td>0.945858</td>\n",
              "      <td>0.945768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>267</th>\n",
              "      <td>0.95</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.382494</td>\n",
              "      <td>0.87650</td>\n",
              "      <td>0.948147</td>\n",
              "      <td>0.948070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268</th>\n",
              "      <td>0.95</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.340258</td>\n",
              "      <td>0.88475</td>\n",
              "      <td>0.953421</td>\n",
              "      <td>0.952706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>0.95</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.433771</td>\n",
              "      <td>0.88025</td>\n",
              "      <td>0.952371</td>\n",
              "      <td>0.951497</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>270 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     alpha  temperature      loss      ACC      AUPR     AUROC\n",
              "0     0.10          1.0  0.451685  0.85425  0.931255  0.931530\n",
              "1     0.10          2.0  0.455778  0.85950  0.939064  0.937212\n",
              "2     0.10          3.0  0.384773  0.85675  0.935255  0.933663\n",
              "3     0.10          4.0  0.409642  0.87125  0.948614  0.948540\n",
              "4     0.10          5.0  0.403470  0.83400  0.915742  0.916235\n",
              "..     ...          ...       ...      ...       ...       ...\n",
              "265   0.95         11.0  0.322380  0.88225  0.950412  0.950509\n",
              "266   0.95         12.0  0.430213  0.87175  0.945858  0.945768\n",
              "267   0.95         13.0  0.382494  0.87650  0.948147  0.948070\n",
              "268   0.95         14.0  0.340258  0.88475  0.953421  0.952706\n",
              "269   0.95         15.0  0.433771  0.88025  0.952371  0.951497\n",
              "\n",
              "[270 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32LbJfGuuC_m"
      },
      "source": [
        "df.to_csv('performancemetrics-greaterrange-multihead')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "e5G23QDkHlqS",
        "outputId": "e93a3a2e-2b6a-4b7a-d430-4e6a08374359"
      },
      "source": [
        "#find best performing combination\r\n",
        "max_metric = experiment_data[0]\r\n",
        "for metrics in experiment_data:\r\n",
        "  if metrics[3]>max_metric[3]:\r\n",
        "    max_metric = metrics\r\n",
        "\r\n",
        "mf = pd.DataFrame(data={'Name':columns, 'Best Performance':max_metric})\r\n",
        "mf"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Best Performance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>alpha</td>\n",
              "      <td>0.950000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>temperature</td>\n",
              "      <td>9.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>loss</td>\n",
              "      <td>0.368708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ACC</td>\n",
              "      <td>0.896250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AUPR</td>\n",
              "      <td>0.961247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>AUROC</td>\n",
              "      <td>0.962551</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Name  Best Performance\n",
              "0        alpha          0.950000\n",
              "1  temperature          9.000000\n",
              "2         loss          0.368708\n",
              "3          ACC          0.896250\n",
              "4         AUPR          0.961247\n",
              "5        AUROC          0.962551"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    }
  ]
}