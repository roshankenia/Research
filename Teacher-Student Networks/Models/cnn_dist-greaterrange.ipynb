{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of cnn_dist.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHecrF91fJ3h"
      },
      "source": [
        "import numpy as np, os\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras as tfk\r\n",
        "keras = tfk\r\n",
        "import datetime as dt\r\n",
        "import six\r\n",
        "import h5py\r\n",
        "import requests\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt \r\n",
        "import seaborn as sns\r\n",
        "sns.set()\r\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3psP7yOQIVD"
      },
      "source": [
        "# Data loading "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKPa5-b4L7hX"
      },
      "source": [
        "# download the data \r\n",
        "url = 'https://www.dropbox.com/s/ysrim2re8mh22z9/synthetic_code_dataset.h5?dl=0'\r\n",
        "save_name = 'data.h5'\r\n",
        "_=!wget {url} -O {save_name}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCvBWh9oNYhd"
      },
      "source": [
        "# load the data into x_train, y_train, .....\r\n",
        "f = h5py.File(save_name, 'r')\r\n",
        "suffixes = ['train', 'test', 'valid']\r\n",
        "for suffix in suffixes:\r\n",
        "    exec(\"x_%s=np.transpose(f.get(\\\"X_%s\\\")[:], (0, 2, 1))\"%(suffix, suffix))\r\n",
        "    exec(\"y_%s=f.get(\\\"Y_%s\\\")[:]\"%(suffix, suffix))\r\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISQFZ12vQKeu"
      },
      "source": [
        "# Model definition function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5qiwx1sfthV"
      },
      "source": [
        "def get_activation(activation = 'relu'):\r\n",
        "    \"\"\"\r\n",
        "    Create an activation function. The activation argument should one of:\r\n",
        "    1. A string representing the keras name of the activation. \r\n",
        "    2. A callable which may or may not be an instance of keras.layers.Layer. \r\n",
        "    \"\"\"\r\n",
        "    if isinstance(activation, str):\r\n",
        "        actfn = tfk.layers.Activation(activation)\r\n",
        "    else:\r\n",
        "        if callable(activation) and not isinstance(activation, tfk.layers.Layer):\r\n",
        "            actfn = tfk.layers.Activation(activation)\r\n",
        "        else:\r\n",
        "            actfn = activation\r\n",
        "    return actfn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baDorqePRC-5"
      },
      "source": [
        "def conv_layer(x, num_filters, kernel_size, padding, activation, dropout=0.5, l2=1e-6, bn=True): \r\n",
        "    \"\"\"\r\n",
        "    A convolutional block comprising of a convolutional layer followed by\r\n",
        "    batch normalization, an activation function, and dropout. \r\n",
        "    \"\"\"\r\n",
        "    y = tfk.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, kernel_regularizer=tfk.regularizers.l2(l2), padding=padding)(x)\r\n",
        "    if bn:\r\n",
        "        y = tfk.layers.BatchNormalization()(y)\r\n",
        "    actfn = get_activation(activation)\r\n",
        "    y = actfn(y)\r\n",
        "    if dropout:\r\n",
        "        y = tfk.layers.Dropout(dropout)(y)\r\n",
        "    return y\r\n",
        "\r\n",
        "def dense_layer(x, num_units, activation, dropout=0.5, l2=None, bn=True):\r\n",
        "    \"\"\"\r\n",
        "    A dense block comprising of a dense layer followed by batch normalization, \r\n",
        "    activation and dropout. \r\n",
        "    \"\"\"\r\n",
        "    y = tfk.layers.Dense(num_units, use_bias=False, kernel_regularizer=tfk.regularizers.l2(l2))(x)\r\n",
        "    if bn:\r\n",
        "        y = tfk.layers.BatchNormalization()(y)\r\n",
        "    actfn = get_activation(activation)\r\n",
        "    y = actfn(y)\r\n",
        "    if dropout:\r\n",
        "        y = tfk.layers.Dropout(dropout)(y)\r\n",
        "    return y\r\n",
        "\r\n",
        "# def get_model(L, A, name=\"cnn_att\"):\r\n",
        "# \t## input layer\r\n",
        "# \tx = tfk.layers.Input((L, A), name='Input')\r\n",
        "\t\r\n",
        "# \t## 1st conv layer\r\n",
        "# \ty = keras.layers.Conv1D(filters=32, kernel_size=19, kernel_regularizer=tfk.regularizers.l2(1e-6), padding='same', name='conv1', use_bias=True)(x)\r\n",
        "# \ty = keras.layers.Activation('relu')(y)\r\n",
        "# \ty = keras.layers.MaxPool1D(pool_size=4)(y)\r\n",
        "\t\r\n",
        "# \t# multi head attention layer\r\n",
        "# \tembedding = keras.layers.Dropout(0.1)(y)\r\n",
        "# \ty, weights = keras.layers.MultiHeadAttention(num_heads=8, key_dim=64, value_dim=64)(embedding, embedding, return_attention_scores=True)\r\n",
        "# \ty = keras.layers.Dropout(0.1)(y)\r\n",
        "# \ty = keras.layers.LayerNormalization(epsilon=1e-6)(y)\r\n",
        "\t\r\n",
        "# \t# everything else\r\n",
        "# \ty = keras.layers.Flatten()(y)\r\n",
        "# \ty = keras.layers.Dense(128, activation=None, use_bias=False)(y)\r\n",
        "# \ty = keras.layers.BatchNormalization()(y)\r\n",
        "# \ty = keras.layers.Activation('relu')(y)\r\n",
        "# \ty = keras.layers.Dropout(0.5)(y)\r\n",
        "# \ty = keras.layers.Dense(1, name='logits')(y)\r\n",
        "# \ty = keras.layers.Activation('sigmoid', name='output')(y)\r\n",
        "# \tmodel = tfk.Model(inputs=x, outputs=y, name=name)\r\n",
        "# \treturn model\r\n",
        "\r\n",
        "def get_model(L, A, activation='relu', name='cnn_dist'):\r\n",
        "    \"\"\"\r\n",
        "    A function to assemble the full CNN distributed model. \r\n",
        "    \"\"\"\r\n",
        "    # input layer \r\n",
        "    x = tfk.layers.Input((L, A), name='input')\r\n",
        "\r\n",
        "    # 1st convolutional block \r\n",
        "    y = conv_layer(x,num_filters=24, kernel_size=19, padding='same', dropout=0.1,l2=1e-6, bn=True, activation=activation)\r\n",
        "    \r\n",
        "    # 2nd conv. block + pooling \r\n",
        "    y = conv_layer(y,num_filters=32, kernel_size=7, padding='same', activation=activation, dropout=0.2,l2=1e-6, bn=True)\r\n",
        "    y = tfk.layers.MaxPool1D(pool_size=4)(y)\r\n",
        "    \r\n",
        "    # 3rd convolutional block + pooling \r\n",
        "    y = conv_layer(y,num_filters=64, kernel_size=3, padding='same', activation=activation, dropout=0.4,l2=1e-6, bn=True)\r\n",
        "    y = tfk.layers.MaxPool1D(pool_size=3, strides=3, padding='same')(y)\r\n",
        "    \r\n",
        "    # dense block and final output layer \r\n",
        "    y = tfk.layers.Flatten()(y)\r\n",
        "    y = dense_layer(y, num_units=96, activation=activation, dropout=0.5, l2=1e-6, bn=True)\r\n",
        "    y = tfk.layers.Dense(1, use_bias=True, name = 'logits')(y)\r\n",
        "    y = tfk.layers.Activation('sigmoid')(y)\r\n",
        "\r\n",
        "    # assemble full model\r\n",
        "    model = tfk.Model(x, y, name=name)\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQzKg97ayWQ-"
      },
      "source": [
        "# Train a teacher model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUkdVPMdifWD",
        "outputId": "519a20e8-c3f8-4858-d031-cbb3f8ff3232"
      },
      "source": [
        "# instantiate the teacher model \r\n",
        "activation = 'relu' \r\n",
        "#activation = lambda x : tf.math.sin(x) + tf.math.cos(x)\r\n",
        "L, A = x_train.shape[1:]\r\n",
        "\r\n",
        "teacher_model = get_model(L, A, name='teacher')\r\n",
        "#teacher_model = get_model(L, A, activation, name='teacher')\r\n",
        "\r\n",
        "# compile the teacher model \r\n",
        "lossfn = tfk.losses.BinaryCrossentropy(name='bce')\r\n",
        "modelmetrics = [tfk.metrics.BinaryAccuracy(name='ACC'), tfk.metrics.AUC(curve='PR', name='AUPR'), tfk.metrics.AUC(curve='ROC', name='AUROC')]\r\n",
        "optimizer = tfk.optimizers.Adam(learning_rate=1e-2)\r\n",
        "teacher_model.compile(loss=lossfn, metrics=modelmetrics, optimizer=optimizer)\r\n",
        "\r\n",
        "# fit the teacher model \r\n",
        "num_epochs = 100\r\n",
        "callbacks = [tfk.callbacks.EarlyStopping(monitor='val_AUROC', patience=20), \r\n",
        "             tfk.callbacks.ModelCheckpoint(\"best_teacher_model.hdf5\", monitor='val_AUROC', mode='max', save_best_only=True)]\r\n",
        "teacher_model.fit(x_train, y_train, \r\n",
        "                    epochs=num_epochs, \r\n",
        "                    batch_size=128, \r\n",
        "                    callbacks=callbacks, \r\n",
        "                    shuffle=True, \r\n",
        "                    initial_epoch=0,\r\n",
        "                    validation_data=(x_valid, y_valid))\r\n",
        "teacher_model = tfk.models.load_model('best_teacher_model.hdf5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "110/110 [==============================] - 10s 18ms/step - loss: 0.6535 - ACC: 0.6771 - AUPR: 0.7147 - AUROC: 0.7406 - val_loss: 0.8775 - val_ACC: 0.6775 - val_AUPR: 0.8643 - val_AUROC: 0.8892\n",
            "Epoch 2/100\n",
            "110/110 [==============================] - 1s 13ms/step - loss: 0.4231 - ACC: 0.8217 - AUPR: 0.8952 - AUROC: 0.9020 - val_loss: 0.3482 - val_ACC: 0.8705 - val_AUPR: 0.9391 - val_AUROC: 0.9422\n",
            "Epoch 3/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.3444 - ACC: 0.8687 - AUPR: 0.9384 - AUROC: 0.9430 - val_loss: 0.4249 - val_ACC: 0.8390 - val_AUPR: 0.9572 - val_AUROC: 0.9582\n",
            "Epoch 4/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.3055 - ACC: 0.8920 - AUPR: 0.9526 - AUROC: 0.9575 - val_loss: 0.5409 - val_ACC: 0.7840 - val_AUPR: 0.9499 - val_AUROC: 0.9526\n",
            "Epoch 5/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.3068 - ACC: 0.8949 - AUPR: 0.9544 - AUROC: 0.9579 - val_loss: 0.3225 - val_ACC: 0.8830 - val_AUPR: 0.9607 - val_AUROC: 0.9640\n",
            "Epoch 6/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.2810 - ACC: 0.9043 - AUPR: 0.9641 - AUROC: 0.9661 - val_loss: 0.2772 - val_ACC: 0.9055 - val_AUPR: 0.9661 - val_AUROC: 0.9695\n",
            "Epoch 7/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.2719 - ACC: 0.9085 - AUPR: 0.9667 - AUROC: 0.9685 - val_loss: 1.8488 - val_ACC: 0.6340 - val_AUPR: 0.8847 - val_AUROC: 0.8394\n",
            "Epoch 8/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.2715 - ACC: 0.9081 - AUPR: 0.9675 - AUROC: 0.9687 - val_loss: 0.3876 - val_ACC: 0.8575 - val_AUPR: 0.9680 - val_AUROC: 0.9699\n",
            "Epoch 9/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.2691 - ACC: 0.9092 - AUPR: 0.9678 - AUROC: 0.9689 - val_loss: 0.4140 - val_ACC: 0.8360 - val_AUPR: 0.9729 - val_AUROC: 0.9753\n",
            "Epoch 10/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.2597 - ACC: 0.9118 - AUPR: 0.9694 - AUROC: 0.9710 - val_loss: 0.3657 - val_ACC: 0.8725 - val_AUPR: 0.9644 - val_AUROC: 0.9719\n",
            "Epoch 11/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.2630 - ACC: 0.9105 - AUPR: 0.9685 - AUROC: 0.9702 - val_loss: 0.2682 - val_ACC: 0.9135 - val_AUPR: 0.9696 - val_AUROC: 0.9727\n",
            "Epoch 12/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.2511 - ACC: 0.9170 - AUPR: 0.9692 - AUROC: 0.9729 - val_loss: 0.3941 - val_ACC: 0.8645 - val_AUPR: 0.9691 - val_AUROC: 0.9709\n",
            "Epoch 13/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.2537 - ACC: 0.9120 - AUPR: 0.9726 - AUROC: 0.9724 - val_loss: 0.5785 - val_ACC: 0.7595 - val_AUPR: 0.9696 - val_AUROC: 0.9765\n",
            "Epoch 14/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.2428 - ACC: 0.9142 - AUPR: 0.9744 - AUROC: 0.9753 - val_loss: 0.5562 - val_ACC: 0.8465 - val_AUPR: 0.9654 - val_AUROC: 0.9603\n",
            "Epoch 15/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.2510 - ACC: 0.9147 - AUPR: 0.9718 - AUROC: 0.9729 - val_loss: 2.9933 - val_ACC: 0.5640 - val_AUPR: 0.7759 - val_AUROC: 0.7072\n",
            "Epoch 16/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.2533 - ACC: 0.9118 - AUPR: 0.9719 - AUROC: 0.9725 - val_loss: 0.3264 - val_ACC: 0.8850 - val_AUPR: 0.9702 - val_AUROC: 0.9763\n",
            "Epoch 17/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.2534 - ACC: 0.9142 - AUPR: 0.9723 - AUROC: 0.9724 - val_loss: 0.3981 - val_ACC: 0.8585 - val_AUPR: 0.9708 - val_AUROC: 0.9719\n",
            "Epoch 18/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.2519 - ACC: 0.9159 - AUPR: 0.9726 - AUROC: 0.9730 - val_loss: 0.5725 - val_ACC: 0.7810 - val_AUPR: 0.9517 - val_AUROC: 0.9672\n",
            "Epoch 19/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.2419 - ACC: 0.9172 - AUPR: 0.9740 - AUROC: 0.9758 - val_loss: 0.7329 - val_ACC: 0.7490 - val_AUPR: 0.9638 - val_AUROC: 0.9610\n",
            "Epoch 20/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.2316 - ACC: 0.9228 - AUPR: 0.9767 - AUROC: 0.9775 - val_loss: 0.4375 - val_ACC: 0.8355 - val_AUPR: 0.9666 - val_AUROC: 0.9759\n",
            "Epoch 21/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.2396 - ACC: 0.9216 - AUPR: 0.9743 - AUROC: 0.9755 - val_loss: 1.3476 - val_ACC: 0.5605 - val_AUPR: 0.8971 - val_AUROC: 0.9426\n",
            "Epoch 22/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.2429 - ACC: 0.9203 - AUPR: 0.9751 - AUROC: 0.9752 - val_loss: 0.3447 - val_ACC: 0.8800 - val_AUPR: 0.9719 - val_AUROC: 0.9745\n",
            "Epoch 23/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.2310 - ACC: 0.9260 - AUPR: 0.9771 - AUROC: 0.9776 - val_loss: 0.3368 - val_ACC: 0.8730 - val_AUPR: 0.9748 - val_AUROC: 0.9793\n",
            "Epoch 24/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.2274 - ACC: 0.9258 - AUPR: 0.9781 - AUROC: 0.9784 - val_loss: 0.2407 - val_ACC: 0.9195 - val_AUPR: 0.9707 - val_AUROC: 0.9754\n",
            "Epoch 25/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.2424 - ACC: 0.9171 - AUPR: 0.9731 - AUROC: 0.9750 - val_loss: 0.4900 - val_ACC: 0.8040 - val_AUPR: 0.9690 - val_AUROC: 0.9786\n",
            "Epoch 26/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.2322 - ACC: 0.9240 - AUPR: 0.9767 - AUROC: 0.9774 - val_loss: 0.2815 - val_ACC: 0.9060 - val_AUPR: 0.9713 - val_AUROC: 0.9755\n",
            "Epoch 27/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.2195 - ACC: 0.9283 - AUPR: 0.9793 - AUROC: 0.9803 - val_loss: 0.2877 - val_ACC: 0.9095 - val_AUPR: 0.9704 - val_AUROC: 0.9733\n",
            "Epoch 28/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.2465 - ACC: 0.9195 - AUPR: 0.9738 - AUROC: 0.9742 - val_loss: 0.2716 - val_ACC: 0.9080 - val_AUPR: 0.9736 - val_AUROC: 0.9778\n",
            "Epoch 29/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.2431 - ACC: 0.9211 - AUPR: 0.9742 - AUROC: 0.9748 - val_loss: 0.2247 - val_ACC: 0.9315 - val_AUPR: 0.9773 - val_AUROC: 0.9789\n",
            "Epoch 30/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.2291 - ACC: 0.9255 - AUPR: 0.9771 - AUROC: 0.9780 - val_loss: 0.2549 - val_ACC: 0.9140 - val_AUPR: 0.9680 - val_AUROC: 0.9738\n",
            "Epoch 31/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.2395 - ACC: 0.9212 - AUPR: 0.9742 - AUROC: 0.9754 - val_loss: 0.4508 - val_ACC: 0.8620 - val_AUPR: 0.9703 - val_AUROC: 0.9685\n",
            "Epoch 32/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.2346 - ACC: 0.9238 - AUPR: 0.9764 - AUROC: 0.9769 - val_loss: 0.2521 - val_ACC: 0.9185 - val_AUPR: 0.9708 - val_AUROC: 0.9764\n",
            "Epoch 33/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.2272 - ACC: 0.9264 - AUPR: 0.9775 - AUROC: 0.9784 - val_loss: 0.3804 - val_ACC: 0.8765 - val_AUPR: 0.9707 - val_AUROC: 0.9710\n",
            "Epoch 34/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.2433 - ACC: 0.9206 - AUPR: 0.9744 - AUROC: 0.9749 - val_loss: 0.3191 - val_ACC: 0.8890 - val_AUPR: 0.9608 - val_AUROC: 0.9713\n",
            "Epoch 35/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.2338 - ACC: 0.9227 - AUPR: 0.9762 - AUROC: 0.9767 - val_loss: 0.3405 - val_ACC: 0.8880 - val_AUPR: 0.9724 - val_AUROC: 0.9713\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kpvZ10Vvxcr"
      },
      "source": [
        "teacher_model = tfk.models.load_model('best_teacher_model.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lep5Pun5E0k-"
      },
      "source": [
        "# Knowledge distillation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTehiZ5nGzou"
      },
      "source": [
        "## Define a `Distiller` class that takes in a trained teacher model, an untrained student model and distills the knowledge in the teacher model onto the student model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tai-epSfGkUz"
      },
      "source": [
        "class Distiller(keras.Model):\r\n",
        "    def get_config(self,):\r\n",
        "        \"\"\"\r\n",
        "        Implement the config dictionary to enable serialization\r\n",
        "        \"\"\"\r\n",
        "        config = {}\r\n",
        "        config['student'] = self.student\r\n",
        "        config['teacher'] = self.teacher\r\n",
        "        return config\r\n",
        "    \r\n",
        "    def __init__(self, student, teacher):\r\n",
        "        super(Distiller, self).__init__()\r\n",
        "        self.teacher = teacher\r\n",
        "        self.student = student\r\n",
        "\r\n",
        "    def compile(\r\n",
        "        self,\r\n",
        "        optimizer,\r\n",
        "        metrics,\r\n",
        "        student_loss_fn,\r\n",
        "        distillation_loss_fn,\r\n",
        "        alpha=0.1,\r\n",
        "        temperature=3,\r\n",
        "    ):\r\n",
        "        \"\"\" Configure the distiller.\r\n",
        "\r\n",
        "        Args:\r\n",
        "            optimizer: Keras optimizer for the student weights\r\n",
        "            metrics: Keras metrics for evaluation\r\n",
        "            student_loss_fn: Loss function of difference between student\r\n",
        "                predictions and ground-truth\r\n",
        "            distillation_loss_fn: Loss function of difference between soft\r\n",
        "                student predictions and soft teacher predictions\r\n",
        "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\r\n",
        "            temperature: Temperature for softening probability distributions.\r\n",
        "                Larger temperature gives softer distributions.\r\n",
        "        \"\"\"\r\n",
        "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\r\n",
        "        self.student_loss_fn = student_loss_fn\r\n",
        "        self.distillation_loss_fn = distillation_loss_fn\r\n",
        "        self.alpha = alpha\r\n",
        "        self.temperature = temperature\r\n",
        "\r\n",
        "    def train_step(self, data):\r\n",
        "        # Unpack data\r\n",
        "        x, y = data\r\n",
        "\r\n",
        "        # Forward pass of teacher\r\n",
        "        teacher_predictions = self.teacher(x, training=False)\r\n",
        "\r\n",
        "        with tf.GradientTape() as tape:\r\n",
        "            # Forward pass of student\r\n",
        "            student_predictions = self.student(x, training=True)\r\n",
        "\r\n",
        "            # Compute losses\r\n",
        "            student_loss = self.student_loss_fn(y, student_predictions)\r\n",
        "            distillation_loss = self.distillation_loss_fn(\r\n",
        "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\r\n",
        "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\r\n",
        "            )\r\n",
        "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\r\n",
        "\r\n",
        "        # Compute gradients\r\n",
        "        trainable_vars = self.student.trainable_variables\r\n",
        "        gradients = tape.gradient(loss, trainable_vars)\r\n",
        "\r\n",
        "        # Update weights\r\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\r\n",
        "\r\n",
        "        # Update the metrics configured in `compile()`.\r\n",
        "        self.compiled_metrics.update_state(y, student_predictions)\r\n",
        "\r\n",
        "        # Return a dict of performance\r\n",
        "        results = {m.name: m.result() for m in self.metrics}\r\n",
        "        results.update(\r\n",
        "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\r\n",
        "        )\r\n",
        "        return results\r\n",
        "\r\n",
        "    def test_step(self, data):\r\n",
        "        # Unpack the data\r\n",
        "        x, y = data\r\n",
        "\r\n",
        "        # Compute predictions\r\n",
        "        y_prediction = self.student(x, training=False)\r\n",
        "\r\n",
        "        # Calculate the loss\r\n",
        "        student_loss = self.student_loss_fn(y, y_prediction)\r\n",
        "\r\n",
        "        # Update the metrics.\r\n",
        "        self.compiled_metrics.update_state(y, y_prediction)\r\n",
        "\r\n",
        "        # Return a dict of performance\r\n",
        "        results = {\"student_loss\": student_loss}\r\n",
        "        results.update({m.name: m.result() for m in self.metrics})\r\n",
        "        return results\r\n",
        "    \r\n",
        "    @property\r\n",
        "    def metrics_names(self):\r\n",
        "        return ['student_loss']+[m.name for m in self.metrics]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvOT3OZDHCgu"
      },
      "source": [
        "def get_student_model(L, A, activation='relu', name='deepbind'):\r\n",
        "    \"\"\"\r\n",
        "    Defining the deepbind architecture in here. \r\n",
        "    \"\"\"\r\n",
        "    x = tfk.layers.Input((L, A), name='input')\r\n",
        "    y = tfk.layers.Conv1D(filters=16, kernel_size=24, padding='valid', kernel_regularizer=tfk.regularizers.l2(1e-6))(x)\r\n",
        "    actfn = get_activation(activation=activation)\r\n",
        "    y = actfn(y)\r\n",
        "    y = tfk.layers.Lambda(lambda x : tf.reduce_max(x, axis=1))(y)  # max pooling\r\n",
        "    y = tfk.layers.Dropout(0.5)(y)  \r\n",
        "    y = tfk.layers.Dense(32, activation='relu')(y)\r\n",
        "    y = tfk.layers.Dense(1, name='logits')(y)\r\n",
        "    y = tfk.layers.Activation('sigmoid', name='output')(y)\r\n",
        "\r\n",
        "    model = tfk.Model(inputs=x, outputs=y, name=name)\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K79gsB5RIhPV",
        "outputId": "afe1dba5-758e-4778-92ec-c7d4f440f748"
      },
      "source": [
        "# instantiate the student model and the distiller \r\n",
        "student_model = get_student_model(L, A)\r\n",
        "distiller = Distiller(student_model, teacher_model)\r\n",
        "\r\n",
        "# compile the distiller\r\n",
        "alpha = 0.8\r\n",
        "temperature = 1. \r\n",
        "distiller.compile(\r\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\r\n",
        "    metrics=modelmetrics,\r\n",
        "    student_loss_fn=keras.losses.BinaryCrossentropy(name='bce'),\r\n",
        "    distillation_loss_fn=keras.losses.KLDivergence(),\r\n",
        "    alpha=alpha,\r\n",
        "    temperature=temperature,\r\n",
        ")\r\n",
        "\r\n",
        "# perform distillation\r\n",
        "num_epochs = 50\r\n",
        "callbacks = [tfk.callbacks.EarlyStopping(monitor='val_AUROC', patience=20), \r\n",
        "             tfk.callbacks.ModelCheckpoint(\"best_distiller.hdf5\", monitor='val_AUROC', mode='max',save_weights_only=True, save_best_only=True)]\r\n",
        "distiller.fit(x_train, y_train, \r\n",
        "                epochs=num_epochs, \r\n",
        "                batch_size=128, \r\n",
        "                callbacks=callbacks, \r\n",
        "                shuffle=True, \r\n",
        "                validation_data=(x_valid, y_valid))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "110/110 [==============================] - 2s 11ms/step - ACC: 0.6177 - AUPR: 0.7117 - AUROC: 0.6928 - student_loss: 0.7002 - distillation_loss: 0.0000e+00 - val_student_loss: 0.6839 - val_ACC: 0.5110 - val_AUPR: 0.6318 - val_AUROC: 0.6358\n",
            "Epoch 2/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.5497 - AUPR: 0.5751 - AUROC: 0.5786 - student_loss: 0.6779 - distillation_loss: 0.0000e+00 - val_student_loss: 0.6589 - val_ACC: 0.6310 - val_AUPR: 0.7732 - val_AUROC: 0.7769\n",
            "Epoch 3/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.6267 - AUPR: 0.6850 - AUROC: 0.6856 - student_loss: 0.6383 - distillation_loss: 0.0000e+00 - val_student_loss: 0.5962 - val_ACC: 0.7300 - val_AUPR: 0.8052 - val_AUROC: 0.8205\n",
            "Epoch 4/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.6881 - AUPR: 0.7451 - AUROC: 0.7521 - student_loss: 0.5858 - distillation_loss: 0.0000e+00 - val_student_loss: 0.5571 - val_ACC: 0.7515 - val_AUPR: 0.8221 - val_AUROC: 0.8350\n",
            "Epoch 5/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.7139 - AUPR: 0.7876 - AUROC: 0.7873 - student_loss: 0.5551 - distillation_loss: 0.0000e+00 - val_student_loss: 0.5381 - val_ACC: 0.7705 - val_AUPR: 0.8423 - val_AUROC: 0.8513\n",
            "Epoch 6/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.7291 - AUPR: 0.8108 - AUROC: 0.8100 - student_loss: 0.5309 - distillation_loss: 0.0000e+00 - val_student_loss: 0.5194 - val_ACC: 0.7835 - val_AUPR: 0.8608 - val_AUROC: 0.8673\n",
            "Epoch 7/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.7465 - AUPR: 0.8254 - AUROC: 0.8258 - student_loss: 0.5135 - distillation_loss: 0.0000e+00 - val_student_loss: 0.5103 - val_ACC: 0.7995 - val_AUPR: 0.8759 - val_AUROC: 0.8809\n",
            "Epoch 8/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.7508 - AUPR: 0.8376 - AUROC: 0.8354 - student_loss: 0.5040 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4760 - val_ACC: 0.8090 - val_AUPR: 0.8884 - val_AUROC: 0.8926\n",
            "Epoch 9/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.7622 - AUPR: 0.8418 - AUROC: 0.8449 - student_loss: 0.4906 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4615 - val_ACC: 0.8180 - val_AUPR: 0.8965 - val_AUROC: 0.9001\n",
            "Epoch 10/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.7753 - AUPR: 0.8498 - AUROC: 0.8560 - student_loss: 0.4770 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4526 - val_ACC: 0.8170 - val_AUPR: 0.8971 - val_AUROC: 0.9014\n",
            "Epoch 11/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.7683 - AUPR: 0.8546 - AUROC: 0.8554 - student_loss: 0.4705 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4307 - val_ACC: 0.8345 - val_AUPR: 0.9099 - val_AUROC: 0.9132\n",
            "Epoch 12/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.7801 - AUPR: 0.8677 - AUROC: 0.8661 - student_loss: 0.4595 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4326 - val_ACC: 0.8400 - val_AUPR: 0.9187 - val_AUROC: 0.9203\n",
            "Epoch 13/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.8003 - AUPR: 0.8760 - AUROC: 0.8806 - student_loss: 0.4327 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4016 - val_ACC: 0.8535 - val_AUPR: 0.9281 - val_AUROC: 0.9309\n",
            "Epoch 14/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.8057 - AUPR: 0.8942 - AUROC: 0.8898 - student_loss: 0.4256 - distillation_loss: 0.0000e+00 - val_student_loss: 0.3834 - val_ACC: 0.8600 - val_AUPR: 0.9297 - val_AUROC: 0.9330\n",
            "Epoch 15/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.8042 - AUPR: 0.8881 - AUROC: 0.8862 - student_loss: 0.4275 - distillation_loss: 0.0000e+00 - val_student_loss: 0.3812 - val_ACC: 0.8575 - val_AUPR: 0.9310 - val_AUROC: 0.9348\n",
            "Epoch 16/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.8120 - AUPR: 0.8914 - AUROC: 0.8879 - student_loss: 0.4221 - distillation_loss: 0.0000e+00 - val_student_loss: 0.3778 - val_ACC: 0.8605 - val_AUPR: 0.9364 - val_AUROC: 0.9408\n",
            "Epoch 17/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.8072 - AUPR: 0.8909 - AUROC: 0.8897 - student_loss: 0.4211 - distillation_loss: 0.0000e+00 - val_student_loss: 0.3754 - val_ACC: 0.8630 - val_AUPR: 0.9343 - val_AUROC: 0.9377\n",
            "Epoch 18/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.8064 - AUPR: 0.8890 - AUROC: 0.8886 - student_loss: 0.4155 - distillation_loss: 0.0000e+00 - val_student_loss: 0.3715 - val_ACC: 0.8605 - val_AUPR: 0.9348 - val_AUROC: 0.9382\n",
            "Epoch 19/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.8143 - AUPR: 0.8979 - AUROC: 0.8951 - student_loss: 0.4102 - distillation_loss: 0.0000e+00 - val_student_loss: 0.3708 - val_ACC: 0.8670 - val_AUPR: 0.9395 - val_AUROC: 0.9425\n",
            "Epoch 20/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.8154 - AUPR: 0.8992 - AUROC: 0.8970 - student_loss: 0.4073 - distillation_loss: 0.0000e+00 - val_student_loss: 0.3625 - val_ACC: 0.8725 - val_AUPR: 0.9386 - val_AUROC: 0.9414\n",
            "Epoch 21/50\n",
            "110/110 [==============================] - 1s 7ms/step - ACC: 0.8150 - AUPR: 0.9032 - AUROC: 0.8984 - student_loss: 0.4022 - distillation_loss: 0.0000e+00 - val_student_loss: 0.3672 - val_ACC: 0.8740 - val_AUPR: 0.9399 - val_AUROC: 0.9419\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4565ed06d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eg2dHq2vKK70"
      },
      "source": [
        "def plot_f_and_grad(model):\r\n",
        "    # pick a random sample \r\n",
        "    N, L, A = x_train.shape\r\n",
        "    xsample = x_train[np.random.randint(0, N)][None, :, :]\r\n",
        "\r\n",
        "    # define a keras model mapping an input sequence to the logits of the teacher model\r\n",
        "    func = tfk.Model(inputs=model.input, outputs=model.get_layer('logits').output)\r\n",
        "\r\n",
        "    # define a set of probe sequences by sampling points in the ith nucleotide, jth channel \r\n",
        "    # i and j are picked randomly\r\n",
        "    n_probe = 100\r\n",
        "    x_probe = np.linspace(0, 1, n_probe)\r\n",
        "    n_samples = 50\r\n",
        "    Is, Js, y_ijs, y_ij_grads = [], [], [], []\r\n",
        "    for i in range(n_samples):  \r\n",
        "        i, j = np.random.randint(0, L), np.random.randint(0, A)\r\n",
        "        Is.append(i)\r\n",
        "        Js.append(j)\r\n",
        "        \r\n",
        "        x_ij_probe = np.zeros((n_probe, L, A))\r\n",
        "        x_ij_probe[:, i, j] = x_probe\r\n",
        "        x_ij_probe = tf.convert_to_tensor(x_ij_probe)\r\n",
        "\r\n",
        "        with tf.GradientTape() as tape:\r\n",
        "            tape.watch(x_ij_probe)\r\n",
        "            y_ij_pred = func(x_ij_probe)\r\n",
        "        y_ij_grad = tape.gradient(y_ij_pred, x_ij_probe)\r\n",
        "        \r\n",
        "        #y_ij_pred = func(x_ij_probe)\r\n",
        "        y_ijs.append(y_ij_pred.numpy())\r\n",
        "        y_ij_grads.append(y_ij_grad.numpy()[:, i, j])\r\n",
        "\r\n",
        "    # plot\r\n",
        "    fig = plt.figure(figsize=(14, 10))\r\n",
        "    for k in range(4):\r\n",
        "        idx = np.random.randint(0, len(Is))\r\n",
        "        i = Is[idx]\r\n",
        "        j = Js[idx]\r\n",
        "        ax = fig.add_subplot(2,2,k+1)\r\n",
        "        ax1 = ax.twinx()\r\n",
        "        title=\"i=%d, j=%d\"%(i, j)\r\n",
        "        figure_options = {'linewidth':2}\r\n",
        "\r\n",
        "        c, c1 = 'blue', 'red'\r\n",
        "        ax.plot(x_probe, y_ijs[idx], color=c, label='$f(x)$',**figure_options)\r\n",
        "        ax.tick_params(axis='y', color=c, labelcolor=c)\r\n",
        "        ax.legend(loc='upper right', fontsize=15)\r\n",
        "        \r\n",
        "        ax1.plot(x_probe, y_ij_grads[idx], color=c1, label=\"$\\\\nabla f_{ij}$\", **figure_options)\r\n",
        "        ax1.tick_params(axis='y',color=c1, labelcolor=c1)\r\n",
        "        ax1.legend(loc='lower left', fontsize=15)\r\n",
        "\r\n",
        "        ax.set_title(title, fontsize=15)\r\n",
        "    fig.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYBEWe-SqBDo"
      },
      "source": [
        "#plot_f_and_grad(distiller.teacher)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfcn7FDxKmlD"
      },
      "source": [
        "#plot_f_and_grad(distiller.student)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43FEwHoLwI1I"
      },
      "source": [
        "## Train a simple student model from scratch without distillation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0HgMTuQoSNj",
        "outputId": "d8672ee0-d5f1-4b44-f497-fb2e01d82fe3"
      },
      "source": [
        "# train a deep bind model by itself \r\n",
        "deepbind_model = get_student_model(L, A)\r\n",
        "\r\n",
        "# compile the model \r\n",
        "lossfn = tfk.losses.BinaryCrossentropy(name='bce')\r\n",
        "modelmetrics = [tfk.metrics.BinaryAccuracy(name='ACC'), tfk.metrics.AUC(curve='PR', name='AUPR'), tfk.metrics.AUC(curve='ROC', name='AUROC')]\r\n",
        "optimizer = tfk.optimizers.Adam(learning_rate=1e-2)\r\n",
        "deepbind_model.compile(loss=lossfn, metrics=modelmetrics, optimizer=optimizer)\r\n",
        "\r\n",
        "# fit the teacher model \r\n",
        "num_epochs = 100\r\n",
        "callbacks = [tfk.callbacks.EarlyStopping(monitor='val_AUROC', patience=20), \r\n",
        "             tfk.callbacks.ModelCheckpoint(\"best_deepbind_model.hdf5\", monitor='val_AUROC', mode='max', save_best_only=True)]\r\n",
        "deepbind_model.fit(x_train, y_train, \r\n",
        "                    epochs=num_epochs, \r\n",
        "                    batch_size=128, \r\n",
        "                    callbacks=callbacks, \r\n",
        "                    shuffle=True, \r\n",
        "                    initial_epoch=0,\r\n",
        "                    validation_data=(x_valid, y_valid))\r\n",
        "deepbind_model = tfk.models.load_model('best_deepbind_model.hdf5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "110/110 [==============================] - 2s 9ms/step - loss: 0.6914 - ACC: 0.5385 - AUPR: 0.5530 - AUROC: 0.5517 - val_loss: 0.5416 - val_ACC: 0.7695 - val_AUPR: 0.8406 - val_AUROC: 0.8507\n",
            "Epoch 2/100\n",
            "110/110 [==============================] - 1s 5ms/step - loss: 0.5979 - ACC: 0.6744 - AUPR: 0.7422 - AUROC: 0.7399 - val_loss: 0.4926 - val_ACC: 0.7965 - val_AUPR: 0.8843 - val_AUROC: 0.8896\n",
            "Epoch 3/100\n",
            "110/110 [==============================] - 1s 5ms/step - loss: 0.5297 - ACC: 0.7344 - AUPR: 0.8141 - AUROC: 0.8094 - val_loss: 0.4571 - val_ACC: 0.8310 - val_AUPR: 0.9016 - val_AUROC: 0.9107\n",
            "Epoch 4/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.5061 - ACC: 0.7526 - AUPR: 0.8344 - AUROC: 0.8297 - val_loss: 0.4575 - val_ACC: 0.8320 - val_AUPR: 0.9067 - val_AUROC: 0.9162\n",
            "Epoch 5/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.5013 - ACC: 0.7574 - AUPR: 0.8362 - AUROC: 0.8339 - val_loss: 0.4246 - val_ACC: 0.8440 - val_AUPR: 0.9179 - val_AUROC: 0.9235\n",
            "Epoch 6/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4872 - ACC: 0.7719 - AUPR: 0.8541 - AUROC: 0.8441 - val_loss: 0.4346 - val_ACC: 0.8405 - val_AUPR: 0.9124 - val_AUROC: 0.9165\n",
            "Epoch 7/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4866 - ACC: 0.7630 - AUPR: 0.8489 - AUROC: 0.8445 - val_loss: 0.4301 - val_ACC: 0.8430 - val_AUPR: 0.9198 - val_AUROC: 0.9252\n",
            "Epoch 8/100\n",
            "110/110 [==============================] - 1s 5ms/step - loss: 0.4825 - ACC: 0.7694 - AUPR: 0.8499 - AUROC: 0.8470 - val_loss: 0.4298 - val_ACC: 0.8295 - val_AUPR: 0.9215 - val_AUROC: 0.9270\n",
            "Epoch 9/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4766 - ACC: 0.7765 - AUPR: 0.8535 - AUROC: 0.8519 - val_loss: 0.4700 - val_ACC: 0.7855 - val_AUPR: 0.9211 - val_AUROC: 0.9289\n",
            "Epoch 10/100\n",
            "110/110 [==============================] - 1s 5ms/step - loss: 0.4768 - ACC: 0.7762 - AUPR: 0.8570 - AUROC: 0.8524 - val_loss: 0.4443 - val_ACC: 0.8350 - val_AUPR: 0.9172 - val_AUROC: 0.9251\n",
            "Epoch 11/100\n",
            "110/110 [==============================] - 1s 5ms/step - loss: 0.4676 - ACC: 0.7811 - AUPR: 0.8603 - AUROC: 0.8590 - val_loss: 0.4473 - val_ACC: 0.8105 - val_AUPR: 0.9200 - val_AUROC: 0.9249\n",
            "Epoch 12/100\n",
            "110/110 [==============================] - 1s 5ms/step - loss: 0.4640 - ACC: 0.7786 - AUPR: 0.8615 - AUROC: 0.8611 - val_loss: 0.5150 - val_ACC: 0.7260 - val_AUPR: 0.9164 - val_AUROC: 0.9240\n",
            "Epoch 13/100\n",
            "110/110 [==============================] - 1s 5ms/step - loss: 0.4787 - ACC: 0.7738 - AUPR: 0.8508 - AUROC: 0.8510 - val_loss: 0.4538 - val_ACC: 0.8130 - val_AUPR: 0.9224 - val_AUROC: 0.9265\n",
            "Epoch 14/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4621 - ACC: 0.7830 - AUPR: 0.8655 - AUROC: 0.8619 - val_loss: 0.4727 - val_ACC: 0.7780 - val_AUPR: 0.9191 - val_AUROC: 0.9250\n",
            "Epoch 15/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4703 - ACC: 0.7793 - AUPR: 0.8632 - AUROC: 0.8563 - val_loss: 0.4816 - val_ACC: 0.7620 - val_AUPR: 0.9186 - val_AUROC: 0.9268\n",
            "Epoch 16/100\n",
            "110/110 [==============================] - 1s 5ms/step - loss: 0.4643 - ACC: 0.7780 - AUPR: 0.8656 - AUROC: 0.8594 - val_loss: 0.5078 - val_ACC: 0.7365 - val_AUPR: 0.9178 - val_AUROC: 0.9279\n",
            "Epoch 17/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4506 - ACC: 0.7915 - AUPR: 0.8719 - AUROC: 0.8697 - val_loss: 0.5294 - val_ACC: 0.6885 - val_AUPR: 0.9210 - val_AUROC: 0.9312\n",
            "Epoch 18/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4651 - ACC: 0.7763 - AUPR: 0.8619 - AUROC: 0.8597 - val_loss: 0.4802 - val_ACC: 0.7620 - val_AUPR: 0.9215 - val_AUROC: 0.9289\n",
            "Epoch 19/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4590 - ACC: 0.7870 - AUPR: 0.8675 - AUROC: 0.8646 - val_loss: 0.5038 - val_ACC: 0.7345 - val_AUPR: 0.9184 - val_AUROC: 0.9276\n",
            "Epoch 20/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4562 - ACC: 0.7878 - AUPR: 0.8725 - AUROC: 0.8652 - val_loss: 0.4775 - val_ACC: 0.7605 - val_AUPR: 0.9226 - val_AUROC: 0.9294\n",
            "Epoch 21/100\n",
            "110/110 [==============================] - 1s 5ms/step - loss: 0.4581 - ACC: 0.7832 - AUPR: 0.8670 - AUROC: 0.8638 - val_loss: 0.4673 - val_ACC: 0.7660 - val_AUPR: 0.9266 - val_AUROC: 0.9336\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_LNYVnewPy7"
      },
      "source": [
        "## Compute metrics on all 3 models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "3ovGuCNkuXuv",
        "outputId": "d48b36cf-d62e-4537-9589-38a2584e990e"
      },
      "source": [
        "distilled_student_metrics = distiller.evaluate(x_test, y_test, verbose=False)\r\n",
        "teacher_metrics = distiller.teacher.evaluate(x_test, y_test, verbose=False)\r\n",
        "deepbind_from_scratch_metrics = deepbind_model.evaluate(x_test, y_test, verbose=False)\r\n",
        "names = deepbind_model.metrics_names\r\n",
        "df = pd.DataFrame(data={'Name':names, 'Student (distilled)':distilled_student_metrics, 'Student (from scratch)':deepbind_from_scratch_metrics, 'Teacher ':teacher_metrics})\r\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Student (distilled)</th>\n",
              "      <th>Student (from scratch)</th>\n",
              "      <th>Teacher</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>loss</td>\n",
              "      <td>0.430821</td>\n",
              "      <td>0.451924</td>\n",
              "      <td>0.335858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ACC</td>\n",
              "      <td>0.871000</td>\n",
              "      <td>0.782750</td>\n",
              "      <td>0.868000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AUPR</td>\n",
              "      <td>0.947302</td>\n",
              "      <td>0.935443</td>\n",
              "      <td>0.977889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AUROC</td>\n",
              "      <td>0.945261</td>\n",
              "      <td>0.940807</td>\n",
              "      <td>0.979868</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Name  Student (distilled)  Student (from scratch)  Teacher \n",
              "0   loss             0.430821                0.451924  0.335858\n",
              "1    ACC             0.871000                0.782750  0.868000\n",
              "2   AUPR             0.947302                0.935443  0.977889\n",
              "3  AUROC             0.945261                0.940807  0.979868"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_QRpUulvAfV"
      },
      "source": [
        "df.to_csv('modelmetrics-greaterrange')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD54SNjNo3Kx"
      },
      "source": [
        "##Run experiment on varying alpha and temperature values\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPsxCNE-jHe1",
        "outputId": "28f6f09c-42f1-4f3e-a583-254de48c26fc"
      },
      "source": [
        "alpha_values = [.1,.2,.3,.4,.5,.6,.7,.8,.9]\r\n",
        "temperature_values = [1.,2.,3.,4.,5.,6.,7.,8.,9.,10.,11.,12.,13.,14.,15.]\r\n",
        "\r\n",
        "experiment_data=[]\r\n",
        "\r\n",
        "\r\n",
        "for alpha_value in alpha_values:\r\n",
        "  for temperature_value in temperature_values:\r\n",
        "    print('Alpha: ',alpha_value, ', Temperature: ',temperature_value)\r\n",
        "    # instantiate the student model and the distiller\r\n",
        "    student_model = get_student_model(L, A)\r\n",
        "    distiller = Distiller(student_model, teacher_model)\r\n",
        "    # compile the distiller\r\n",
        "    alpha = alpha_value\r\n",
        "    temperature = temperature_value\r\n",
        "    distiller.compile(\r\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\r\n",
        "        metrics=modelmetrics,\r\n",
        "        student_loss_fn=keras.losses.BinaryCrossentropy(name='bce'),\r\n",
        "        distillation_loss_fn=keras.losses.KLDivergence(),\r\n",
        "        alpha=alpha,\r\n",
        "        temperature=temperature,\r\n",
        "    )\r\n",
        "\r\n",
        "    # perform distillation\r\n",
        "    num_epochs = 50\r\n",
        "    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_AUROC', patience=20), \r\n",
        "                tfk.callbacks.ModelCheckpoint(\"best_distiller.hdf5\", monitor='val_AUROC', mode='max',save_weights_only=True, save_best_only=True)]\r\n",
        "    distiller.fit(x_train, y_train, \r\n",
        "                    epochs=num_epochs, \r\n",
        "                    batch_size=128, \r\n",
        "                    callbacks=callbacks, \r\n",
        "                    shuffle=True, \r\n",
        "                    validation_data=(x_valid, y_valid),\r\n",
        "                    verbose=0)\r\n",
        "    \r\n",
        "    #evaluate and save Distilled metrics\r\n",
        "    experiment_dist_student_metrics = distiller.evaluate(x_test, y_test, verbose=False)\r\n",
        "    hyperparameters = [alpha_value, temperature_value]\r\n",
        "    all_values = hyperparameters+experiment_dist_student_metrics\r\n",
        "    #add data to list\r\n",
        "    experiment_data.append(all_values)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Alpha:  0.05 , Temperature:  1.0\n",
            "Alpha:  0.05 , Temperature:  2.0\n",
            "Alpha:  0.05 , Temperature:  3.0\n",
            "Alpha:  0.05 , Temperature:  4.0\n",
            "Alpha:  0.05 , Temperature:  5.0\n",
            "Alpha:  0.05 , Temperature:  6.0\n",
            "Alpha:  0.05 , Temperature:  7.0\n",
            "Alpha:  0.05 , Temperature:  8.0\n",
            "Alpha:  0.05 , Temperature:  9.0\n",
            "Alpha:  0.05 , Temperature:  10.0\n",
            "Alpha:  0.05 , Temperature:  11.0\n",
            "Alpha:  0.05 , Temperature:  12.0\n",
            "Alpha:  0.05 , Temperature:  13.0\n",
            "Alpha:  0.05 , Temperature:  14.0\n",
            "Alpha:  0.05 , Temperature:  15.0\n",
            "Alpha:  0.1 , Temperature:  1.0\n",
            "Alpha:  0.1 , Temperature:  2.0\n",
            "Alpha:  0.1 , Temperature:  3.0\n",
            "Alpha:  0.1 , Temperature:  4.0\n",
            "Alpha:  0.1 , Temperature:  5.0\n",
            "Alpha:  0.1 , Temperature:  6.0\n",
            "Alpha:  0.1 , Temperature:  7.0\n",
            "Alpha:  0.1 , Temperature:  8.0\n",
            "Alpha:  0.1 , Temperature:  9.0\n",
            "Alpha:  0.1 , Temperature:  10.0\n",
            "Alpha:  0.1 , Temperature:  11.0\n",
            "Alpha:  0.1 , Temperature:  12.0\n",
            "Alpha:  0.1 , Temperature:  13.0\n",
            "Alpha:  0.1 , Temperature:  14.0\n",
            "Alpha:  0.1 , Temperature:  15.0\n",
            "Alpha:  0.15 , Temperature:  1.0\n",
            "Alpha:  0.15 , Temperature:  2.0\n",
            "Alpha:  0.15 , Temperature:  3.0\n",
            "Alpha:  0.15 , Temperature:  4.0\n",
            "Alpha:  0.15 , Temperature:  5.0\n",
            "Alpha:  0.15 , Temperature:  6.0\n",
            "Alpha:  0.15 , Temperature:  7.0\n",
            "Alpha:  0.15 , Temperature:  8.0\n",
            "Alpha:  0.15 , Temperature:  9.0\n",
            "Alpha:  0.15 , Temperature:  10.0\n",
            "Alpha:  0.15 , Temperature:  11.0\n",
            "Alpha:  0.15 , Temperature:  12.0\n",
            "Alpha:  0.15 , Temperature:  13.0\n",
            "Alpha:  0.15 , Temperature:  14.0\n",
            "Alpha:  0.15 , Temperature:  15.0\n",
            "Alpha:  0.2 , Temperature:  1.0\n",
            "Alpha:  0.2 , Temperature:  2.0\n",
            "Alpha:  0.2 , Temperature:  3.0\n",
            "Alpha:  0.2 , Temperature:  4.0\n",
            "Alpha:  0.2 , Temperature:  5.0\n",
            "Alpha:  0.2 , Temperature:  6.0\n",
            "Alpha:  0.2 , Temperature:  7.0\n",
            "Alpha:  0.2 , Temperature:  8.0\n",
            "Alpha:  0.2 , Temperature:  9.0\n",
            "Alpha:  0.2 , Temperature:  10.0\n",
            "Alpha:  0.2 , Temperature:  11.0\n",
            "Alpha:  0.2 , Temperature:  12.0\n",
            "Alpha:  0.2 , Temperature:  13.0\n",
            "Alpha:  0.2 , Temperature:  14.0\n",
            "Alpha:  0.2 , Temperature:  15.0\n",
            "Alpha:  0.25 , Temperature:  1.0\n",
            "Alpha:  0.25 , Temperature:  2.0\n",
            "Alpha:  0.25 , Temperature:  3.0\n",
            "Alpha:  0.25 , Temperature:  4.0\n",
            "Alpha:  0.25 , Temperature:  5.0\n",
            "Alpha:  0.25 , Temperature:  6.0\n",
            "Alpha:  0.25 , Temperature:  7.0\n",
            "Alpha:  0.25 , Temperature:  8.0\n",
            "Alpha:  0.25 , Temperature:  9.0\n",
            "Alpha:  0.25 , Temperature:  10.0\n",
            "Alpha:  0.25 , Temperature:  11.0\n",
            "Alpha:  0.25 , Temperature:  12.0\n",
            "Alpha:  0.25 , Temperature:  13.0\n",
            "Alpha:  0.25 , Temperature:  14.0\n",
            "Alpha:  0.25 , Temperature:  15.0\n",
            "Alpha:  0.3 , Temperature:  1.0\n",
            "Alpha:  0.3 , Temperature:  2.0\n",
            "Alpha:  0.3 , Temperature:  3.0\n",
            "Alpha:  0.3 , Temperature:  4.0\n",
            "Alpha:  0.3 , Temperature:  5.0\n",
            "Alpha:  0.3 , Temperature:  6.0\n",
            "Alpha:  0.3 , Temperature:  7.0\n",
            "Alpha:  0.3 , Temperature:  8.0\n",
            "Alpha:  0.3 , Temperature:  9.0\n",
            "Alpha:  0.3 , Temperature:  10.0\n",
            "Alpha:  0.3 , Temperature:  11.0\n",
            "Alpha:  0.3 , Temperature:  12.0\n",
            "Alpha:  0.3 , Temperature:  13.0\n",
            "Alpha:  0.3 , Temperature:  14.0\n",
            "Alpha:  0.3 , Temperature:  15.0\n",
            "Alpha:  0.35 , Temperature:  1.0\n",
            "Alpha:  0.35 , Temperature:  2.0\n",
            "Alpha:  0.35 , Temperature:  3.0\n",
            "Alpha:  0.35 , Temperature:  4.0\n",
            "Alpha:  0.35 , Temperature:  5.0\n",
            "Alpha:  0.35 , Temperature:  6.0\n",
            "Alpha:  0.35 , Temperature:  7.0\n",
            "Alpha:  0.35 , Temperature:  8.0\n",
            "Alpha:  0.35 , Temperature:  9.0\n",
            "Alpha:  0.35 , Temperature:  10.0\n",
            "Alpha:  0.35 , Temperature:  11.0\n",
            "Alpha:  0.35 , Temperature:  12.0\n",
            "Alpha:  0.35 , Temperature:  13.0\n",
            "Alpha:  0.35 , Temperature:  14.0\n",
            "Alpha:  0.35 , Temperature:  15.0\n",
            "Alpha:  0.4 , Temperature:  1.0\n",
            "Alpha:  0.4 , Temperature:  2.0\n",
            "Alpha:  0.4 , Temperature:  3.0\n",
            "Alpha:  0.4 , Temperature:  4.0\n",
            "Alpha:  0.4 , Temperature:  5.0\n",
            "Alpha:  0.4 , Temperature:  6.0\n",
            "Alpha:  0.4 , Temperature:  7.0\n",
            "Alpha:  0.4 , Temperature:  8.0\n",
            "Alpha:  0.4 , Temperature:  9.0\n",
            "Alpha:  0.4 , Temperature:  10.0\n",
            "Alpha:  0.4 , Temperature:  11.0\n",
            "Alpha:  0.4 , Temperature:  12.0\n",
            "Alpha:  0.4 , Temperature:  13.0\n",
            "Alpha:  0.4 , Temperature:  14.0\n",
            "Alpha:  0.4 , Temperature:  15.0\n",
            "Alpha:  0.45 , Temperature:  1.0\n",
            "Alpha:  0.45 , Temperature:  2.0\n",
            "Alpha:  0.45 , Temperature:  3.0\n",
            "Alpha:  0.45 , Temperature:  4.0\n",
            "Alpha:  0.45 , Temperature:  5.0\n",
            "Alpha:  0.45 , Temperature:  6.0\n",
            "Alpha:  0.45 , Temperature:  7.0\n",
            "Alpha:  0.45 , Temperature:  8.0\n",
            "Alpha:  0.45 , Temperature:  9.0\n",
            "Alpha:  0.45 , Temperature:  10.0\n",
            "Alpha:  0.45 , Temperature:  11.0\n",
            "Alpha:  0.45 , Temperature:  12.0\n",
            "Alpha:  0.45 , Temperature:  13.0\n",
            "Alpha:  0.45 , Temperature:  14.0\n",
            "Alpha:  0.45 , Temperature:  15.0\n",
            "Alpha:  0.5 , Temperature:  1.0\n",
            "Alpha:  0.5 , Temperature:  2.0\n",
            "Alpha:  0.5 , Temperature:  3.0\n",
            "Alpha:  0.5 , Temperature:  4.0\n",
            "Alpha:  0.5 , Temperature:  5.0\n",
            "Alpha:  0.5 , Temperature:  6.0\n",
            "Alpha:  0.5 , Temperature:  7.0\n",
            "Alpha:  0.5 , Temperature:  8.0\n",
            "Alpha:  0.5 , Temperature:  9.0\n",
            "Alpha:  0.5 , Temperature:  10.0\n",
            "Alpha:  0.5 , Temperature:  11.0\n",
            "Alpha:  0.5 , Temperature:  12.0\n",
            "Alpha:  0.5 , Temperature:  13.0\n",
            "Alpha:  0.5 , Temperature:  14.0\n",
            "Alpha:  0.5 , Temperature:  15.0\n",
            "Alpha:  0.55 , Temperature:  1.0\n",
            "Alpha:  0.55 , Temperature:  2.0\n",
            "Alpha:  0.55 , Temperature:  3.0\n",
            "Alpha:  0.55 , Temperature:  4.0\n",
            "Alpha:  0.55 , Temperature:  5.0\n",
            "Alpha:  0.55 , Temperature:  6.0\n",
            "Alpha:  0.55 , Temperature:  7.0\n",
            "Alpha:  0.55 , Temperature:  8.0\n",
            "Alpha:  0.55 , Temperature:  9.0\n",
            "Alpha:  0.55 , Temperature:  10.0\n",
            "Alpha:  0.55 , Temperature:  11.0\n",
            "Alpha:  0.55 , Temperature:  12.0\n",
            "Alpha:  0.55 , Temperature:  13.0\n",
            "Alpha:  0.55 , Temperature:  14.0\n",
            "Alpha:  0.55 , Temperature:  15.0\n",
            "Alpha:  0.6 , Temperature:  1.0\n",
            "Alpha:  0.6 , Temperature:  2.0\n",
            "Alpha:  0.6 , Temperature:  3.0\n",
            "Alpha:  0.6 , Temperature:  4.0\n",
            "Alpha:  0.6 , Temperature:  5.0\n",
            "Alpha:  0.6 , Temperature:  6.0\n",
            "Alpha:  0.6 , Temperature:  7.0\n",
            "Alpha:  0.6 , Temperature:  8.0\n",
            "Alpha:  0.6 , Temperature:  9.0\n",
            "Alpha:  0.6 , Temperature:  10.0\n",
            "Alpha:  0.6 , Temperature:  11.0\n",
            "Alpha:  0.6 , Temperature:  12.0\n",
            "Alpha:  0.6 , Temperature:  13.0\n",
            "Alpha:  0.6 , Temperature:  14.0\n",
            "Alpha:  0.6 , Temperature:  15.0\n",
            "Alpha:  0.65 , Temperature:  1.0\n",
            "Alpha:  0.65 , Temperature:  2.0\n",
            "Alpha:  0.65 , Temperature:  3.0\n",
            "Alpha:  0.65 , Temperature:  4.0\n",
            "Alpha:  0.65 , Temperature:  5.0\n",
            "Alpha:  0.65 , Temperature:  6.0\n",
            "Alpha:  0.65 , Temperature:  7.0\n",
            "Alpha:  0.65 , Temperature:  8.0\n",
            "Alpha:  0.65 , Temperature:  9.0\n",
            "Alpha:  0.65 , Temperature:  10.0\n",
            "Alpha:  0.65 , Temperature:  11.0\n",
            "Alpha:  0.65 , Temperature:  12.0\n",
            "Alpha:  0.65 , Temperature:  13.0\n",
            "Alpha:  0.65 , Temperature:  14.0\n",
            "Alpha:  0.65 , Temperature:  15.0\n",
            "Alpha:  0.7 , Temperature:  1.0\n",
            "Alpha:  0.7 , Temperature:  2.0\n",
            "Alpha:  0.7 , Temperature:  3.0\n",
            "Alpha:  0.7 , Temperature:  4.0\n",
            "Alpha:  0.7 , Temperature:  5.0\n",
            "Alpha:  0.7 , Temperature:  6.0\n",
            "Alpha:  0.7 , Temperature:  7.0\n",
            "Alpha:  0.7 , Temperature:  8.0\n",
            "Alpha:  0.7 , Temperature:  9.0\n",
            "Alpha:  0.7 , Temperature:  10.0\n",
            "Alpha:  0.7 , Temperature:  11.0\n",
            "Alpha:  0.7 , Temperature:  12.0\n",
            "Alpha:  0.7 , Temperature:  13.0\n",
            "Alpha:  0.7 , Temperature:  14.0\n",
            "Alpha:  0.7 , Temperature:  15.0\n",
            "Alpha:  0.75 , Temperature:  1.0\n",
            "Alpha:  0.75 , Temperature:  2.0\n",
            "Alpha:  0.75 , Temperature:  3.0\n",
            "Alpha:  0.75 , Temperature:  4.0\n",
            "Alpha:  0.75 , Temperature:  5.0\n",
            "Alpha:  0.75 , Temperature:  6.0\n",
            "Alpha:  0.75 , Temperature:  7.0\n",
            "Alpha:  0.75 , Temperature:  8.0\n",
            "Alpha:  0.75 , Temperature:  9.0\n",
            "Alpha:  0.75 , Temperature:  10.0\n",
            "Alpha:  0.75 , Temperature:  11.0\n",
            "Alpha:  0.75 , Temperature:  12.0\n",
            "Alpha:  0.75 , Temperature:  13.0\n",
            "Alpha:  0.75 , Temperature:  14.0\n",
            "Alpha:  0.75 , Temperature:  15.0\n",
            "Alpha:  0.8 , Temperature:  1.0\n",
            "Alpha:  0.8 , Temperature:  2.0\n",
            "Alpha:  0.8 , Temperature:  3.0\n",
            "Alpha:  0.8 , Temperature:  4.0\n",
            "Alpha:  0.8 , Temperature:  5.0\n",
            "Alpha:  0.8 , Temperature:  6.0\n",
            "Alpha:  0.8 , Temperature:  7.0\n",
            "Alpha:  0.8 , Temperature:  8.0\n",
            "Alpha:  0.8 , Temperature:  9.0\n",
            "Alpha:  0.8 , Temperature:  10.0\n",
            "Alpha:  0.8 , Temperature:  11.0\n",
            "Alpha:  0.8 , Temperature:  12.0\n",
            "Alpha:  0.8 , Temperature:  13.0\n",
            "Alpha:  0.8 , Temperature:  14.0\n",
            "Alpha:  0.8 , Temperature:  15.0\n",
            "Alpha:  0.85 , Temperature:  1.0\n",
            "Alpha:  0.85 , Temperature:  2.0\n",
            "Alpha:  0.85 , Temperature:  3.0\n",
            "Alpha:  0.85 , Temperature:  4.0\n",
            "Alpha:  0.85 , Temperature:  5.0\n",
            "Alpha:  0.85 , Temperature:  6.0\n",
            "Alpha:  0.85 , Temperature:  7.0\n",
            "Alpha:  0.85 , Temperature:  8.0\n",
            "Alpha:  0.85 , Temperature:  9.0\n",
            "Alpha:  0.85 , Temperature:  10.0\n",
            "Alpha:  0.85 , Temperature:  11.0\n",
            "Alpha:  0.85 , Temperature:  12.0\n",
            "Alpha:  0.85 , Temperature:  13.0\n",
            "Alpha:  0.85 , Temperature:  14.0\n",
            "Alpha:  0.85 , Temperature:  15.0\n",
            "Alpha:  0.9 , Temperature:  1.0\n",
            "Alpha:  0.9 , Temperature:  2.0\n",
            "Alpha:  0.9 , Temperature:  3.0\n",
            "Alpha:  0.9 , Temperature:  4.0\n",
            "Alpha:  0.9 , Temperature:  5.0\n",
            "Alpha:  0.9 , Temperature:  6.0\n",
            "Alpha:  0.9 , Temperature:  7.0\n",
            "Alpha:  0.9 , Temperature:  8.0\n",
            "Alpha:  0.9 , Temperature:  9.0\n",
            "Alpha:  0.9 , Temperature:  10.0\n",
            "Alpha:  0.9 , Temperature:  11.0\n",
            "Alpha:  0.9 , Temperature:  12.0\n",
            "Alpha:  0.9 , Temperature:  13.0\n",
            "Alpha:  0.9 , Temperature:  14.0\n",
            "Alpha:  0.9 , Temperature:  15.0\n",
            "Alpha:  0.95 , Temperature:  1.0\n",
            "Alpha:  0.95 , Temperature:  2.0\n",
            "Alpha:  0.95 , Temperature:  3.0\n",
            "Alpha:  0.95 , Temperature:  4.0\n",
            "Alpha:  0.95 , Temperature:  5.0\n",
            "Alpha:  0.95 , Temperature:  6.0\n",
            "Alpha:  0.95 , Temperature:  7.0\n",
            "Alpha:  0.95 , Temperature:  8.0\n",
            "Alpha:  0.95 , Temperature:  9.0\n",
            "Alpha:  0.95 , Temperature:  10.0\n",
            "Alpha:  0.95 , Temperature:  11.0\n",
            "Alpha:  0.95 , Temperature:  12.0\n",
            "Alpha:  0.95 , Temperature:  13.0\n",
            "Alpha:  0.95 , Temperature:  14.0\n",
            "Alpha:  0.95 , Temperature:  15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "bQw21PwHm9Sv",
        "outputId": "ee5084e9-5859-48bd-ac3d-d315505a66f5"
      },
      "source": [
        "#put results into a data table\r\n",
        "df = pd.DataFrame(experiment_data)\r\n",
        "columns = ['alpha', 'temperature', 'loss', 'ACC', 'AUPR', 'AUROC']\r\n",
        "df.columns=columns\r\n",
        "df\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>alpha</th>\n",
              "      <th>temperature</th>\n",
              "      <th>loss</th>\n",
              "      <th>ACC</th>\n",
              "      <th>AUPR</th>\n",
              "      <th>AUROC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.05</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.425047</td>\n",
              "      <td>0.84450</td>\n",
              "      <td>0.926413</td>\n",
              "      <td>0.927706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.05</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.358539</td>\n",
              "      <td>0.87025</td>\n",
              "      <td>0.945901</td>\n",
              "      <td>0.944091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.05</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.358184</td>\n",
              "      <td>0.87225</td>\n",
              "      <td>0.946038</td>\n",
              "      <td>0.945535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.05</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.359338</td>\n",
              "      <td>0.86750</td>\n",
              "      <td>0.947673</td>\n",
              "      <td>0.948683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.05</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.428123</td>\n",
              "      <td>0.86050</td>\n",
              "      <td>0.937678</td>\n",
              "      <td>0.939057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>280</th>\n",
              "      <td>0.95</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.316801</td>\n",
              "      <td>0.87625</td>\n",
              "      <td>0.949570</td>\n",
              "      <td>0.949182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>281</th>\n",
              "      <td>0.95</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.335234</td>\n",
              "      <td>0.86525</td>\n",
              "      <td>0.947330</td>\n",
              "      <td>0.946513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>282</th>\n",
              "      <td>0.95</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.379830</td>\n",
              "      <td>0.86275</td>\n",
              "      <td>0.940430</td>\n",
              "      <td>0.939232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>283</th>\n",
              "      <td>0.95</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.336157</td>\n",
              "      <td>0.86500</td>\n",
              "      <td>0.946295</td>\n",
              "      <td>0.944978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284</th>\n",
              "      <td>0.95</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.370108</td>\n",
              "      <td>0.88550</td>\n",
              "      <td>0.954724</td>\n",
              "      <td>0.955143</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>285 rows  6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     alpha  temperature      loss      ACC      AUPR     AUROC\n",
              "0     0.05          1.0  0.425047  0.84450  0.926413  0.927706\n",
              "1     0.05          2.0  0.358539  0.87025  0.945901  0.944091\n",
              "2     0.05          3.0  0.358184  0.87225  0.946038  0.945535\n",
              "3     0.05          4.0  0.359338  0.86750  0.947673  0.948683\n",
              "4     0.05          5.0  0.428123  0.86050  0.937678  0.939057\n",
              "..     ...          ...       ...      ...       ...       ...\n",
              "280   0.95         11.0  0.316801  0.87625  0.949570  0.949182\n",
              "281   0.95         12.0  0.335234  0.86525  0.947330  0.946513\n",
              "282   0.95         13.0  0.379830  0.86275  0.940430  0.939232\n",
              "283   0.95         14.0  0.336157  0.86500  0.946295  0.944978\n",
              "284   0.95         15.0  0.370108  0.88550  0.954724  0.955143\n",
              "\n",
              "[285 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32LbJfGuuC_m"
      },
      "source": [
        "df.to_csv('performancemetrics-greaterrange')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "e5G23QDkHlqS",
        "outputId": "2c8dece1-7baf-4293-92db-bfd951eaea3b"
      },
      "source": [
        "#find best performing combination\r\n",
        "max_metric = experiment_data[0]\r\n",
        "for metrics in experiment_data:\r\n",
        "  if metrics[3]>max_metric[3]:\r\n",
        "    max_metric = metrics\r\n",
        "\r\n",
        "mf = pd.DataFrame(data={'Name':columns, 'Best Performance':max_metric})\r\n",
        "mf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Best Performance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>alpha</td>\n",
              "      <td>0.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>temperature</td>\n",
              "      <td>11.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>loss</td>\n",
              "      <td>0.279779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ACC</td>\n",
              "      <td>0.898000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AUPR</td>\n",
              "      <td>0.962194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>AUROC</td>\n",
              "      <td>0.962347</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Name  Best Performance\n",
              "0        alpha          0.300000\n",
              "1  temperature         11.000000\n",
              "2         loss          0.279779\n",
              "3          ACC          0.898000\n",
              "4         AUPR          0.962194\n",
              "5        AUROC          0.962347"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    }
  ]
}