{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of cnn_dist.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHecrF91fJ3h"
      },
      "source": [
        "import numpy as np, os\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras as tfk\r\n",
        "keras = tfk\r\n",
        "import datetime as dt\r\n",
        "import six\r\n",
        "import h5py\r\n",
        "import requests\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt \r\n",
        "import seaborn as sns\r\n",
        "sns.set()\r\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3psP7yOQIVD"
      },
      "source": [
        "# Data loading "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKPa5-b4L7hX"
      },
      "source": [
        "# download the data \r\n",
        "url = 'https://www.dropbox.com/s/ysrim2re8mh22z9/synthetic_code_dataset.h5?dl=0'\r\n",
        "save_name = 'data.h5'\r\n",
        "_=!wget {url} -O {save_name}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCvBWh9oNYhd"
      },
      "source": [
        "# load the data into x_train, y_train, .....\r\n",
        "f = h5py.File(save_name, 'r')\r\n",
        "suffixes = ['train', 'test', 'valid']\r\n",
        "for suffix in suffixes:\r\n",
        "    exec(\"x_%s=np.transpose(f.get(\\\"X_%s\\\")[:], (0, 2, 1))\"%(suffix, suffix))\r\n",
        "    exec(\"y_%s=f.get(\\\"Y_%s\\\")[:]\"%(suffix, suffix))\r\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISQFZ12vQKeu"
      },
      "source": [
        "# Model definition function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5qiwx1sfthV"
      },
      "source": [
        "def get_activation(activation = 'relu'):\r\n",
        "    \"\"\"\r\n",
        "    Create an activation function. The activation argument should one of:\r\n",
        "    1. A string representing the keras name of the activation. \r\n",
        "    2. A callable which may or may not be an instance of keras.layers.Layer. \r\n",
        "    \"\"\"\r\n",
        "    if isinstance(activation, str):\r\n",
        "        actfn = tfk.layers.Activation(activation)\r\n",
        "    else:\r\n",
        "        if callable(activation) and not isinstance(activation, tfk.layers.Layer):\r\n",
        "            actfn = tfk.layers.Activation(activation)\r\n",
        "        else:\r\n",
        "            actfn = activation\r\n",
        "    return actfn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baDorqePRC-5"
      },
      "source": [
        "def conv_layer(x, num_filters, kernel_size, padding, activation, dropout=0.5, l2=1e-6, bn=True): \r\n",
        "    \"\"\"\r\n",
        "    A convolutional block comprising of a convolutional layer followed by\r\n",
        "    batch normalization, an activation function, and dropout. \r\n",
        "    \"\"\"\r\n",
        "    y = tfk.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, kernel_regularizer=tfk.regularizers.l2(l2), padding=padding)(x)\r\n",
        "    if bn:\r\n",
        "        y = tfk.layers.BatchNormalization()(y)\r\n",
        "    actfn = get_activation(activation)\r\n",
        "    y = actfn(y)\r\n",
        "    if dropout:\r\n",
        "        y = tfk.layers.Dropout(dropout)(y)\r\n",
        "    return y\r\n",
        "\r\n",
        "def dense_layer(x, num_units, activation, dropout=0.5, l2=None, bn=True):\r\n",
        "    \"\"\"\r\n",
        "    A dense block comprising of a dense layer followed by batch normalization, \r\n",
        "    activation and dropout. \r\n",
        "    \"\"\"\r\n",
        "    y = tfk.layers.Dense(num_units, use_bias=False, kernel_regularizer=tfk.regularizers.l2(l2))(x)\r\n",
        "    if bn:\r\n",
        "        y = tfk.layers.BatchNormalization()(y)\r\n",
        "    actfn = get_activation(activation)\r\n",
        "    y = actfn(y)\r\n",
        "    if dropout:\r\n",
        "        y = tfk.layers.Dropout(dropout)(y)\r\n",
        "    return y\r\n",
        "\r\n",
        "# def get_model(L, A, name=\"cnn_att\"):\r\n",
        "# \t## input layer\r\n",
        "# \tx = tfk.layers.Input((L, A), name='Input')\r\n",
        "\t\r\n",
        "# \t## 1st conv layer\r\n",
        "# \ty = keras.layers.Conv1D(filters=32, kernel_size=19, kernel_regularizer=tfk.regularizers.l2(1e-6), padding='same', name='conv1', use_bias=True)(x)\r\n",
        "# \ty = keras.layers.Activation('relu')(y)\r\n",
        "# \ty = keras.layers.MaxPool1D(pool_size=4)(y)\r\n",
        "\t\r\n",
        "# \t# multi head attention layer\r\n",
        "# \tembedding = keras.layers.Dropout(0.1)(y)\r\n",
        "# \ty, weights = keras.layers.MultiHeadAttention(num_heads=8, key_dim=64, value_dim=64)(embedding, embedding, return_attention_scores=True)\r\n",
        "# \ty = keras.layers.Dropout(0.1)(y)\r\n",
        "# \ty = keras.layers.LayerNormalization(epsilon=1e-6)(y)\r\n",
        "\t\r\n",
        "# \t# everything else\r\n",
        "# \ty = keras.layers.Flatten()(y)\r\n",
        "# \ty = keras.layers.Dense(128, activation=None, use_bias=False)(y)\r\n",
        "# \ty = keras.layers.BatchNormalization()(y)\r\n",
        "# \ty = keras.layers.Activation('relu')(y)\r\n",
        "# \ty = keras.layers.Dropout(0.5)(y)\r\n",
        "# \ty = keras.layers.Dense(1, name='logits')(y)\r\n",
        "# \ty = keras.layers.Activation('sigmoid', name='output')(y)\r\n",
        "# \tmodel = tfk.Model(inputs=x, outputs=y, name=name)\r\n",
        "# \treturn model\r\n",
        "\r\n",
        "def get_model(L, A, activation='relu', name='cnn_dist'):\r\n",
        "    \"\"\"\r\n",
        "    A function to assemble the full CNN distributed model. \r\n",
        "    \"\"\"\r\n",
        "    # input layer \r\n",
        "    x = tfk.layers.Input((L, A), name='input')\r\n",
        "\r\n",
        "    # 1st convolutional block \r\n",
        "    y = conv_layer(x,num_filters=24, kernel_size=19, padding='same', dropout=0.1,l2=1e-6, bn=True, activation=activation)\r\n",
        "    \r\n",
        "    # 2nd conv. block + pooling \r\n",
        "    y = conv_layer(y,num_filters=32, kernel_size=7, padding='same', activation=activation, dropout=0.2,l2=1e-6, bn=True)\r\n",
        "    y = tfk.layers.MaxPool1D(pool_size=4)(y)\r\n",
        "    \r\n",
        "    # 3rd convolutional block + pooling \r\n",
        "    y = conv_layer(y,num_filters=64, kernel_size=3, padding='same', activation=activation, dropout=0.4,l2=1e-6, bn=True)\r\n",
        "    y = tfk.layers.MaxPool1D(pool_size=3, strides=3, padding='same')(y)\r\n",
        "    \r\n",
        "    # dense block and final output layer \r\n",
        "    y = tfk.layers.Flatten()(y)\r\n",
        "    y = dense_layer(y, num_units=96, activation=activation, dropout=0.5, l2=1e-6, bn=True)\r\n",
        "    y = tfk.layers.Dense(1, use_bias=True, name = 'logits')(y)\r\n",
        "    y = tfk.layers.Activation('sigmoid')(y)\r\n",
        "\r\n",
        "    # assemble full model\r\n",
        "    model = tfk.Model(x, y, name=name)\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQzKg97ayWQ-"
      },
      "source": [
        "# Train a teacher model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUkdVPMdifWD",
        "outputId": "03ed6b00-5a04-415c-8f1c-cee5e862101f"
      },
      "source": [
        "# instantiate the teacher model \r\n",
        "activation = 'relu' \r\n",
        "#activation = lambda x : tf.math.sin(x) + tf.math.cos(x)\r\n",
        "L, A = x_train.shape[1:]\r\n",
        "\r\n",
        "teacher_model = get_model(L, A, name='teacher')\r\n",
        "#teacher_model = get_model(L, A, activation, name='teacher')\r\n",
        "\r\n",
        "# compile the teacher model \r\n",
        "lossfn = tfk.losses.BinaryCrossentropy(name='bce')\r\n",
        "modelmetrics = [tfk.metrics.BinaryAccuracy(name='ACC'), tfk.metrics.AUC(curve='PR', name='AUPR'), tfk.metrics.AUC(curve='ROC', name='AUROC')]\r\n",
        "optimizer = tfk.optimizers.Adam(learning_rate=1e-2)\r\n",
        "teacher_model.compile(loss=lossfn, metrics=modelmetrics, optimizer=optimizer)\r\n",
        "\r\n",
        "# fit the teacher model \r\n",
        "num_epochs = 100\r\n",
        "callbacks = [tfk.callbacks.EarlyStopping(monitor='val_AUROC', patience=20), \r\n",
        "             tfk.callbacks.ModelCheckpoint(\"best_teacher_model.hdf5\", monitor='val_AUROC', mode='max', save_best_only=True)]\r\n",
        "teacher_model.fit(x_train, y_train, \r\n",
        "                    epochs=num_epochs, \r\n",
        "                    batch_size=128, \r\n",
        "                    callbacks=callbacks, \r\n",
        "                    shuffle=True, \r\n",
        "                    initial_epoch=0,\r\n",
        "                    validation_data=(x_valid, y_valid))\r\n",
        "teacher_model = tfk.models.load_model('best_teacher_model.hdf5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "110/110 [==============================] - 10s 20ms/step - loss: 0.6524 - ACC: 0.6793 - AUPR: 0.7242 - AUROC: 0.7358 - val_loss: 0.4425 - val_ACC: 0.8080 - val_AUPR: 0.8936 - val_AUROC: 0.8965\n",
            "Epoch 2/100\n",
            "110/110 [==============================] - 1s 13ms/step - loss: 0.4107 - ACC: 0.8131 - AUPR: 0.8857 - AUROC: 0.8945 - val_loss: 1.4897 - val_ACC: 0.5735 - val_AUPR: 0.8289 - val_AUROC: 0.8925\n",
            "Epoch 3/100\n",
            "110/110 [==============================] - 1s 13ms/step - loss: 0.3234 - ACC: 0.8590 - AUPR: 0.9339 - AUROC: 0.9362 - val_loss: 0.3907 - val_ACC: 0.8345 - val_AUPR: 0.9366 - val_AUROC: 0.9468\n",
            "Epoch 4/100\n",
            "110/110 [==============================] - 1s 13ms/step - loss: 0.2711 - ACC: 0.8847 - AUPR: 0.9529 - AUROC: 0.9552 - val_loss: 0.3028 - val_ACC: 0.8940 - val_AUPR: 0.9534 - val_AUROC: 0.9603\n",
            "Epoch 5/100\n",
            "110/110 [==============================] - 1s 13ms/step - loss: 0.2390 - ACC: 0.9036 - AUPR: 0.9625 - AUROC: 0.9650 - val_loss: 0.7680 - val_ACC: 0.7475 - val_AUPR: 0.9516 - val_AUROC: 0.9450\n",
            "Epoch 6/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.2026 - ACC: 0.9208 - AUPR: 0.9721 - AUROC: 0.9746 - val_loss: 0.2237 - val_ACC: 0.9155 - val_AUPR: 0.9650 - val_AUROC: 0.9706\n",
            "Epoch 7/100\n",
            "110/110 [==============================] - 1s 13ms/step - loss: 0.1973 - ACC: 0.9208 - AUPR: 0.9736 - AUROC: 0.9760 - val_loss: 0.2339 - val_ACC: 0.9125 - val_AUPR: 0.9675 - val_AUROC: 0.9734\n",
            "Epoch 8/100\n",
            "110/110 [==============================] - 1s 13ms/step - loss: 0.1867 - ACC: 0.9261 - AUPR: 0.9769 - AUROC: 0.9783 - val_loss: 0.2809 - val_ACC: 0.9015 - val_AUPR: 0.9566 - val_AUROC: 0.9695\n",
            "Epoch 9/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.1816 - ACC: 0.9292 - AUPR: 0.9774 - AUROC: 0.9795 - val_loss: 0.2205 - val_ACC: 0.9200 - val_AUPR: 0.9683 - val_AUROC: 0.9717\n",
            "Epoch 10/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.1726 - ACC: 0.9328 - AUPR: 0.9792 - AUROC: 0.9810 - val_loss: 0.2078 - val_ACC: 0.9200 - val_AUPR: 0.9693 - val_AUROC: 0.9735\n",
            "Epoch 11/100\n",
            "110/110 [==============================] - 1s 13ms/step - loss: 0.1675 - ACC: 0.9359 - AUPR: 0.9815 - AUROC: 0.9826 - val_loss: 0.2826 - val_ACC: 0.8980 - val_AUPR: 0.9642 - val_AUROC: 0.9743\n",
            "Epoch 12/100\n",
            "110/110 [==============================] - 1s 13ms/step - loss: 0.1520 - ACC: 0.9421 - AUPR: 0.9837 - AUROC: 0.9853 - val_loss: 0.4904 - val_ACC: 0.8475 - val_AUPR: 0.9680 - val_AUROC: 0.9640\n",
            "Epoch 13/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.1503 - ACC: 0.9398 - AUPR: 0.9854 - AUROC: 0.9861 - val_loss: 0.1868 - val_ACC: 0.9235 - val_AUPR: 0.9739 - val_AUROC: 0.9779\n",
            "Epoch 14/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.1521 - ACC: 0.9400 - AUPR: 0.9851 - AUROC: 0.9856 - val_loss: 0.4081 - val_ACC: 0.8515 - val_AUPR: 0.9556 - val_AUROC: 0.9706\n",
            "Epoch 15/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.1514 - ACC: 0.9436 - AUPR: 0.9830 - AUROC: 0.9853 - val_loss: 0.2133 - val_ACC: 0.9205 - val_AUPR: 0.9664 - val_AUROC: 0.9750\n",
            "Epoch 16/100\n",
            "110/110 [==============================] - 1s 13ms/step - loss: 0.1469 - ACC: 0.9400 - AUPR: 0.9853 - AUROC: 0.9866 - val_loss: 0.2858 - val_ACC: 0.9010 - val_AUPR: 0.9600 - val_AUROC: 0.9717\n",
            "Epoch 17/100\n",
            "110/110 [==============================] - 1s 13ms/step - loss: 0.1475 - ACC: 0.9440 - AUPR: 0.9859 - AUROC: 0.9866 - val_loss: 0.3030 - val_ACC: 0.8670 - val_AUPR: 0.9728 - val_AUROC: 0.9747\n",
            "Epoch 18/100\n",
            "110/110 [==============================] - 1s 12ms/step - loss: 0.1429 - ACC: 0.9445 - AUPR: 0.9862 - AUROC: 0.9870 - val_loss: 0.2002 - val_ACC: 0.9260 - val_AUPR: 0.9759 - val_AUROC: 0.9775\n",
            "Epoch 19/100\n",
            "110/110 [==============================] - 1s 13ms/step - loss: 0.1418 - ACC: 0.9424 - AUPR: 0.9863 - AUROC: 0.9872 - val_loss: 0.2360 - val_ACC: 0.9140 - val_AUPR: 0.9740 - val_AUROC: 0.9749\n",
            "Epoch 20/100\n",
            "110/110 [==============================] - 1s 13ms/step - loss: 0.1249 - ACC: 0.9541 - AUPR: 0.9898 - AUROC: 0.9900 - val_loss: 0.3528 - val_ACC: 0.8815 - val_AUPR: 0.9722 - val_AUROC: 0.9701\n",
            "Epoch 21/100\n",
            "110/110 [==============================] - 1s 13ms/step - loss: 0.1311 - ACC: 0.9518 - AUPR: 0.9895 - AUROC: 0.9893 - val_loss: 0.2704 - val_ACC: 0.9030 - val_AUPR: 0.9676 - val_AUROC: 0.9762\n",
            "Epoch 22/100\n",
            "110/110 [==============================] - 1s 13ms/step - loss: 0.1259 - ACC: 0.9521 - AUPR: 0.9899 - AUROC: 0.9901 - val_loss: 0.2197 - val_ACC: 0.9185 - val_AUPR: 0.9707 - val_AUROC: 0.9753\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kpvZ10Vvxcr"
      },
      "source": [
        "teacher_model = tfk.models.load_model('best_teacher_model.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lep5Pun5E0k-"
      },
      "source": [
        "# Knowledge distillation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTehiZ5nGzou"
      },
      "source": [
        "## Define a `Distiller` class that takes in a trained teacher model, an untrained student model and distills the knowledge in the teacher model onto the student model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tai-epSfGkUz"
      },
      "source": [
        "class Distiller(keras.Model):\r\n",
        "    def get_config(self,):\r\n",
        "        \"\"\"\r\n",
        "        Implement the config dictionary to enable serialization\r\n",
        "        \"\"\"\r\n",
        "        config = {}\r\n",
        "        config['student'] = self.student\r\n",
        "        config['teacher'] = self.teacher\r\n",
        "        return config\r\n",
        "    \r\n",
        "    def __init__(self, student, teacher):\r\n",
        "        super(Distiller, self).__init__()\r\n",
        "        self.teacher = teacher\r\n",
        "        self.student = student\r\n",
        "\r\n",
        "    def compile(\r\n",
        "        self,\r\n",
        "        optimizer,\r\n",
        "        metrics,\r\n",
        "        student_loss_fn,\r\n",
        "        distillation_loss_fn,\r\n",
        "        alpha=0.1,\r\n",
        "        temperature=3,\r\n",
        "    ):\r\n",
        "        \"\"\" Configure the distiller.\r\n",
        "\r\n",
        "        Args:\r\n",
        "            optimizer: Keras optimizer for the student weights\r\n",
        "            metrics: Keras metrics for evaluation\r\n",
        "            student_loss_fn: Loss function of difference between student\r\n",
        "                predictions and ground-truth\r\n",
        "            distillation_loss_fn: Loss function of difference between soft\r\n",
        "                student predictions and soft teacher predictions\r\n",
        "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\r\n",
        "            temperature: Temperature for softening probability distributions.\r\n",
        "                Larger temperature gives softer distributions.\r\n",
        "        \"\"\"\r\n",
        "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\r\n",
        "        self.student_loss_fn = student_loss_fn\r\n",
        "        self.distillation_loss_fn = distillation_loss_fn\r\n",
        "        self.alpha = alpha\r\n",
        "        self.temperature = temperature\r\n",
        "\r\n",
        "    def train_step(self, data):\r\n",
        "        # Unpack data\r\n",
        "        x, y = data\r\n",
        "\r\n",
        "        # Forward pass of teacher\r\n",
        "        teacher_predictions = self.teacher(x, training=False)\r\n",
        "\r\n",
        "        with tf.GradientTape() as tape:\r\n",
        "            # Forward pass of student\r\n",
        "            student_predictions = self.student(x, training=True)\r\n",
        "\r\n",
        "            # Compute losses\r\n",
        "            student_loss = self.student_loss_fn(y, student_predictions)\r\n",
        "            distillation_loss = self.distillation_loss_fn(\r\n",
        "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\r\n",
        "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\r\n",
        "            )\r\n",
        "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\r\n",
        "\r\n",
        "        # Compute gradients\r\n",
        "        trainable_vars = self.student.trainable_variables\r\n",
        "        gradients = tape.gradient(loss, trainable_vars)\r\n",
        "\r\n",
        "        # Update weights\r\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\r\n",
        "\r\n",
        "        # Update the metrics configured in `compile()`.\r\n",
        "        self.compiled_metrics.update_state(y, student_predictions)\r\n",
        "\r\n",
        "        # Return a dict of performance\r\n",
        "        results = {m.name: m.result() for m in self.metrics}\r\n",
        "        results.update(\r\n",
        "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\r\n",
        "        )\r\n",
        "        return results\r\n",
        "\r\n",
        "    def test_step(self, data):\r\n",
        "        # Unpack the data\r\n",
        "        x, y = data\r\n",
        "\r\n",
        "        # Compute predictions\r\n",
        "        y_prediction = self.student(x, training=False)\r\n",
        "\r\n",
        "        # Calculate the loss\r\n",
        "        student_loss = self.student_loss_fn(y, y_prediction)\r\n",
        "\r\n",
        "        # Update the metrics.\r\n",
        "        self.compiled_metrics.update_state(y, y_prediction)\r\n",
        "\r\n",
        "        # Return a dict of performance\r\n",
        "        results = {\"student_loss\": student_loss}\r\n",
        "        results.update({m.name: m.result() for m in self.metrics})\r\n",
        "        return results\r\n",
        "    \r\n",
        "    @property\r\n",
        "    def metrics_names(self):\r\n",
        "        return ['student_loss']+[m.name for m in self.metrics]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvOT3OZDHCgu"
      },
      "source": [
        "def get_student_model(L, A, activation='relu', name='deepbind'):\r\n",
        "    \"\"\"\r\n",
        "    Defining the deepbind architecture in here. \r\n",
        "    \"\"\"\r\n",
        "    x = tfk.layers.Input((L, A), name='input')\r\n",
        "    y = tfk.layers.Conv1D(filters=16, kernel_size=24, padding='valid', kernel_regularizer=tfk.regularizers.l2(1e-6))(x)\r\n",
        "    actfn = get_activation(activation=activation)\r\n",
        "    y = actfn(y)\r\n",
        "    y = tfk.layers.Lambda(lambda x : tf.reduce_max(x, axis=1))(y)  # max pooling\r\n",
        "    y = tfk.layers.Dropout(0.5)(y)  \r\n",
        "    y = tfk.layers.Dense(32, activation='relu')(y)\r\n",
        "    y = tfk.layers.Dense(1, name='logits')(y)\r\n",
        "    y = tfk.layers.Activation('sigmoid', name='output')(y)\r\n",
        "\r\n",
        "    model = tfk.Model(inputs=x, outputs=y, name=name)\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K79gsB5RIhPV",
        "outputId": "ff63121b-55c3-4c58-ef7e-c953efbe74e9"
      },
      "source": [
        "# instantiate the student model and the distiller \r\n",
        "student_model = get_student_model(L, A)\r\n",
        "distiller = Distiller(student_model, teacher_model)\r\n",
        "\r\n",
        "# compile the distiller\r\n",
        "alpha = 0.8\r\n",
        "temperature = 1. \r\n",
        "distiller.compile(\r\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\r\n",
        "    metrics=modelmetrics,\r\n",
        "    student_loss_fn=keras.losses.BinaryCrossentropy(name='bce'),\r\n",
        "    distillation_loss_fn=keras.losses.KLDivergence(),\r\n",
        "    alpha=alpha,\r\n",
        "    temperature=temperature,\r\n",
        ")\r\n",
        "\r\n",
        "# perform distillation\r\n",
        "num_epochs = 50\r\n",
        "callbacks = [tfk.callbacks.EarlyStopping(monitor='val_AUROC', patience=20), \r\n",
        "             tfk.callbacks.ModelCheckpoint(\"best_distiller.hdf5\", monitor='val_AUROC', mode='max',save_weights_only=True, save_best_only=True)]\r\n",
        "distiller.fit(x_train, y_train, \r\n",
        "                epochs=num_epochs, \r\n",
        "                batch_size=128, \r\n",
        "                callbacks=callbacks, \r\n",
        "                shuffle=True, \r\n",
        "                validation_data=(x_valid, y_valid))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "110/110 [==============================] - 2s 12ms/step - ACC: 0.6240 - AUPR: 0.7223 - AUROC: 0.7035 - student_loss: 0.7011 - distillation_loss: 0.0000e+00 - val_student_loss: 0.6872 - val_ACC: 0.6435 - val_AUPR: 0.6604 - val_AUROC: 0.6949\n",
            "Epoch 2/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.5596 - AUPR: 0.5755 - AUROC: 0.5880 - student_loss: 0.6757 - distillation_loss: 0.0000e+00 - val_student_loss: 0.6677 - val_ACC: 0.7075 - val_AUPR: 0.7689 - val_AUROC: 0.7906\n",
            "Epoch 3/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.6347 - AUPR: 0.6745 - AUROC: 0.6909 - student_loss: 0.6372 - distillation_loss: 0.0000e+00 - val_student_loss: 0.6180 - val_ACC: 0.7380 - val_AUPR: 0.8093 - val_AUROC: 0.8192\n",
            "Epoch 4/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.6852 - AUPR: 0.7342 - AUROC: 0.7511 - student_loss: 0.5885 - distillation_loss: 0.0000e+00 - val_student_loss: 0.5754 - val_ACC: 0.7580 - val_AUPR: 0.8378 - val_AUROC: 0.8426\n",
            "Epoch 5/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.7231 - AUPR: 0.7897 - AUROC: 0.7961 - student_loss: 0.5476 - distillation_loss: 0.0000e+00 - val_student_loss: 0.5519 - val_ACC: 0.7910 - val_AUPR: 0.8680 - val_AUROC: 0.8723\n",
            "Epoch 6/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.7425 - AUPR: 0.8156 - AUROC: 0.8209 - student_loss: 0.5127 - distillation_loss: 0.0000e+00 - val_student_loss: 0.5135 - val_ACC: 0.8190 - val_AUPR: 0.8951 - val_AUROC: 0.9015\n",
            "Epoch 7/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.7604 - AUPR: 0.8489 - AUROC: 0.8452 - student_loss: 0.4939 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4695 - val_ACC: 0.8385 - val_AUPR: 0.9094 - val_AUROC: 0.9129\n",
            "Epoch 8/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.7751 - AUPR: 0.8573 - AUROC: 0.8555 - student_loss: 0.4729 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4413 - val_ACC: 0.8530 - val_AUPR: 0.9229 - val_AUROC: 0.9255\n",
            "Epoch 9/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.7892 - AUPR: 0.8642 - AUROC: 0.8646 - student_loss: 0.4636 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4363 - val_ACC: 0.8605 - val_AUPR: 0.9270 - val_AUROC: 0.9289\n",
            "Epoch 10/50\n",
            "110/110 [==============================] - 1s 10ms/step - ACC: 0.7998 - AUPR: 0.8778 - AUROC: 0.8762 - student_loss: 0.4522 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4236 - val_ACC: 0.8650 - val_AUPR: 0.9311 - val_AUROC: 0.9317\n",
            "Epoch 11/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.7959 - AUPR: 0.8765 - AUROC: 0.8720 - student_loss: 0.4478 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4197 - val_ACC: 0.8650 - val_AUPR: 0.9358 - val_AUROC: 0.9354\n",
            "Epoch 12/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.7957 - AUPR: 0.8808 - AUROC: 0.8767 - student_loss: 0.4425 - distillation_loss: 0.0000e+00 - val_student_loss: 0.3950 - val_ACC: 0.8735 - val_AUPR: 0.9390 - val_AUROC: 0.9388\n",
            "Epoch 13/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.8004 - AUPR: 0.8841 - AUROC: 0.8824 - student_loss: 0.4349 - distillation_loss: 0.0000e+00 - val_student_loss: 0.4023 - val_ACC: 0.8665 - val_AUPR: 0.9407 - val_AUROC: 0.9408\n",
            "Epoch 14/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.8071 - AUPR: 0.8892 - AUROC: 0.8875 - student_loss: 0.4290 - distillation_loss: 0.0000e+00 - val_student_loss: 0.3905 - val_ACC: 0.8730 - val_AUPR: 0.9400 - val_AUROC: 0.9416\n",
            "Epoch 15/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.8064 - AUPR: 0.8897 - AUROC: 0.8850 - student_loss: 0.4206 - distillation_loss: 0.0000e+00 - val_student_loss: 0.3865 - val_ACC: 0.8715 - val_AUPR: 0.9409 - val_AUROC: 0.9419\n",
            "Epoch 16/50\n",
            "110/110 [==============================] - 1s 8ms/step - ACC: 0.8070 - AUPR: 0.8922 - AUROC: 0.8891 - student_loss: 0.4269 - distillation_loss: 0.0000e+00 - val_student_loss: 0.3855 - val_ACC: 0.8650 - val_AUPR: 0.9387 - val_AUROC: 0.9401\n",
            "Epoch 17/50\n",
            "110/110 [==============================] - 1s 9ms/step - ACC: 0.8136 - AUPR: 0.8936 - AUROC: 0.8918 - student_loss: 0.4187 - distillation_loss: 0.0000e+00 - val_student_loss: 0.3800 - val_ACC: 0.8670 - val_AUPR: 0.9444 - val_AUROC: 0.9468\n",
            "Epoch 18/50\n",
            "110/110 [==============================] - 1s 9ms/step - ACC: 0.8137 - AUPR: 0.8948 - AUROC: 0.8923 - student_loss: 0.4147 - distillation_loss: 0.0000e+00 - val_student_loss: 0.3742 - val_ACC: 0.8695 - val_AUPR: 0.9438 - val_AUROC: 0.9464\n",
            "Epoch 19/50\n",
            "110/110 [==============================] - 1s 9ms/step - ACC: 0.8157 - AUPR: 0.8993 - AUROC: 0.8947 - student_loss: 0.4144 - distillation_loss: 0.0000e+00 - val_student_loss: 0.3729 - val_ACC: 0.8745 - val_AUPR: 0.9456 - val_AUROC: 0.9478\n",
            "Epoch 20/50\n",
            "110/110 [==============================] - 1s 9ms/step - ACC: 0.8204 - AUPR: 0.8978 - AUROC: 0.8960 - student_loss: 0.4051 - distillation_loss: 0.0000e+00 - val_student_loss: 0.3649 - val_ACC: 0.8790 - val_AUPR: 0.9457 - val_AUROC: 0.9476\n",
            "Epoch 21/50\n",
            "110/110 [==============================] - 1s 9ms/step - ACC: 0.8202 - AUPR: 0.8933 - AUROC: 0.8915 - student_loss: 0.4072 - distillation_loss: 0.0000e+00 - val_student_loss: 0.3575 - val_ACC: 0.8775 - val_AUPR: 0.9461 - val_AUROC: 0.9480\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdc7ed332b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eg2dHq2vKK70"
      },
      "source": [
        "def plot_f_and_grad(model):\r\n",
        "    # pick a random sample \r\n",
        "    N, L, A = x_train.shape\r\n",
        "    xsample = x_train[np.random.randint(0, N)][None, :, :]\r\n",
        "\r\n",
        "    # define a keras model mapping an input sequence to the logits of the teacher model\r\n",
        "    func = tfk.Model(inputs=model.input, outputs=model.get_layer('logits').output)\r\n",
        "\r\n",
        "    # define a set of probe sequences by sampling points in the ith nucleotide, jth channel \r\n",
        "    # i and j are picked randomly\r\n",
        "    n_probe = 100\r\n",
        "    x_probe = np.linspace(0, 1, n_probe)\r\n",
        "    n_samples = 50\r\n",
        "    Is, Js, y_ijs, y_ij_grads = [], [], [], []\r\n",
        "    for i in range(n_samples):  \r\n",
        "        i, j = np.random.randint(0, L), np.random.randint(0, A)\r\n",
        "        Is.append(i)\r\n",
        "        Js.append(j)\r\n",
        "        \r\n",
        "        x_ij_probe = np.zeros((n_probe, L, A))\r\n",
        "        x_ij_probe[:, i, j] = x_probe\r\n",
        "        x_ij_probe = tf.convert_to_tensor(x_ij_probe)\r\n",
        "\r\n",
        "        with tf.GradientTape() as tape:\r\n",
        "            tape.watch(x_ij_probe)\r\n",
        "            y_ij_pred = func(x_ij_probe)\r\n",
        "        y_ij_grad = tape.gradient(y_ij_pred, x_ij_probe)\r\n",
        "        \r\n",
        "        #y_ij_pred = func(x_ij_probe)\r\n",
        "        y_ijs.append(y_ij_pred.numpy())\r\n",
        "        y_ij_grads.append(y_ij_grad.numpy()[:, i, j])\r\n",
        "\r\n",
        "    # plot\r\n",
        "    fig = plt.figure(figsize=(14, 10))\r\n",
        "    for k in range(4):\r\n",
        "        idx = np.random.randint(0, len(Is))\r\n",
        "        i = Is[idx]\r\n",
        "        j = Js[idx]\r\n",
        "        ax = fig.add_subplot(2,2,k+1)\r\n",
        "        ax1 = ax.twinx()\r\n",
        "        title=\"i=%d, j=%d\"%(i, j)\r\n",
        "        figure_options = {'linewidth':2}\r\n",
        "\r\n",
        "        c, c1 = 'blue', 'red'\r\n",
        "        ax.plot(x_probe, y_ijs[idx], color=c, label='$f(x)$',**figure_options)\r\n",
        "        ax.tick_params(axis='y', color=c, labelcolor=c)\r\n",
        "        ax.legend(loc='upper right', fontsize=15)\r\n",
        "        \r\n",
        "        ax1.plot(x_probe, y_ij_grads[idx], color=c1, label=\"$\\\\nabla f_{ij}$\", **figure_options)\r\n",
        "        ax1.tick_params(axis='y',color=c1, labelcolor=c1)\r\n",
        "        ax1.legend(loc='lower left', fontsize=15)\r\n",
        "\r\n",
        "        ax.set_title(title, fontsize=15)\r\n",
        "    fig.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYBEWe-SqBDo"
      },
      "source": [
        "#plot_f_and_grad(distiller.teacher)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfcn7FDxKmlD"
      },
      "source": [
        "#plot_f_and_grad(distiller.student)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43FEwHoLwI1I"
      },
      "source": [
        "## Train a simple student model from scratch without distillation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0HgMTuQoSNj",
        "outputId": "0d405e02-a8ec-4b17-eb6f-5a1ebb1e62c7"
      },
      "source": [
        "# train a deep bind model by itself \r\n",
        "deepbind_model = get_student_model(L, A)\r\n",
        "\r\n",
        "# compile the model \r\n",
        "lossfn = tfk.losses.BinaryCrossentropy(name='bce')\r\n",
        "modelmetrics = [tfk.metrics.BinaryAccuracy(name='ACC'), tfk.metrics.AUC(curve='PR', name='AUPR'), tfk.metrics.AUC(curve='ROC', name='AUROC')]\r\n",
        "optimizer = tfk.optimizers.Adam(learning_rate=1e-2)\r\n",
        "deepbind_model.compile(loss=lossfn, metrics=modelmetrics, optimizer=optimizer)\r\n",
        "\r\n",
        "# fit the teacher model \r\n",
        "num_epochs = 100\r\n",
        "callbacks = [tfk.callbacks.EarlyStopping(monitor='val_AUROC', patience=20), \r\n",
        "             tfk.callbacks.ModelCheckpoint(\"best_deepbind_model.hdf5\", monitor='val_AUROC', mode='max', save_best_only=True)]\r\n",
        "deepbind_model.fit(x_train, y_train, \r\n",
        "                    epochs=num_epochs, \r\n",
        "                    batch_size=128, \r\n",
        "                    callbacks=callbacks, \r\n",
        "                    shuffle=True, \r\n",
        "                    initial_epoch=0,\r\n",
        "                    validation_data=(x_valid, y_valid))\r\n",
        "deepbind_model = tfk.models.load_model('best_deepbind_model.hdf5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "110/110 [==============================] - 2s 9ms/step - loss: 0.6998 - ACC: 0.5252 - AUPR: 0.5312 - AUROC: 0.5375 - val_loss: 0.6176 - val_ACC: 0.6650 - val_AUPR: 0.7906 - val_AUROC: 0.7933\n",
            "Epoch 2/100\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.6034 - ACC: 0.6718 - AUPR: 0.7385 - AUROC: 0.7338 - val_loss: 0.4843 - val_ACC: 0.7960 - val_AUPR: 0.8737 - val_AUROC: 0.8804\n",
            "Epoch 3/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.5275 - ACC: 0.7407 - AUPR: 0.8216 - AUROC: 0.8091 - val_loss: 0.4502 - val_ACC: 0.8490 - val_AUPR: 0.9167 - val_AUROC: 0.9237\n",
            "Epoch 4/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.5046 - ACC: 0.7567 - AUPR: 0.8397 - AUROC: 0.8305 - val_loss: 0.3941 - val_ACC: 0.8545 - val_AUPR: 0.9265 - val_AUROC: 0.9329\n",
            "Epoch 5/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4932 - ACC: 0.7611 - AUPR: 0.8481 - AUROC: 0.8366 - val_loss: 0.3951 - val_ACC: 0.8530 - val_AUPR: 0.9301 - val_AUROC: 0.9355\n",
            "Epoch 6/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4829 - ACC: 0.7703 - AUPR: 0.8541 - AUROC: 0.8449 - val_loss: 0.3621 - val_ACC: 0.8640 - val_AUPR: 0.9428 - val_AUROC: 0.9432\n",
            "Epoch 7/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4913 - ACC: 0.7714 - AUPR: 0.8467 - AUROC: 0.8387 - val_loss: 0.3725 - val_ACC: 0.8695 - val_AUPR: 0.9401 - val_AUROC: 0.9454\n",
            "Epoch 8/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4688 - ACC: 0.7877 - AUPR: 0.8661 - AUROC: 0.8534 - val_loss: 0.3758 - val_ACC: 0.8700 - val_AUPR: 0.9414 - val_AUROC: 0.9457\n",
            "Epoch 9/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4703 - ACC: 0.7792 - AUPR: 0.8672 - AUROC: 0.8532 - val_loss: 0.3771 - val_ACC: 0.8660 - val_AUPR: 0.9391 - val_AUROC: 0.9443\n",
            "Epoch 10/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4732 - ACC: 0.7815 - AUPR: 0.8616 - AUROC: 0.8521 - val_loss: 0.3890 - val_ACC: 0.8660 - val_AUPR: 0.9398 - val_AUROC: 0.9447\n",
            "Epoch 11/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4833 - ACC: 0.7730 - AUPR: 0.8526 - AUROC: 0.8434 - val_loss: 0.3519 - val_ACC: 0.8765 - val_AUPR: 0.9430 - val_AUROC: 0.9476\n",
            "Epoch 12/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4735 - ACC: 0.7810 - AUPR: 0.8598 - AUROC: 0.8503 - val_loss: 0.3811 - val_ACC: 0.8710 - val_AUPR: 0.9394 - val_AUROC: 0.9452\n",
            "Epoch 13/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4747 - ACC: 0.7833 - AUPR: 0.8572 - AUROC: 0.8511 - val_loss: 0.3730 - val_ACC: 0.8810 - val_AUPR: 0.9476 - val_AUROC: 0.9516\n",
            "Epoch 14/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4667 - ACC: 0.7828 - AUPR: 0.8693 - AUROC: 0.8559 - val_loss: 0.3472 - val_ACC: 0.8815 - val_AUPR: 0.9481 - val_AUROC: 0.9502\n",
            "Epoch 15/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4751 - ACC: 0.7787 - AUPR: 0.8623 - AUROC: 0.8500 - val_loss: 0.3496 - val_ACC: 0.8805 - val_AUPR: 0.9492 - val_AUROC: 0.9529\n",
            "Epoch 16/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4686 - ACC: 0.7856 - AUPR: 0.8605 - AUROC: 0.8529 - val_loss: 0.3671 - val_ACC: 0.8770 - val_AUPR: 0.9470 - val_AUROC: 0.9508\n",
            "Epoch 17/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4638 - ACC: 0.7896 - AUPR: 0.8635 - AUROC: 0.8568 - val_loss: 0.3621 - val_ACC: 0.8750 - val_AUPR: 0.9424 - val_AUROC: 0.9482\n",
            "Epoch 18/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4680 - ACC: 0.7824 - AUPR: 0.8682 - AUROC: 0.8537 - val_loss: 0.3656 - val_ACC: 0.8775 - val_AUPR: 0.9427 - val_AUROC: 0.9481\n",
            "Epoch 19/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4697 - ACC: 0.7849 - AUPR: 0.8662 - AUROC: 0.8547 - val_loss: 0.3404 - val_ACC: 0.8710 - val_AUPR: 0.9436 - val_AUROC: 0.9471\n",
            "Epoch 20/100\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.4621 - ACC: 0.7919 - AUPR: 0.8700 - AUROC: 0.8578 - val_loss: 0.3690 - val_ACC: 0.8690 - val_AUPR: 0.9449 - val_AUROC: 0.9499\n",
            "Epoch 21/100\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.4565 - ACC: 0.7927 - AUPR: 0.8716 - AUROC: 0.8623 - val_loss: 0.3783 - val_ACC: 0.8755 - val_AUPR: 0.9438 - val_AUROC: 0.9478\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_LNYVnewPy7"
      },
      "source": [
        "## Compute metrics on all 3 models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "3ovGuCNkuXuv",
        "outputId": "848e342a-d8f8-43dd-daaf-51372c212d23"
      },
      "source": [
        "distilled_student_metrics = distiller.evaluate(x_test, y_test, verbose=False)\r\n",
        "teacher_metrics = distiller.teacher.evaluate(x_test, y_test, verbose=False)\r\n",
        "deepbind_from_scratch_metrics = deepbind_model.evaluate(x_test, y_test, verbose=False)\r\n",
        "names = deepbind_model.metrics_names\r\n",
        "df = pd.DataFrame(data={'Name':names, 'Student (distilled)':distilled_student_metrics, 'Student (from scratch)':deepbind_from_scratch_metrics, 'Teacher ':teacher_metrics})\r\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Student (distilled)</th>\n",
              "      <th>Student (from scratch)</th>\n",
              "      <th>Teacher</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>loss</td>\n",
              "      <td>0.350156</td>\n",
              "      <td>0.351006</td>\n",
              "      <td>0.173992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ACC</td>\n",
              "      <td>0.873750</td>\n",
              "      <td>0.873250</td>\n",
              "      <td>0.933250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AUPR</td>\n",
              "      <td>0.948689</td>\n",
              "      <td>0.950714</td>\n",
              "      <td>0.979900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AUROC</td>\n",
              "      <td>0.947340</td>\n",
              "      <td>0.951920</td>\n",
              "      <td>0.981071</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Name  Student (distilled)  Student (from scratch)  Teacher \n",
              "0   loss             0.350156                0.351006  0.173992\n",
              "1    ACC             0.873750                0.873250  0.933250\n",
              "2   AUPR             0.948689                0.950714  0.979900\n",
              "3  AUROC             0.947340                0.951920  0.981071"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_QRpUulvAfV"
      },
      "source": [
        "df.to_csv('modelmetrics-hightemp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD54SNjNo3Kx"
      },
      "source": [
        "##Run experiment on varying alpha and temperature values\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPsxCNE-jHe1",
        "outputId": "fed102c5-f88f-4d4a-bc29-4345c394ab38"
      },
      "source": [
        "alpha_values = [.1,.2,.3,.4,.5,.6,.7,.8,.9,]\r\n",
        "temperature_values = [6.,7.,8.,9.,10.,11.,12.,13.,14.,15.,16.,17.,18.,19.,20.]\r\n",
        "\r\n",
        "experiment_data=[]\r\n",
        "\r\n",
        "\r\n",
        "for alpha_value in alpha_values:\r\n",
        "  for temperature_value in temperature_values:\r\n",
        "    print('Alpha: ',alpha_value, ', Temperature: ',temperature_value)\r\n",
        "    # instantiate the student model and the distiller\r\n",
        "    student_model = get_student_model(L, A)\r\n",
        "    distiller = Distiller(student_model, teacher_model)\r\n",
        "    # compile the distiller\r\n",
        "    alpha = alpha_value\r\n",
        "    temperature = temperature_value\r\n",
        "    distiller.compile(\r\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\r\n",
        "        metrics=modelmetrics,\r\n",
        "        student_loss_fn=keras.losses.BinaryCrossentropy(name='bce'),\r\n",
        "        distillation_loss_fn=keras.losses.KLDivergence(),\r\n",
        "        alpha=alpha,\r\n",
        "        temperature=temperature,\r\n",
        "    )\r\n",
        "\r\n",
        "    # perform distillation\r\n",
        "    num_epochs = 50\r\n",
        "    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_AUROC', patience=20), \r\n",
        "                tfk.callbacks.ModelCheckpoint(\"best_distiller.hdf5\", monitor='val_AUROC', mode='max',save_weights_only=True, save_best_only=True)]\r\n",
        "    distiller.fit(x_train, y_train, \r\n",
        "                    epochs=num_epochs, \r\n",
        "                    batch_size=128, \r\n",
        "                    callbacks=callbacks, \r\n",
        "                    shuffle=True, \r\n",
        "                    validation_data=(x_valid, y_valid),\r\n",
        "                    verbose=0)\r\n",
        "    \r\n",
        "    #evaluate and save Distilled metrics\r\n",
        "    experiment_dist_student_metrics = distiller.evaluate(x_test, y_test, verbose=False)\r\n",
        "    hyperparameters = [alpha_value, temperature_value]\r\n",
        "    all_values = hyperparameters+experiment_dist_student_metrics\r\n",
        "    #add data to list\r\n",
        "    experiment_data.append(all_values)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Alpha:  0.1 , Temperature:  1.0\n",
            "Alpha:  0.1 , Temperature:  2.0\n",
            "Alpha:  0.1 , Temperature:  3.0\n",
            "Alpha:  0.1 , Temperature:  4.0\n",
            "Alpha:  0.1 , Temperature:  5.0\n",
            "Alpha:  0.1 , Temperature:  6.0\n",
            "Alpha:  0.1 , Temperature:  7.0\n",
            "Alpha:  0.1 , Temperature:  8.0\n",
            "Alpha:  0.1 , Temperature:  9.0\n",
            "Alpha:  0.1 , Temperature:  10.0\n",
            "Alpha:  0.1 , Temperature:  11.0\n",
            "Alpha:  0.1 , Temperature:  12.0\n",
            "Alpha:  0.1 , Temperature:  13.0\n",
            "Alpha:  0.1 , Temperature:  14.0\n",
            "Alpha:  0.1 , Temperature:  15.0\n",
            "Alpha:  0.15 , Temperature:  1.0\n",
            "Alpha:  0.15 , Temperature:  2.0\n",
            "Alpha:  0.15 , Temperature:  3.0\n",
            "Alpha:  0.15 , Temperature:  4.0\n",
            "Alpha:  0.15 , Temperature:  5.0\n",
            "Alpha:  0.15 , Temperature:  6.0\n",
            "Alpha:  0.15 , Temperature:  7.0\n",
            "Alpha:  0.15 , Temperature:  8.0\n",
            "Alpha:  0.15 , Temperature:  9.0\n",
            "Alpha:  0.15 , Temperature:  10.0\n",
            "Alpha:  0.15 , Temperature:  11.0\n",
            "Alpha:  0.15 , Temperature:  12.0\n",
            "Alpha:  0.15 , Temperature:  13.0\n",
            "Alpha:  0.15 , Temperature:  14.0\n",
            "Alpha:  0.15 , Temperature:  15.0\n",
            "Alpha:  0.2 , Temperature:  1.0\n",
            "Alpha:  0.2 , Temperature:  2.0\n",
            "Alpha:  0.2 , Temperature:  3.0\n",
            "Alpha:  0.2 , Temperature:  4.0\n",
            "Alpha:  0.2 , Temperature:  5.0\n",
            "Alpha:  0.2 , Temperature:  6.0\n",
            "Alpha:  0.2 , Temperature:  7.0\n",
            "Alpha:  0.2 , Temperature:  8.0\n",
            "Alpha:  0.2 , Temperature:  9.0\n",
            "Alpha:  0.2 , Temperature:  10.0\n",
            "Alpha:  0.2 , Temperature:  11.0\n",
            "Alpha:  0.2 , Temperature:  12.0\n",
            "Alpha:  0.2 , Temperature:  13.0\n",
            "Alpha:  0.2 , Temperature:  14.0\n",
            "Alpha:  0.2 , Temperature:  15.0\n",
            "Alpha:  0.25 , Temperature:  1.0\n",
            "Alpha:  0.25 , Temperature:  2.0\n",
            "Alpha:  0.25 , Temperature:  3.0\n",
            "Alpha:  0.25 , Temperature:  4.0\n",
            "Alpha:  0.25 , Temperature:  5.0\n",
            "Alpha:  0.25 , Temperature:  6.0\n",
            "Alpha:  0.25 , Temperature:  7.0\n",
            "Alpha:  0.25 , Temperature:  8.0\n",
            "Alpha:  0.25 , Temperature:  9.0\n",
            "Alpha:  0.25 , Temperature:  10.0\n",
            "Alpha:  0.25 , Temperature:  11.0\n",
            "Alpha:  0.25 , Temperature:  12.0\n",
            "Alpha:  0.25 , Temperature:  13.0\n",
            "Alpha:  0.25 , Temperature:  14.0\n",
            "Alpha:  0.25 , Temperature:  15.0\n",
            "Alpha:  0.3 , Temperature:  1.0\n",
            "Alpha:  0.3 , Temperature:  2.0\n",
            "Alpha:  0.3 , Temperature:  3.0\n",
            "Alpha:  0.3 , Temperature:  4.0\n",
            "Alpha:  0.3 , Temperature:  5.0\n",
            "Alpha:  0.3 , Temperature:  6.0\n",
            "Alpha:  0.3 , Temperature:  7.0\n",
            "Alpha:  0.3 , Temperature:  8.0\n",
            "Alpha:  0.3 , Temperature:  9.0\n",
            "Alpha:  0.3 , Temperature:  10.0\n",
            "Alpha:  0.3 , Temperature:  11.0\n",
            "Alpha:  0.3 , Temperature:  12.0\n",
            "Alpha:  0.3 , Temperature:  13.0\n",
            "Alpha:  0.3 , Temperature:  14.0\n",
            "Alpha:  0.3 , Temperature:  15.0\n",
            "Alpha:  0.35 , Temperature:  1.0\n",
            "Alpha:  0.35 , Temperature:  2.0\n",
            "Alpha:  0.35 , Temperature:  3.0\n",
            "Alpha:  0.35 , Temperature:  4.0\n",
            "Alpha:  0.35 , Temperature:  5.0\n",
            "Alpha:  0.35 , Temperature:  6.0\n",
            "Alpha:  0.35 , Temperature:  7.0\n",
            "Alpha:  0.35 , Temperature:  8.0\n",
            "Alpha:  0.35 , Temperature:  9.0\n",
            "Alpha:  0.35 , Temperature:  10.0\n",
            "Alpha:  0.35 , Temperature:  11.0\n",
            "Alpha:  0.35 , Temperature:  12.0\n",
            "Alpha:  0.35 , Temperature:  13.0\n",
            "Alpha:  0.35 , Temperature:  14.0\n",
            "Alpha:  0.35 , Temperature:  15.0\n",
            "Alpha:  0.4 , Temperature:  1.0\n",
            "Alpha:  0.4 , Temperature:  2.0\n",
            "Alpha:  0.4 , Temperature:  3.0\n",
            "Alpha:  0.4 , Temperature:  4.0\n",
            "Alpha:  0.4 , Temperature:  5.0\n",
            "Alpha:  0.4 , Temperature:  6.0\n",
            "Alpha:  0.4 , Temperature:  7.0\n",
            "Alpha:  0.4 , Temperature:  8.0\n",
            "Alpha:  0.4 , Temperature:  9.0\n",
            "Alpha:  0.4 , Temperature:  10.0\n",
            "Alpha:  0.4 , Temperature:  11.0\n",
            "Alpha:  0.4 , Temperature:  12.0\n",
            "Alpha:  0.4 , Temperature:  13.0\n",
            "Alpha:  0.4 , Temperature:  14.0\n",
            "Alpha:  0.4 , Temperature:  15.0\n",
            "Alpha:  0.45 , Temperature:  1.0\n",
            "Alpha:  0.45 , Temperature:  2.0\n",
            "Alpha:  0.45 , Temperature:  3.0\n",
            "Alpha:  0.45 , Temperature:  4.0\n",
            "Alpha:  0.45 , Temperature:  5.0\n",
            "Alpha:  0.45 , Temperature:  6.0\n",
            "Alpha:  0.45 , Temperature:  7.0\n",
            "Alpha:  0.45 , Temperature:  8.0\n",
            "Alpha:  0.45 , Temperature:  9.0\n",
            "Alpha:  0.45 , Temperature:  10.0\n",
            "Alpha:  0.45 , Temperature:  11.0\n",
            "Alpha:  0.45 , Temperature:  12.0\n",
            "Alpha:  0.45 , Temperature:  13.0\n",
            "Alpha:  0.45 , Temperature:  14.0\n",
            "Alpha:  0.45 , Temperature:  15.0\n",
            "Alpha:  0.5 , Temperature:  1.0\n",
            "Alpha:  0.5 , Temperature:  2.0\n",
            "Alpha:  0.5 , Temperature:  3.0\n",
            "Alpha:  0.5 , Temperature:  4.0\n",
            "Alpha:  0.5 , Temperature:  5.0\n",
            "Alpha:  0.5 , Temperature:  6.0\n",
            "Alpha:  0.5 , Temperature:  7.0\n",
            "Alpha:  0.5 , Temperature:  8.0\n",
            "Alpha:  0.5 , Temperature:  9.0\n",
            "Alpha:  0.5 , Temperature:  10.0\n",
            "Alpha:  0.5 , Temperature:  11.0\n",
            "Alpha:  0.5 , Temperature:  12.0\n",
            "Alpha:  0.5 , Temperature:  13.0\n",
            "Alpha:  0.5 , Temperature:  14.0\n",
            "Alpha:  0.5 , Temperature:  15.0\n",
            "Alpha:  0.55 , Temperature:  1.0\n",
            "Alpha:  0.55 , Temperature:  2.0\n",
            "Alpha:  0.55 , Temperature:  3.0\n",
            "Alpha:  0.55 , Temperature:  4.0\n",
            "Alpha:  0.55 , Temperature:  5.0\n",
            "Alpha:  0.55 , Temperature:  6.0\n",
            "Alpha:  0.55 , Temperature:  7.0\n",
            "Alpha:  0.55 , Temperature:  8.0\n",
            "Alpha:  0.55 , Temperature:  9.0\n",
            "Alpha:  0.55 , Temperature:  10.0\n",
            "Alpha:  0.55 , Temperature:  11.0\n",
            "Alpha:  0.55 , Temperature:  12.0\n",
            "Alpha:  0.55 , Temperature:  13.0\n",
            "Alpha:  0.55 , Temperature:  14.0\n",
            "Alpha:  0.55 , Temperature:  15.0\n",
            "Alpha:  0.6 , Temperature:  1.0\n",
            "Alpha:  0.6 , Temperature:  2.0\n",
            "Alpha:  0.6 , Temperature:  3.0\n",
            "Alpha:  0.6 , Temperature:  4.0\n",
            "Alpha:  0.6 , Temperature:  5.0\n",
            "Alpha:  0.6 , Temperature:  6.0\n",
            "Alpha:  0.6 , Temperature:  7.0\n",
            "Alpha:  0.6 , Temperature:  8.0\n",
            "Alpha:  0.6 , Temperature:  9.0\n",
            "Alpha:  0.6 , Temperature:  10.0\n",
            "Alpha:  0.6 , Temperature:  11.0\n",
            "Alpha:  0.6 , Temperature:  12.0\n",
            "Alpha:  0.6 , Temperature:  13.0\n",
            "Alpha:  0.6 , Temperature:  14.0\n",
            "Alpha:  0.6 , Temperature:  15.0\n",
            "Alpha:  0.65 , Temperature:  1.0\n",
            "Alpha:  0.65 , Temperature:  2.0\n",
            "Alpha:  0.65 , Temperature:  3.0\n",
            "Alpha:  0.65 , Temperature:  4.0\n",
            "Alpha:  0.65 , Temperature:  5.0\n",
            "Alpha:  0.65 , Temperature:  6.0\n",
            "Alpha:  0.65 , Temperature:  7.0\n",
            "Alpha:  0.65 , Temperature:  8.0\n",
            "Alpha:  0.65 , Temperature:  9.0\n",
            "Alpha:  0.65 , Temperature:  10.0\n",
            "Alpha:  0.65 , Temperature:  11.0\n",
            "Alpha:  0.65 , Temperature:  12.0\n",
            "Alpha:  0.65 , Temperature:  13.0\n",
            "Alpha:  0.65 , Temperature:  14.0\n",
            "Alpha:  0.65 , Temperature:  15.0\n",
            "Alpha:  0.7 , Temperature:  1.0\n",
            "Alpha:  0.7 , Temperature:  2.0\n",
            "Alpha:  0.7 , Temperature:  3.0\n",
            "Alpha:  0.7 , Temperature:  4.0\n",
            "Alpha:  0.7 , Temperature:  5.0\n",
            "Alpha:  0.7 , Temperature:  6.0\n",
            "Alpha:  0.7 , Temperature:  7.0\n",
            "Alpha:  0.7 , Temperature:  8.0\n",
            "Alpha:  0.7 , Temperature:  9.0\n",
            "Alpha:  0.7 , Temperature:  10.0\n",
            "Alpha:  0.7 , Temperature:  11.0\n",
            "Alpha:  0.7 , Temperature:  12.0\n",
            "Alpha:  0.7 , Temperature:  13.0\n",
            "Alpha:  0.7 , Temperature:  14.0\n",
            "Alpha:  0.7 , Temperature:  15.0\n",
            "Alpha:  0.75 , Temperature:  1.0\n",
            "Alpha:  0.75 , Temperature:  2.0\n",
            "Alpha:  0.75 , Temperature:  3.0\n",
            "Alpha:  0.75 , Temperature:  4.0\n",
            "Alpha:  0.75 , Temperature:  5.0\n",
            "Alpha:  0.75 , Temperature:  6.0\n",
            "Alpha:  0.75 , Temperature:  7.0\n",
            "Alpha:  0.75 , Temperature:  8.0\n",
            "Alpha:  0.75 , Temperature:  9.0\n",
            "Alpha:  0.75 , Temperature:  10.0\n",
            "Alpha:  0.75 , Temperature:  11.0\n",
            "Alpha:  0.75 , Temperature:  12.0\n",
            "Alpha:  0.75 , Temperature:  13.0\n",
            "Alpha:  0.75 , Temperature:  14.0\n",
            "Alpha:  0.75 , Temperature:  15.0\n",
            "Alpha:  0.8 , Temperature:  1.0\n",
            "Alpha:  0.8 , Temperature:  2.0\n",
            "Alpha:  0.8 , Temperature:  3.0\n",
            "Alpha:  0.8 , Temperature:  4.0\n",
            "Alpha:  0.8 , Temperature:  5.0\n",
            "Alpha:  0.8 , Temperature:  6.0\n",
            "Alpha:  0.8 , Temperature:  7.0\n",
            "Alpha:  0.8 , Temperature:  8.0\n",
            "Alpha:  0.8 , Temperature:  9.0\n",
            "Alpha:  0.8 , Temperature:  10.0\n",
            "Alpha:  0.8 , Temperature:  11.0\n",
            "Alpha:  0.8 , Temperature:  12.0\n",
            "Alpha:  0.8 , Temperature:  13.0\n",
            "Alpha:  0.8 , Temperature:  14.0\n",
            "Alpha:  0.8 , Temperature:  15.0\n",
            "Alpha:  0.85 , Temperature:  1.0\n",
            "Alpha:  0.85 , Temperature:  2.0\n",
            "Alpha:  0.85 , Temperature:  3.0\n",
            "Alpha:  0.85 , Temperature:  4.0\n",
            "Alpha:  0.85 , Temperature:  5.0\n",
            "Alpha:  0.85 , Temperature:  6.0\n",
            "Alpha:  0.85 , Temperature:  7.0\n",
            "Alpha:  0.85 , Temperature:  8.0\n",
            "Alpha:  0.85 , Temperature:  9.0\n",
            "Alpha:  0.85 , Temperature:  10.0\n",
            "Alpha:  0.85 , Temperature:  11.0\n",
            "Alpha:  0.85 , Temperature:  12.0\n",
            "Alpha:  0.85 , Temperature:  13.0\n",
            "Alpha:  0.85 , Temperature:  14.0\n",
            "Alpha:  0.85 , Temperature:  15.0\n",
            "Alpha:  0.9 , Temperature:  1.0\n",
            "Alpha:  0.9 , Temperature:  2.0\n",
            "Alpha:  0.9 , Temperature:  3.0\n",
            "Alpha:  0.9 , Temperature:  4.0\n",
            "Alpha:  0.9 , Temperature:  5.0\n",
            "Alpha:  0.9 , Temperature:  6.0\n",
            "Alpha:  0.9 , Temperature:  7.0\n",
            "Alpha:  0.9 , Temperature:  8.0\n",
            "Alpha:  0.9 , Temperature:  9.0\n",
            "Alpha:  0.9 , Temperature:  10.0\n",
            "Alpha:  0.9 , Temperature:  11.0\n",
            "Alpha:  0.9 , Temperature:  12.0\n",
            "Alpha:  0.9 , Temperature:  13.0\n",
            "Alpha:  0.9 , Temperature:  14.0\n",
            "Alpha:  0.9 , Temperature:  15.0\n",
            "Alpha:  0.95 , Temperature:  1.0\n",
            "Alpha:  0.95 , Temperature:  2.0\n",
            "Alpha:  0.95 , Temperature:  3.0\n",
            "Alpha:  0.95 , Temperature:  4.0\n",
            "Alpha:  0.95 , Temperature:  5.0\n",
            "Alpha:  0.95 , Temperature:  6.0\n",
            "Alpha:  0.95 , Temperature:  7.0\n",
            "Alpha:  0.95 , Temperature:  8.0\n",
            "Alpha:  0.95 , Temperature:  9.0\n",
            "Alpha:  0.95 , Temperature:  10.0\n",
            "Alpha:  0.95 , Temperature:  11.0\n",
            "Alpha:  0.95 , Temperature:  12.0\n",
            "Alpha:  0.95 , Temperature:  13.0\n",
            "Alpha:  0.95 , Temperature:  14.0\n",
            "Alpha:  0.95 , Temperature:  15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "bQw21PwHm9Sv",
        "outputId": "00908e1f-5e03-408c-9946-a4fd4211cd7d"
      },
      "source": [
        "#put results into a data table\r\n",
        "df = pd.DataFrame(experiment_data)\r\n",
        "columns = ['alpha', 'temperature', 'loss', 'ACC', 'AUPR', 'AUROC']\r\n",
        "df.columns=columns\r\n",
        "df\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>alpha</th>\n",
              "      <th>temperature</th>\n",
              "      <th>loss</th>\n",
              "      <th>ACC</th>\n",
              "      <th>AUPR</th>\n",
              "      <th>AUROC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.326731</td>\n",
              "      <td>0.87475</td>\n",
              "      <td>0.949032</td>\n",
              "      <td>0.948867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.10</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.412578</td>\n",
              "      <td>0.85500</td>\n",
              "      <td>0.934352</td>\n",
              "      <td>0.932151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.10</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.344072</td>\n",
              "      <td>0.87950</td>\n",
              "      <td>0.954441</td>\n",
              "      <td>0.952908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.10</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.393650</td>\n",
              "      <td>0.85875</td>\n",
              "      <td>0.940936</td>\n",
              "      <td>0.941597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.10</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.358533</td>\n",
              "      <td>0.88075</td>\n",
              "      <td>0.958433</td>\n",
              "      <td>0.957247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>265</th>\n",
              "      <td>0.95</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.318248</td>\n",
              "      <td>0.87550</td>\n",
              "      <td>0.953018</td>\n",
              "      <td>0.951944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>266</th>\n",
              "      <td>0.95</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.297036</td>\n",
              "      <td>0.88625</td>\n",
              "      <td>0.952102</td>\n",
              "      <td>0.952104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>267</th>\n",
              "      <td>0.95</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.415707</td>\n",
              "      <td>0.86625</td>\n",
              "      <td>0.943639</td>\n",
              "      <td>0.943936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268</th>\n",
              "      <td>0.95</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.431425</td>\n",
              "      <td>0.84925</td>\n",
              "      <td>0.928057</td>\n",
              "      <td>0.927825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>0.95</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.380735</td>\n",
              "      <td>0.87125</td>\n",
              "      <td>0.948027</td>\n",
              "      <td>0.948601</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>270 rows  6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     alpha  temperature      loss      ACC      AUPR     AUROC\n",
              "0     0.10          1.0  0.326731  0.87475  0.949032  0.948867\n",
              "1     0.10          2.0  0.412578  0.85500  0.934352  0.932151\n",
              "2     0.10          3.0  0.344072  0.87950  0.954441  0.952908\n",
              "3     0.10          4.0  0.393650  0.85875  0.940936  0.941597\n",
              "4     0.10          5.0  0.358533  0.88075  0.958433  0.957247\n",
              "..     ...          ...       ...      ...       ...       ...\n",
              "265   0.95         11.0  0.318248  0.87550  0.953018  0.951944\n",
              "266   0.95         12.0  0.297036  0.88625  0.952102  0.952104\n",
              "267   0.95         13.0  0.415707  0.86625  0.943639  0.943936\n",
              "268   0.95         14.0  0.431425  0.84925  0.928057  0.927825\n",
              "269   0.95         15.0  0.380735  0.87125  0.948027  0.948601\n",
              "\n",
              "[270 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32LbJfGuuC_m"
      },
      "source": [
        "df.to_csv('performancemetrics-hightemp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "e5G23QDkHlqS",
        "outputId": "7e85e332-5c0d-443e-db92-28e9c02d765f"
      },
      "source": [
        "#find best performing combination\r\n",
        "max_metric = experiment_data[0]\r\n",
        "for metrics in experiment_data:\r\n",
        "  if metrics[3]>max_metric[3]:\r\n",
        "    max_metric = metrics\r\n",
        "\r\n",
        "mf = pd.DataFrame(data={'Name':columns, 'Best Performance':max_metric})\r\n",
        "mf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Best Performance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>alpha</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>temperature</td>\n",
              "      <td>9.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>loss</td>\n",
              "      <td>0.297651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ACC</td>\n",
              "      <td>0.894000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AUPR</td>\n",
              "      <td>0.958145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>AUROC</td>\n",
              "      <td>0.959086</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Name  Best Performance\n",
              "0        alpha          0.800000\n",
              "1  temperature          9.000000\n",
              "2         loss          0.297651\n",
              "3          ACC          0.894000\n",
              "4         AUPR          0.958145\n",
              "5        AUROC          0.959086"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    }
  ]
}